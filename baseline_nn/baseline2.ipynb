{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "# from seq_feature_generate import seq_feature_generate\n",
    "# from seq_feature_generate_v2 import seq_feature_generate_v2\n",
    "from util_func.eval_function import auroc, cal_ks, auc, ks, fea_psi_calc\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing.data import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from util_func import TCN_V3 as TCN\n",
    "from util_func import self_attention as sf\n",
    "from util_func.look_ahead import Lookahead\n",
    "from util_func.LayerNormalization import LayerNormalization\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "load_model =  keras.models.load_model\n",
    "Sequential = keras.models.Sequential\n",
    "Dense = keras.layers.Dense\n",
    "Activation = keras.layers.Activation\n",
    "Dropout = keras.layers.Dropout\n",
    "initializers = keras.initializers\n",
    "regularizers = keras.regularizers\n",
    "optimizers = keras.optimizers\n",
    "Input = keras.layers.Input\n",
    "add = keras.layers.add\n",
    "Model = keras.models.Model\n",
    "BatchNormalization = keras.layers.BatchNormalization \n",
    "EarlyStopping =  keras.callbacks.EarlyStopping\n",
    "ModelCheckpoint = keras.callbacks.ModelCheckpoint\n",
    "K = keras.backend\n",
    "Concatenate = keras.layers.Concatenate\n",
    "Reshape = keras.layers.Reshape\n",
    "Flatten = keras.layers.Flatten\n",
    "Lambda = keras.layers.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LOAN_SEQ_LENGTH=10\n",
    "LOAN_FEATURE_NUM=26\n",
    "\n",
    "REPAY_SEQ_LENGTH=15\n",
    "REPAY_FEATURE_NUM=17\n",
    "# EVENT_ACTION_SEQ_LENGTH=30\n",
    "# EVENT_ACTION_FEATURE_NUM=2\n",
    "\n",
    "EVENT_SEQ_LENGTH=20\n",
    "EVENT_FEATURE_NUM=55\n",
    "\n",
    "STAT_FEATURE_NUM = 194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/'\n",
    "\n",
    "train_info_path = data_dir + 'feature_model_file/train_info.csv'\n",
    "test1_info_path = data_dir + 'feature_model_file/test1_info.csv'\n",
    "test_info_path = data_dir + 'feature_model_file/test_info.csv'\n",
    "\n",
    "train_label_path = data_dir + 'feature_model_file/train_label.npy'\n",
    "test1_label_path = data_dir + 'feature_model_file/test1_label.npy'\n",
    "test_label_path = data_dir + 'feature_model_file/test_label.npy'\n",
    "\n",
    "train_loan_seq_matrix_path = data_dir + 'feature_model_file/train_loan_seq_matrix.npy'\n",
    "train_repay_seq_matrix_path = data_dir + 'feature_model_file/train_repay_seq_matrix.npy'\n",
    "train_event1_seq_matrix_path = data_dir + 'feature_model_file/train_event1_seq_matrix.npy'\n",
    "train_event2_seq_matrix_path = data_dir + 'feature_model_file/train_event2_seq_matrix.npy'\n",
    "\n",
    "test1_loan_seq_matrix_path = data_dir + 'feature_model_file/test1_loan_seq_matrix.npy'\n",
    "test1_repay_seq_matrix_path = data_dir + 'feature_model_file/test1_repay_seq_matrix.npy'\n",
    "test1_event1_seq_matrix_path = data_dir + 'feature_model_file/test1_event1_seq_matrix.npy'\n",
    "test1_event2_seq_matrix_path = data_dir + 'feature_model_file/test1_event2_seq_matrix.npy'\n",
    "\n",
    "test_loan_seq_matrix_path = data_dir + 'feature_model_file/test_loan_seq_matrix.npy'\n",
    "test_repay_seq_matrix_path = data_dir + 'feature_model_file/test_repay_seq_matrix.npy'\n",
    "test_event1_seq_matrix_path = data_dir + 'feature_model_file/test_event1_seq_matrix.npy'\n",
    "test_event2_seq_matrix_path = data_dir + 'feature_model_file/test_event2_seq_matrix.npy'\n",
    "\n",
    "stat_feature_train_norm_path = data_dir + 'feature_model_file/stat_feature_train.npy'\n",
    "stat_feature_test1_norm_path = data_dir + 'feature_model_file/stat_feature_test1.npy'\n",
    "stat_feature_test_norm_path = data_dir + 'feature_model_file/stat_feature_test.npy'\n",
    "\n",
    "save_model_file = data_dir + 'model_file/user_demand_model_tmp_0820.h5'\n",
    "save_pb_file_path = data_dir + 'model_file'\n",
    "save_pb_file_name = 'user_demand_model_online.pb'\n",
    "\n",
    "new_model_pred_train_path = data_dir + 'model_file/new_model_pred_train.csv'\n",
    "new_model_pred_test_path = data_dir + 'model_file/new_model_pred_test.csv'\n",
    "new_model_pred_test1_path = data_dir + 'model_file/new_model_pred_test1.csv'\n",
    "\n",
    "wy_eval_path = data_dir + 'model_file/cali_score_final_eval.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_event_seq_matrix = np.load(train_event2_seq_matrix_path)\n",
    "train_loan_seq_matrix = np.load(train_loan_seq_matrix_path)\n",
    "train_repay_seq_matrix = np.load(train_repay_seq_matrix_path)\n",
    "\n",
    "test_event_seq_matrix = np.load(test_event2_seq_matrix_path)\n",
    "test_loan_seq_matrix = np.load(test_loan_seq_matrix_path)\n",
    "test_repay_seq_matrix = np.load(test_repay_seq_matrix_path)\n",
    "\n",
    "test1_event_seq_matrix = np.load(test1_event2_seq_matrix_path)\n",
    "test1_loan_seq_matrix = np.load(test1_loan_seq_matrix_path)\n",
    "test1_repay_seq_matrix = np.load(test1_repay_seq_matrix_path)\n",
    "\n",
    "baseline_feature_top_train_norm = np.load(stat_feature_train_norm_path)\n",
    "baseline_feature_top_test_norm = np.load(stat_feature_test_norm_path)\n",
    "baseline_feature_top_test1_norm = np.load(stat_feature_test1_norm_path)\n",
    "\n",
    "train_label = np.load(train_label_path)\n",
    "test_label = np.load(test_label_path)\n",
    "test1_label = np.load(test1_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((781990, 20, 55), (781990, 10, 26), (781990, 15, 17))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_event_seq_matrix.shape, train_loan_seq_matrix.shape, train_repay_seq_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def Demand_model_attention_structure():\n",
    "   \n",
    "    K.clear_session()\n",
    "\n",
    "    # --------------------------------------------- encoders ------------------------------------\n",
    "    # ---------- loan seq encoder ----------\n",
    "    loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "    print('use gru to encode loan seq...')\n",
    "    # use gru to encode loan seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    loan_o = keras.layers.GRU( \n",
    "        input_shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM),\n",
    "        units=32, use_bias=True, \n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "        name = 'loan_gru')(loan_inputs)\n",
    "\n",
    "    \n",
    "    # -------------- repay seq encoder -------------\n",
    "    repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "    print('use gru to encode repay seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    repay_o = keras.layers.GRU( \n",
    "                input_shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM),\n",
    "                units=32, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='repay_gru')(repay_inputs)\n",
    "\n",
    "    \n",
    "    # -------------- event seq encoder ---------------\n",
    "    event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "    print('use gru to encode event seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    event_o = keras.layers.GRU( \n",
    "                input_shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM),\n",
    "                units=32, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='event_gru')(event_inputs)\n",
    "    print(event_o)\n",
    "    \n",
    "    \n",
    "    # self attention for click, loan, repay encode vector\n",
    "    combine_input_multi = keras.layers.concatenate([loan_o, event_o], name='encoder_concatenate')\n",
    "    combine_input_multi = keras.layers.Reshape((2,32), name = 'reshape_1')(combine_input_multi)\n",
    "    seq_encoder = sf.Self_Attention_cross_seq(32,32)(combine_input_multi)\n",
    "    seq_encoder = keras.layers.Flatten(name='flatten_encoder')(seq_encoder)\n",
    "\n",
    "    # --------- baseline dense layer ---------\n",
    "    baseline_input = keras.layers.Input(shape=(STAT_FEATURE_NUM,), name='baseline_top_inputs')\n",
    "\n",
    "    baseline_o = Dense(128, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer1')(baseline_input)\n",
    "    baseline_o = Dropout(0.2)(baseline_o)\n",
    "\n",
    "    baseline_o = Dense(64, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer2')(baseline_o)\n",
    "    baseline_o = Dropout(0.1)(baseline_o)\n",
    "    \n",
    "    \n",
    "    # -------- combine layer ------------\n",
    "    combine_o = keras.layers.concatenate([baseline_o, seq_encoder], name='comine_input')\n",
    "\n",
    "    # -------- combine dense ------------\n",
    "    o = Dense(1, activation = 'sigmoid',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_output')(combine_o)\n",
    "\n",
    "    # whole global model\n",
    "    combine_model = Model(inputs = [loan_inputs, repay_inputs, event_inputs, baseline_input], outputs = o)\n",
    "    print(combine_model.summary())\n",
    "\n",
    "    return combine_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Demand_model_structure():\n",
    "   \n",
    "    K.clear_session()\n",
    "\n",
    "    # --------------------------------------------- encoders ------------------------------------\n",
    "    # ---------- loan seq encoder ----------\n",
    "    loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "    print('use gru to encode loan seq...')\n",
    "    # use gru to encode loan seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    loan_o = keras.layers.GRU( \n",
    "        input_shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM),\n",
    "        units=26, use_bias=True, \n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "        name = 'loan_gru')(loan_inputs)\n",
    "    print(loan_o)\n",
    "\n",
    "    \n",
    "\n",
    "    # -------------- repay seq encoder -------------\n",
    "    repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "    print('use gru to encode repay seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    repay_o = keras.layers.GRU( \n",
    "                input_shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM),\n",
    "                units=17, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='repay_gru')(repay_inputs)\n",
    "    print(repay_o)\n",
    "    \n",
    "    # -------------- event seq encoder ---------------\n",
    "#     event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='click_seq_input')\n",
    "#     print('use tcn to encode event seq...')\n",
    "#     # use tcn to encode click seq\n",
    "#     event_tcn = TCN.TCN(nb_filters=32,\n",
    "#                 kernel_size=7,\n",
    "#                 nb_stacks=1,\n",
    "#                 dilations=(1, 7),\n",
    "#                 padding='causal',\n",
    "#                 use_skip_connections=False,\n",
    "#                 dropout_rate=0.3,\n",
    "#                 return_sequences=True,\n",
    "#                 activation='relu',\n",
    "#                 name='event_tcn',\n",
    "#                 kernel_initializer='he_normal',\n",
    "#                 use_batch_norm=False,\n",
    "#                 block_size=1)(event_inputs)\n",
    "\n",
    "#     event_o = Lambda(lambda tt: keras.layers.average([tt[:,EVENT_SEQ_LENGTH//2,:], tt[:, -1, :]]))(event_tcn)\n",
    "#     print(event_o)\n",
    "    \n",
    "    event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "    print('use gru to encode event seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    event_o = keras.layers.GRU( \n",
    "                input_shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM),\n",
    "                units=55, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='event_gru')(event_inputs)\n",
    "    print(event_o)\n",
    "    \n",
    "    \n",
    "    # self attention for click, loan, repay encode vector\n",
    "    combine_input_multi = keras.layers.concatenate([loan_o, repay_o, event_o], name='encoder_concatenate')\n",
    "#     combine_input_multi = keras.layers.Reshape((3,32), name = 'reshape_1')(combine_input_multi)\n",
    "#     seq_encoder = sf.Self_Attention_cross_seq(32,32)(combine_input_multi)\n",
    "#     seq_encoder = keras.layers.Flatten(name='flatten_encoder')(seq_encoder)\n",
    "\n",
    "    # --------- baseline dense layer ---------\n",
    "    baseline_input = keras.layers.Input(shape=(197,), name='baseline_top_inputs')\n",
    "\n",
    "    baseline_o = Dense(128, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer1')(baseline_input)\n",
    "    baseline_o = Dropout(0.2)(baseline_o)\n",
    "\n",
    "    baseline_o = Dense(64, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer2')(baseline_o)\n",
    "    baseline_o = Dropout(0.1)(baseline_o)\n",
    "    \n",
    "    \n",
    "    # -------- combine layer ------------\n",
    "    combine_o = keras.layers.concatenate([baseline_o, combine_input_multi], name='comine_input')\n",
    "\n",
    "    # -------- combine dense ------------\n",
    "    o = Dense(1, activation = 'sigmoid',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_output')(combine_o)\n",
    "\n",
    "    # whole global model\n",
    "    combine_model = Model(inputs = [loan_inputs, repay_inputs, event_inputs, baseline_input], outputs = o)\n",
    "    print(combine_model.summary())\n",
    "\n",
    "    return combine_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Demand_model_structure_only_seq():\n",
    "   \n",
    "    K.clear_session()\n",
    "\n",
    "    # --------------------------------------------- encoders ------------------------------------\n",
    "    # ---------- loan seq encoder ----------\n",
    "    loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "    print('use gru to encode loan seq...')\n",
    "    # use gru to encode loan seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    loan_o = keras.layers.GRU( \n",
    "        input_shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM),\n",
    "        units=26, use_bias=True, \n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "        name = 'loan_gru')(loan_inputs)\n",
    "    print(loan_o)\n",
    "\n",
    "    # -------------- repay seq encoder -------------\n",
    "    repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "    print('use gru to encode repay seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    repay_o = keras.layers.GRU( \n",
    "                input_shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM),\n",
    "                units=17, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='repay_gru')(repay_inputs)\n",
    "    print(repay_o)\n",
    "    \n",
    "    # -------------- click seq encoder ---------------\n",
    "#     event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='click_seq_input')\n",
    "#     print('use tcn to encode event seq...')\n",
    "#     # use tcn to encode click seq\n",
    "#     event_tcn = TCN.TCN(nb_filters=32,\n",
    "#                 kernel_size=7,\n",
    "#                 nb_stacks=1,\n",
    "#                 dilations=(1, 7),\n",
    "#                 padding='causal',\n",
    "#                 use_skip_connections=False,\n",
    "#                 dropout_rate=0.3,\n",
    "#                 return_sequences=True,\n",
    "#                 activation='relu',\n",
    "#                 name='event_tcn',\n",
    "#                 kernel_initializer='he_normal',\n",
    "#                 use_batch_norm=False,\n",
    "#                 block_size=1)(event_inputs)\n",
    "\n",
    "#     event_o = Lambda(lambda tt: keras.layers.average([tt[:,EVENT_SEQ_LENGTH//2,:], tt[:, -1, :]]))(event_tcn)\n",
    "#     print(event_o)\n",
    "    \n",
    "    event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "    print('use gru to encode event seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    event_o = keras.layers.GRU( \n",
    "                input_shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM),\n",
    "                units=55, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='event_gru')(event_inputs)\n",
    "    print(event_o)\n",
    "    \n",
    "    \n",
    "    # self attention for click, loan, repay encode vector\n",
    "    combine_input_multi = keras.layers.concatenate([loan_o, repay_o, event_o], name='encoder_concatenate')\n",
    "#     combine_input_multi = keras.layers.Reshape((3,32), name = 'reshape_1')(combine_input_multi)\n",
    "#     seq_encoder = sf.Self_Attention_cross_seq(32,32)(combine_input_multi)\n",
    "#     seq_encoder = keras.layers.Flatten(name='flatten_encoder')(seq_encoder)\n",
    "\n",
    "    # -------- combine dense ------------\n",
    "    o = Dense(1, activation = 'sigmoid',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=1e-4, l1=0.0),name = 'fc_output')(combine_input_multi)\n",
    "\n",
    "    # whole global model\n",
    "    combine_model = Model(inputs = [loan_inputs, repay_inputs,event_inputs], outputs = o)\n",
    "\n",
    "    print(combine_model.summary())\n",
    "\n",
    "    return combine_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def update_model_refresh(save_model_file, \n",
    "                         train_loan_seq_matrix, train_repay_seq_matrix,train_event_seq_matrix, \n",
    "                         baseline_feature_top_train_norm,train_label, \n",
    "                         test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix, \n",
    "                         baseline_feature_top_test_norm,test_label):\n",
    "    # Setting optimization config\n",
    "#     combine_model_1=Demand_model_structure()\n",
    "#     combine_model_1=Demand_model_structure_only_seq()\n",
    "    combine_model_1=Demand_model_attention_structure()\n",
    "    \n",
    "    learning_rate= 1e-3\n",
    "    adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07,)\n",
    "    look_ahead = Lookahead(adam, sync_period=5, slow_step=0.5)\n",
    "    \n",
    "    combine_model_1.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = [auroc], sample_weight_mode = None) \n",
    "    \n",
    "    # # Training\n",
    "    early_stopping = EarlyStopping(monitor='val_auroc', patience=20, verbose=2, mode='max')\n",
    "    filepath=save_model_file\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_auroc', verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    history_v1 = combine_model_1.fit(\n",
    "        \n",
    "        [train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm],\n",
    "#         [train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix],\n",
    "\n",
    "        train_label,\n",
    "        epochs = 50,\n",
    "        batch_size = 512,\n",
    "        \n",
    "        #verbose = 10,  ## 每个epoch输出一行记录\n",
    "        # validation_split = 0.25,\n",
    "        \n",
    "        validation_data = ([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm], test_label),\n",
    "#         validation_data = ([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix], test_label),\n",
    "\n",
    "        shuffle = True,\n",
    "        class_weight = {0:1, 1:1},\n",
    "        initial_epoch=0,\n",
    "        callbacks=[early_stopping, checkpoint]\n",
    "    )      \n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_evaluation_compare(new_model_filepath,\n",
    "                            train_event_seq_matrix, train_loan_seq_matrix, train_repay_seq_matrix, baseline_feature_top_train_norm,train_label,\n",
    "                            test1_event_seq_matrix, test1_loan_seq_matrix, test1_repay_seq_matrix, baseline_feature_top_test1_norm,test1_label,\n",
    "                            test_event_seq_matrix, test_loan_seq_matrix, test_repay_seq_matrix, baseline_feature_top_test_norm,test_label,\n",
    "                            new_model_pred_train_path,new_model_pred_test_path,new_model_pred_test1_path\n",
    "                            ):\n",
    "    ### evaluation new_model\n",
    "    model = Demand_model_structure()\n",
    "    model.load_weights(new_model_filepath)\n",
    "    print(\"-------------------model evaluatie------------------\")\n",
    "    print(\"evaluate on train data:\")\n",
    "    pred_train = model.predict([train_loan_seq_matrix, train_repay_seq_matrix,train_event_seq_matrix, baseline_feature_top_train_norm])\n",
    "    train_auc = auc(pred_train[:,0], train_label)\n",
    "    train_ks = ks(train_label, pred_train[:,0])\n",
    "    print(train_auc, train_ks)\n",
    "\n",
    "    print(\"evaluate on test data:\")\n",
    "    pred_test = model.predict([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix, baseline_feature_top_test_norm])\n",
    "    test_auc = auc(pred_test[:,0], test_label)\n",
    "    test_ks = ks(test_label, pred_test[:,0])\n",
    "    print(test_auc, test_ks)\n",
    "\n",
    "    print(\"evaluate on test data:\")\n",
    "    pred_test1 = model.predict([test1_loan_seq_matrix, test1_repay_seq_matrix, test1_event_seq_matrix, baseline_feature_top_test1_norm])\n",
    "    test1_auc = auc(pred_test1[:,0], test1_label)\n",
    "    test1_ks = ks(test1_label, pred_test1[:,0])\n",
    "    print(test_auc, test_ks)\n",
    "\n",
    "    # output \n",
    "    pd.DataFrame(pred_train,columns=['new_model_pred_train']).to_csv(new_model_pred_train_path, sep='\\t',index=0)\n",
    "    pd.DataFrame(pred_test,columns=['new_model_pred_test']).to_csv(new_model_pred_test_path, sep='\\t',index=0)\n",
    "    pd.DataFrame(pred_test1,columns=['new_model_pred_test1']).to_csv(new_model_pred_test1_path, sep='\\t',index=0)\n",
    "\n",
    "    ### compare new_model\n",
    "    model.load_weights(new_model_filepath)\n",
    "\n",
    "    print(\"evaluate on train data:\")\n",
    "    new_model_pred_train = model.predict([train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm])\n",
    "    train_auc = auc(new_model_pred_train[:,0], train_label)\n",
    "    train_ks = ks(train_label, new_model_pred_train[:,0])\n",
    "    print(train_auc, train_ks)\n",
    "\n",
    "    print(\"evaluate on test data:\")\n",
    "    new_model_pred_test = model.predict([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix, baseline_feature_top_test_norm])\n",
    "    test_auc = auc(new_model_pred_test[:,0], test_label)\n",
    "    test_ks = ks(test_label, new_model_pred_test[:,0])\n",
    "    print(test_auc, test_ks)\n",
    "\n",
    "    print(\"evaluate on test1 data:\")\n",
    "    new_model_pred_test1 = model.predict([test1_loan_seq_matrix, test1_repay_seq_matrix, test1_event_seq_matrix, baseline_feature_top_test1_norm])\n",
    "    test1_auc = auc(new_model_pred_test1[:,0], test1_label)\n",
    "    test1_ks = ks(test1_label, new_model_pred_test1[:,0])\n",
    "    print(test1_auc, test1_ks)\n",
    "\n",
    "    pd.DataFrame(new_model_pred_train,columns=['new_model_pred_train']).to_csv(new_model_pred_train_path, sep='\\t',index=0)\n",
    "    pd.DataFrame(new_model_pred_test,columns=['new_model_pred_test']).to_csv(new_model_pred_test_path, sep='\\t',index=0)\n",
    "    pd.DataFrame(new_model_pred_test1,columns=['new_model_pred_test1']).to_csv(new_model_pred_test1_path, sep='\\t',index=0)\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gru to encode loan seq...\n",
      "use gru to encode repay seq...\n",
      "use gru to encode event seq...\n",
      "Tensor(\"event_gru/TensorArrayReadV3:0\", shape=(?, 32), dtype=float32)\n",
      "WQ.shape (?, 2, 32)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (?, 32, 2)\n",
      "QK.shape (?, 2, 2)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "loan_seq_input (InputLayer)     (None, 10, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_seq_input (InputLayer)    (None, 20, 55)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "baseline_top_inputs (InputLayer (None, 194)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loan_gru (GRU)                  (None, 32)           5664        loan_seq_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "event_gru (GRU)                 (None, 32)           8448        event_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 128)          24960       baseline_top_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_concatenate (Concatenat (None, 64)           0           loan_gru[0][0]                   \n",
      "                                                                 event_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 32)        0           encoder_concatenate[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_cross_seq_1 (Se (None, 2, 32)        3072        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_encoder (Flatten)       (None, 64)           0           self__attention_cross_seq_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "comine_input (Concatenate)      (None, 128)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fc_output (Dense)               (None, 1)            129         comine_input[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 50,529\n",
      "Trainable params: 50,529\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 781990 samples, validate on 131122 samples\n",
      "Epoch 1/50\n",
      "781990/781990 [==============================] - 161s 206us/step - loss: 0.4162 - auroc: 0.8319 - val_loss: 0.4306 - val_auroc: 0.8404\n",
      "\n",
      "Epoch 00001: val_auroc improved from -inf to 0.84036, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 2/50\n",
      "781990/781990 [==============================] - 130s 166us/step - loss: 0.3842 - auroc: 0.8504 - val_loss: 0.4212 - val_auroc: 0.8460\n",
      "\n",
      "Epoch 00002: val_auroc improved from 0.84036 to 0.84603, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 3/50\n",
      "781990/781990 [==============================] - 128s 163us/step - loss: 0.3777 - auroc: 0.8552 - val_loss: 0.4157 - val_auroc: 0.8507\n",
      "\n",
      "Epoch 00003: val_auroc improved from 0.84603 to 0.85074, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 4/50\n",
      "781990/781990 [==============================] - 128s 164us/step - loss: 0.3736 - auroc: 0.8583 - val_loss: 0.4125 - val_auroc: 0.8531\n",
      "\n",
      "Epoch 00004: val_auroc improved from 0.85074 to 0.85309, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 5/50\n",
      "781990/781990 [==============================] - 128s 163us/step - loss: 0.3709 - auroc: 0.8604 - val_loss: 0.4154 - val_auroc: 0.8527\n",
      "\n",
      "Epoch 00005: val_auroc did not improve from 0.85309\n",
      "Epoch 6/50\n",
      "781990/781990 [==============================] - 129s 165us/step - loss: 0.3691 - auroc: 0.8619 - val_loss: 0.4083 - val_auroc: 0.8562\n",
      "\n",
      "Epoch 00006: val_auroc improved from 0.85309 to 0.85618, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 7/50\n",
      "781990/781990 [==============================] - 130s 167us/step - loss: 0.3673 - auroc: 0.8633 - val_loss: 0.4110 - val_auroc: 0.8543\n",
      "\n",
      "Epoch 00007: val_auroc did not improve from 0.85618\n",
      "Epoch 8/50\n",
      "781990/781990 [==============================] - 130s 166us/step - loss: 0.3662 - auroc: 0.8643 - val_loss: 0.4113 - val_auroc: 0.8558\n",
      "\n",
      "Epoch 00008: val_auroc did not improve from 0.85618\n",
      "Epoch 9/50\n",
      "781990/781990 [==============================] - 130s 166us/step - loss: 0.3652 - auroc: 0.8652 - val_loss: 0.4096 - val_auroc: 0.8555\n",
      "\n",
      "Epoch 00009: val_auroc did not improve from 0.85618\n",
      "Epoch 10/50\n",
      "781990/781990 [==============================] - 132s 169us/step - loss: 0.3642 - auroc: 0.8661 - val_loss: 0.4068 - val_auroc: 0.8561\n",
      "\n",
      "Epoch 00010: val_auroc did not improve from 0.85618\n",
      "Epoch 11/50\n",
      "781990/781990 [==============================] - 169s 216us/step - loss: 0.3635 - auroc: 0.8664 - val_loss: 0.4084 - val_auroc: 0.8567\n",
      "\n",
      "Epoch 00011: val_auroc improved from 0.85618 to 0.85675, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 12/50\n",
      "781990/781990 [==============================] - 207s 265us/step - loss: 0.3631 - auroc: 0.8673 - val_loss: 0.4057 - val_auroc: 0.8575\n",
      "\n",
      "Epoch 00012: val_auroc improved from 0.85675 to 0.85755, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 13/50\n",
      "781990/781990 [==============================] - 195s 249us/step - loss: 0.3619 - auroc: 0.8678 - val_loss: 0.4057 - val_auroc: 0.8576\n",
      "\n",
      "Epoch 00013: val_auroc improved from 0.85755 to 0.85763, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 14/50\n",
      "781990/781990 [==============================] - 137s 175us/step - loss: 0.3614 - auroc: 0.8683 - val_loss: 0.4076 - val_auroc: 0.8588\n",
      "\n",
      "Epoch 00014: val_auroc improved from 0.85763 to 0.85880, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 15/50\n",
      "781990/781990 [==============================] - 127s 163us/step - loss: 0.3608 - auroc: 0.8687 - val_loss: 0.4037 - val_auroc: 0.8590\n",
      "\n",
      "Epoch 00015: val_auroc improved from 0.85880 to 0.85902, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 16/50\n",
      "781990/781990 [==============================] - 126s 162us/step - loss: 0.3605 - auroc: 0.8693 - val_loss: 0.4060 - val_auroc: 0.8586\n",
      "\n",
      "Epoch 00016: val_auroc did not improve from 0.85902\n",
      "Epoch 17/50\n",
      "781990/781990 [==============================] - 131s 168us/step - loss: 0.3598 - auroc: 0.8700 - val_loss: 0.4078 - val_auroc: 0.8596\n",
      "\n",
      "Epoch 00017: val_auroc improved from 0.85902 to 0.85960, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 18/50\n",
      "781990/781990 [==============================] - 129s 165us/step - loss: 0.3593 - auroc: 0.8703 - val_loss: 0.4087 - val_auroc: 0.8593\n",
      "\n",
      "Epoch 00018: val_auroc did not improve from 0.85960\n",
      "Epoch 19/50\n",
      "781990/781990 [==============================] - 127s 163us/step - loss: 0.3589 - auroc: 0.8706 - val_loss: 0.4067 - val_auroc: 0.8606\n",
      "\n",
      "Epoch 00019: val_auroc improved from 0.85960 to 0.86060, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 20/50\n",
      "781990/781990 [==============================] - 126s 161us/step - loss: 0.3586 - auroc: 0.8709 - val_loss: 0.4084 - val_auroc: 0.8598\n",
      "\n",
      "Epoch 00020: val_auroc did not improve from 0.86060\n",
      "Epoch 21/50\n",
      "781990/781990 [==============================] - 137s 175us/step - loss: 0.3584 - auroc: 0.8712 - val_loss: 0.4044 - val_auroc: 0.8598\n",
      "\n",
      "Epoch 00021: val_auroc did not improve from 0.86060\n",
      "Epoch 22/50\n",
      "781990/781990 [==============================] - 128s 164us/step - loss: 0.3581 - auroc: 0.8714 - val_loss: 0.4064 - val_auroc: 0.8593\n",
      "\n",
      "Epoch 00022: val_auroc did not improve from 0.86060\n",
      "Epoch 23/50\n",
      "781990/781990 [==============================] - 133s 170us/step - loss: 0.3576 - auroc: 0.8717 - val_loss: 0.4015 - val_auroc: 0.8603\n",
      "\n",
      "Epoch 00023: val_auroc did not improve from 0.86060\n",
      "Epoch 24/50\n",
      "781990/781990 [==============================] - 146s 187us/step - loss: 0.3574 - auroc: 0.8718 - val_loss: 0.4054 - val_auroc: 0.8608\n",
      "\n",
      "Epoch 00024: val_auroc improved from 0.86060 to 0.86083, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 25/50\n",
      "781990/781990 [==============================] - 196s 251us/step - loss: 0.3572 - auroc: 0.8722 - val_loss: 0.4022 - val_auroc: 0.8603\n",
      "\n",
      "Epoch 00025: val_auroc did not improve from 0.86083\n",
      "Epoch 26/50\n",
      "781990/781990 [==============================] - 201s 257us/step - loss: 0.3567 - auroc: 0.8725 - val_loss: 0.4067 - val_auroc: 0.8592\n",
      "\n",
      "Epoch 00026: val_auroc did not improve from 0.86083\n",
      "Epoch 27/50\n",
      "781990/781990 [==============================] - 191s 245us/step - loss: 0.3566 - auroc: 0.8726 - val_loss: 0.4041 - val_auroc: 0.8602\n",
      "\n",
      "Epoch 00027: val_auroc did not improve from 0.86083\n",
      "Epoch 28/50\n",
      "781990/781990 [==============================] - 187s 239us/step - loss: 0.3564 - auroc: 0.8728 - val_loss: 0.4039 - val_auroc: 0.8601\n",
      "\n",
      "Epoch 00028: val_auroc did not improve from 0.86083\n",
      "Epoch 29/50\n",
      "781990/781990 [==============================] - 185s 237us/step - loss: 0.3563 - auroc: 0.8731 - val_loss: 0.4044 - val_auroc: 0.8596\n",
      "\n",
      "Epoch 00029: val_auroc did not improve from 0.86083\n",
      "Epoch 30/50\n",
      "781990/781990 [==============================] - 189s 242us/step - loss: 0.3559 - auroc: 0.8735 - val_loss: 0.4018 - val_auroc: 0.8610\n",
      "\n",
      "Epoch 00030: val_auroc improved from 0.86083 to 0.86101, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 31/50\n",
      "781990/781990 [==============================] - 159s 203us/step - loss: 0.3556 - auroc: 0.8735 - val_loss: 0.4035 - val_auroc: 0.8602\n",
      "\n",
      "Epoch 00031: val_auroc did not improve from 0.86101\n",
      "Epoch 32/50\n",
      "781990/781990 [==============================] - 148s 189us/step - loss: 0.3558 - auroc: 0.8733 - val_loss: 0.4045 - val_auroc: 0.8599\n",
      "\n",
      "Epoch 00032: val_auroc did not improve from 0.86101\n",
      "Epoch 33/50\n",
      "781990/781990 [==============================] - 130s 167us/step - loss: 0.3556 - auroc: 0.8739 - val_loss: 0.4035 - val_auroc: 0.8614\n",
      "\n",
      "Epoch 00033: val_auroc improved from 0.86101 to 0.86135, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 34/50\n",
      "781990/781990 [==============================] - 133s 170us/step - loss: 0.3552 - auroc: 0.8742 - val_loss: 0.4023 - val_auroc: 0.8605\n",
      "\n",
      "Epoch 00034: val_auroc did not improve from 0.86135\n",
      "Epoch 35/50\n",
      "781990/781990 [==============================] - 145s 186us/step - loss: 0.3547 - auroc: 0.8744 - val_loss: 0.4036 - val_auroc: 0.8616\n",
      "\n",
      "Epoch 00035: val_auroc improved from 0.86135 to 0.86156, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 36/50\n",
      "781990/781990 [==============================] - 148s 189us/step - loss: 0.3549 - auroc: 0.8744 - val_loss: 0.4074 - val_auroc: 0.8593\n",
      "\n",
      "Epoch 00036: val_auroc did not improve from 0.86156\n",
      "Epoch 37/50\n",
      "781990/781990 [==============================] - 139s 177us/step - loss: 0.3548 - auroc: 0.8744 - val_loss: 0.4040 - val_auroc: 0.8601\n",
      "\n",
      "Epoch 00037: val_auroc did not improve from 0.86156\n",
      "Epoch 38/50\n",
      "781990/781990 [==============================] - 150s 192us/step - loss: 0.3546 - auroc: 0.8748 - val_loss: 0.4060 - val_auroc: 0.8608\n",
      "\n",
      "Epoch 00038: val_auroc did not improve from 0.86156\n",
      "Epoch 39/50\n",
      "781990/781990 [==============================] - 153s 196us/step - loss: 0.3544 - auroc: 0.8748 - val_loss: 0.4028 - val_auroc: 0.8603\n",
      "\n",
      "Epoch 00039: val_auroc did not improve from 0.86156\n",
      "Epoch 40/50\n",
      "781990/781990 [==============================] - 152s 194us/step - loss: 0.3542 - auroc: 0.8751 - val_loss: 0.4068 - val_auroc: 0.8601\n",
      "\n",
      "Epoch 00040: val_auroc did not improve from 0.86156\n",
      "Epoch 41/50\n",
      "781990/781990 [==============================] - 139s 177us/step - loss: 0.3540 - auroc: 0.8751 - val_loss: 0.4037 - val_auroc: 0.8614\n",
      "\n",
      "Epoch 00041: val_auroc did not improve from 0.86156\n",
      "Epoch 42/50\n",
      "781990/781990 [==============================] - 153s 195us/step - loss: 0.3539 - auroc: 0.8754 - val_loss: 0.4077 - val_auroc: 0.8594\n",
      "\n",
      "Epoch 00042: val_auroc did not improve from 0.86156\n",
      "Epoch 43/50\n",
      "781990/781990 [==============================] - 137s 176us/step - loss: 0.3539 - auroc: 0.8755 - val_loss: 0.4034 - val_auroc: 0.8610\n",
      "\n",
      "Epoch 00043: val_auroc did not improve from 0.86156\n",
      "Epoch 44/50\n",
      "781990/781990 [==============================] - 151s 194us/step - loss: 0.3537 - auroc: 0.8756 - val_loss: 0.4032 - val_auroc: 0.8603\n",
      "\n",
      "Epoch 00044: val_auroc did not improve from 0.86156\n",
      "Epoch 45/50\n",
      "781990/781990 [==============================] - 136s 174us/step - loss: 0.3534 - auroc: 0.8759 - val_loss: 0.4028 - val_auroc: 0.8607\n",
      "\n",
      "Epoch 00045: val_auroc did not improve from 0.86156\n",
      "Epoch 46/50\n",
      "781990/781990 [==============================] - 130s 166us/step - loss: 0.3532 - auroc: 0.8761 - val_loss: 0.4046 - val_auroc: 0.8604\n",
      "\n",
      "Epoch 00046: val_auroc did not improve from 0.86156\n",
      "Epoch 47/50\n",
      "781990/781990 [==============================] - 130s 166us/step - loss: 0.3532 - auroc: 0.8762 - val_loss: 0.4040 - val_auroc: 0.8612\n",
      "\n",
      "Epoch 00047: val_auroc did not improve from 0.86156\n",
      "Epoch 48/50\n",
      "781990/781990 [==============================] - 149s 191us/step - loss: 0.3531 - auroc: 0.8762 - val_loss: 0.4022 - val_auroc: 0.8616\n",
      "\n",
      "Epoch 00048: val_auroc improved from 0.86156 to 0.86161, saving model to ./baseline2_nn_attention_0902.h5\n",
      "Epoch 49/50\n",
      "781990/781990 [==============================] - 184s 235us/step - loss: 0.3529 - auroc: 0.8764 - val_loss: 0.4030 - val_auroc: 0.8613\n",
      "\n",
      "Epoch 00049: val_auroc did not improve from 0.86161\n",
      "Epoch 50/50\n",
      "781990/781990 [==============================] - 184s 235us/step - loss: 0.3530 - auroc: 0.8764 - val_loss: 0.4039 - val_auroc: 0.8606\n",
      "\n",
      "Epoch 00050: val_auroc did not improve from 0.86161\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "update_model_refresh('./baseline2_nn_attention_0902.h5',\n",
    "                     train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, \n",
    "                     baseline_feature_top_train_norm,train_label, \n",
    "                     test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix, \n",
    "                     baseline_feature_top_test_norm,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1: 0.8622 \n",
    "2: attention ： 0.86236\n",
    "3: attention - norepay :0.86108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gru to encode loan seq...\n",
      "use gru to encode repay seq...\n",
      "use gru to encode event seq...\n",
      "Tensor(\"event_gru/TensorArrayReadV3:0\", shape=(?, 32), dtype=float32)\n",
      "WQ.shape (?, 2, 32)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (?, 32, 2)\n",
      "QK.shape (?, 2, 2)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "loan_seq_input (InputLayer)     (None, 10, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_seq_input (InputLayer)    (None, 20, 55)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "baseline_top_inputs (InputLayer (None, 194)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loan_gru (GRU)                  (None, 32)           5664        loan_seq_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "event_gru (GRU)                 (None, 32)           8448        event_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 128)          24960       baseline_top_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_concatenate (Concatenat (None, 64)           0           loan_gru[0][0]                   \n",
      "                                                                 event_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 32)        0           encoder_concatenate[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_cross_seq_1 (Se (None, 2, 32)        3072        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_encoder (Flatten)       (None, 64)           0           self__attention_cross_seq_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "comine_input (Concatenate)      (None, 128)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fc_output (Dense)               (None, 1)            129         comine_input[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 50,529\n",
      "Trainable params: 50,529\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "combine_model=Demand_model_attention_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gru to encode loan seq...\n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "use gru to encode repay seq...\n",
      "use gru to encode event seq...\n",
      "Tensor(\"event_gru/TensorArrayReadV3:0\", shape=(?, 32), dtype=float32)\n",
      "WQ.shape (?, 2, 32)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (?, 32, 2)\n",
      "QK.shape (?, 2, 2)\n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "loan_seq_input (InputLayer)     (None, 10, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_seq_input (InputLayer)    (None, 20, 55)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "baseline_top_inputs (InputLayer (None, 194)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loan_gru (GRU)                  (None, 32)           5664        loan_seq_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "event_gru (GRU)                 (None, 32)           8448        event_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 128)          24960       baseline_top_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_concatenate (Concatenat (None, 64)           0           loan_gru[0][0]                   \n",
      "                                                                 event_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 32)        0           encoder_concatenate[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_cross_seq_1 (Se (None, 2, 32)        3072        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_encoder (Flatten)       (None, 64)           0           self__attention_cross_seq_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "comine_input (Concatenate)      (None, 128)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fc_output (Dense)               (None, 1)            129         comine_input[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 50,529\n",
      "Trainable params: 50,529\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "-------------------model evaluatie------------------\n",
      "evaluate on train data:\n",
      "0.8844790844947544 0.5966831968957278\n",
      "evaluate on test data:\n",
      "0.8623637304676147 0.5550165084524823\n",
      "evaluate on test1 data:\n",
      "0.8688129331080892 0.5692838315044323\n"
     ]
    }
   ],
   "source": [
    "combine_model = Demand_model_attention_structure()\n",
    "combine_model.load_weights('./baseline2_nn_attention_0902.h5')\n",
    "\n",
    "print(\"-------------------model evaluatie------------------\")\n",
    "print(\"evaluate on train data:\")\n",
    "pred_train = combine_model.predict([train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm])\n",
    "train_auc = auc(pred_train[:,0], train_label)\n",
    "train_ks = ks(train_label, pred_train[:,0])\n",
    "print(train_auc, train_ks)\n",
    "\n",
    "print(\"evaluate on test data:\")\n",
    "pred_test = combine_model.predict([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm])\n",
    "test_auc = auc(pred_test[:,0], test_label)\n",
    "test_ks = ks(test_label, pred_test[:,0])\n",
    "print(test_auc, test_ks)\n",
    "\n",
    "print(\"evaluate on test1 data:\")\n",
    "pred_test1 = combine_model.predict([test1_loan_seq_matrix, test1_repay_seq_matrix, test1_event_seq_matrix,baseline_feature_top_test1_norm])\n",
    "test1_auc = auc(pred_test1[:,0], test1_label)\n",
    "test1_ks = ks(test1_label, pred_test1[:,0])\n",
    "print(test1_auc, test1_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plot_model(combine_model_1, 'model2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
