{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import keras \n",
    "\n",
    "optimizers = keras.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyFlatten(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MyFlatten, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask==None:\n",
    "            return mask\n",
    "        return K.batch_flatten(mask)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return K.batch_flatten(inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], np.prod(input_shape[1:]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MyMeanPool(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        super(MyMeanPool, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # need not to pass the mask to next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            if K.ndim(x)!=K.ndim(mask):\n",
    "                mask = K.repeat(mask, x.shape[-1])\n",
    "                mask = tf.transpose(mask, [0,2,1])\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=self.axis) / K.sum(mask, axis=self.axis)\n",
    "        else:\n",
    "            return K.mean(x, axis=self.axis)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = []\n",
    "        for i in range(len(input_shape)):\n",
    "            if i!=self.axis:\n",
    "                output_shape.append(input_shape[i])\n",
    "        return tuple(output_shape)\n",
    "    \n",
    "\n",
    "class MySumLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        super(MySumLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask (batch, time)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            if K.ndim(x)!=K.ndim(mask):\n",
    "                mask = K.repeat(mask, x.shape[-1])\n",
    "                mask = tf.transpose(mask, [0,2,1])\n",
    "            x = x * mask\n",
    "            if K.ndim(x)==2:\n",
    "                x = K.expand_dims(x)\n",
    "            return K.sum(x, axis=self.axis)\n",
    "        else:\n",
    "            if K.ndim(x)==2:\n",
    "                x = K.expand_dims(x)\n",
    "            return K.sum(x, axis=self.axis)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = []\n",
    "        for i in range(len(input_shape)):\n",
    "            if i!=self.axis:\n",
    "                output_shape.append(input_shape[i])\n",
    "        if len(output_shape)==1:\n",
    "            output_shape.append(1)\n",
    "        return tuple(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# numeric fields\n",
    "USER_DIM=20\n",
    "ACCOUNT_DIM=40\n",
    "\n",
    "user_inputs = Input(shape=[USER_DIM], name=\"user\") # None*USER_DIM\n",
    "account_inputs = Input(shape=[ACCOUNT_DIM], name=\"account\") # None*ACCOUNT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'user:0' shape=(?, 20) dtype=float32>,\n",
       " <tf.Tensor 'account:0' shape=(?, 40) dtype=float32>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_inputs, account_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "user_inputs_ = np.random.randint(0,2, (5120, USER_DIM))\n",
    "account_inputs_ = np.random.randint(0,40, (5120, ACCOUNT_DIM))\n",
    "label_ = np.random.randint(0,2, (5120, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5120, 20), (5120, 40), (5120, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_inputs_.shape, account_inputs_.shape, label_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# y_first_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 20), dtype=float32) Tensor(\"reshape_2/Reshape:0\", shape=(?, 40), dtype=float32)\n",
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 60), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# First Order Embeddings \n",
    "# dense_user = Dense(20)(user_inputs) # None*20\n",
    "# dense_account = Dense(40)(account_inputs) # None*40\n",
    "# dense_user, dense_account\n",
    "\n",
    "emb_user = Reshape([USER_DIM])(Embedding(USER_DIM, 1)(user_inputs)) # None*1, \n",
    "emb_account = Reshape([ACCOUNT_DIM])(Embedding(ACCOUNT_DIM, 1)(account_inputs)) # None*1\n",
    "print(emb_user, emb_account)\n",
    "\n",
    "y_first_order = Concatenate(axis=1)([emb_user, emb_account])\n",
    "print(y_first_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# y_second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_3/embedding_lookup/Identity:0\", shape=(?, 20, 8), dtype=float32) Tensor(\"embedding_4/embedding_lookup/Identity:0\", shape=(?, 40, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "latent = 8\n",
    "'''Second Order Embeddings'''\n",
    "emb_user_2 = Embedding(USER_DIM, latent)(user_inputs) # None * USER_DIM * K\n",
    "emb_account_2 = Embedding(ACCOUNT_DIM, latent)(account_inputs) # None * ACCOUNT_DIM * K\n",
    "print(emb_user_2, emb_account_2)\n",
    "\n",
    "# emb_user_2 = RepeatVector(1)(MyMeanPool(axis=1)(emb_user_2))\n",
    "# emb_account_2 = RepeatVector(1)(MyMeanPool(axis=1)(emb_account_2))\n",
    "# print(emb_user_2, emb_account_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 60, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "emb = Concatenate(axis=1)([emb_user_2,emb_account_2]) # None * (USER_DIM+ ACCOUNT_DIM) * K\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"multiply_1/mul:0\", shape=(?, 8), dtype=float32) Tensor(\"my_sum_layer_2/Sum:0\", shape=(?, 8), dtype=float32)\n",
      "Tensor(\"lambda_1/mul:0\", shape=(?, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''compute FM part'''\n",
    "summed_features_emb = MySumLayer(axis=1)(emb) # None * K\n",
    "summed_features_emb_square = Multiply()([summed_features_emb,summed_features_emb]) # None * K\n",
    "\n",
    "squared_features_emb = Multiply()([emb, emb]) # None * 9 * K\n",
    "squared_sum_features_emb = MySumLayer(axis=1)(squared_features_emb) # Non * K\n",
    "print(summed_features_emb_square, squared_sum_features_emb)\n",
    "\n",
    "sub = Subtract()([summed_features_emb_square, squared_sum_features_emb]) # None * K\n",
    "sub = Lambda(lambda x:x*0.5)(sub) # None * K\n",
    "print(sub)\n",
    "y_second_order = MySumLayer(axis=1)(sub) # None * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'my_sum_layer_3/Sum:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_second_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# deep parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "'''deep parts'''\n",
    "y_deep = MyFlatten()(emb) # None*(6*K)\n",
    "y_deep = Dropout(0.5)(Dense(128, activation='relu')(y_deep))\n",
    "y_deep = Dropout(0.5)(Dense(64, activation='relu')(y_deep))\n",
    "y_deep = Dropout(0.5)(Dense(32, activation='relu')(y_deep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_3/cond/Merge:0' shape=(?, 32) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# concat - deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "'''deepFM'''\n",
    "y = Concatenate(axis=1)([y_first_order, y_second_order, y_deep])\n",
    "y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "model = Model(inputs=[user_inputs, account_inputs], outputs=[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user (InputLayer)               (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "account (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 8)        160         user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 40, 8)        320         account[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 60, 8)        0           embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "my_flatten_1 (MyFlatten)        (None, 480)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          61568       my_flatten_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_1 (MySumLayer)     (None, 8)            0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 60, 8)        0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           my_sum_layer_1[0][0]             \n",
      "                                                                 my_sum_layer_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_2 (MySumLayer)     (None, 8)            0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 1)        20          user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 1)        40          account[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 8)            0           multiply_1[0][0]                 \n",
      "                                                                 my_sum_layer_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 20)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 40)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 8)            0           subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           2080        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60)           0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_3 (MySumLayer)     (None, 1)            0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 93)           0           concatenate_1[0][0]              \n",
      "                                                                 my_sum_layer_3[0][0]             \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            94          concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 72,538\n",
      "Trainable params: 72,538\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer = adam, loss = 'binary_crossentropy', sample_weight_mode = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5120 samples, validate on 5120 samples\n",
      "Epoch 1/100\n",
      "5120/5120 [==============================] - 1s 103us/step - loss: 0.0305 - val_loss: 3.8589\n",
      "Epoch 2/100\n",
      "5120/5120 [==============================] - 1s 110us/step - loss: 0.0225 - val_loss: 3.5935\n",
      "Epoch 3/100\n",
      "5120/5120 [==============================] - 1s 106us/step - loss: 0.0261 - val_loss: 3.6186\n",
      "Epoch 4/100\n",
      "5120/5120 [==============================] - 1s 106us/step - loss: 0.0276 - val_loss: 3.7094\n",
      "Epoch 5/100\n",
      "5120/5120 [==============================] - 1s 132us/step - loss: 0.0250 - val_loss: 3.5333\n",
      "Epoch 6/100\n",
      "5120/5120 [==============================] - 1s 112us/step - loss: 0.0305 - val_loss: 3.5763\n",
      "Epoch 7/100\n",
      "5120/5120 [==============================] - 1s 114us/step - loss: 0.0253 - val_loss: 3.7103\n",
      "Epoch 8/100\n",
      "5120/5120 [==============================] - 0s 94us/step - loss: 0.0304 - val_loss: 3.5706\n",
      "Epoch 9/100\n",
      "5120/5120 [==============================] - 1s 119us/step - loss: 0.0354 - val_loss: 3.5254\n",
      "Epoch 10/100\n",
      "5120/5120 [==============================] - 1s 106us/step - loss: 0.0290 - val_loss: 3.5903\n",
      "Epoch 11/100\n",
      "5120/5120 [==============================] - 0s 95us/step - loss: 0.0218 - val_loss: 3.5130\n",
      "Epoch 12/100\n",
      "5120/5120 [==============================] - 0s 87us/step - loss: 0.0245 - val_loss: 3.7194\n",
      "Epoch 13/100\n",
      "5120/5120 [==============================] - 1s 118us/step - loss: 0.0243 - val_loss: 3.6682\n",
      "Epoch 14/100\n",
      "5120/5120 [==============================] - 1s 108us/step - loss: 0.0238 - val_loss: 4.0312\n",
      "Epoch 15/100\n",
      "5120/5120 [==============================] - 1s 121us/step - loss: 0.0246 - val_loss: 3.7125\n",
      "Epoch 16/100\n",
      "5120/5120 [==============================] - 1s 113us/step - loss: 0.0236 - val_loss: 3.6915\n",
      "Epoch 17/100\n",
      "5120/5120 [==============================] - 1s 102us/step - loss: 0.0236 - val_loss: 3.8395\n",
      "Epoch 18/100\n",
      "5120/5120 [==============================] - 1s 117us/step - loss: 0.0319 - val_loss: 3.7448\n",
      "Epoch 19/100\n",
      "5120/5120 [==============================] - 0s 95us/step - loss: 0.0264 - val_loss: 3.7205\n",
      "Epoch 20/100\n",
      "5120/5120 [==============================] - 1s 110us/step - loss: 0.0226 - val_loss: 3.8157\n",
      "Epoch 21/100\n",
      "5120/5120 [==============================] - 1s 102us/step - loss: 0.0209 - val_loss: 3.9275\n",
      "Epoch 22/100\n",
      "5120/5120 [==============================] - 1s 116us/step - loss: 0.0249 - val_loss: 3.9495\n",
      "Epoch 23/100\n",
      "5120/5120 [==============================] - 1s 135us/step - loss: 0.0236 - val_loss: 3.7237\n",
      "Epoch 24/100\n",
      "5120/5120 [==============================] - 1s 117us/step - loss: 0.0284 - val_loss: 3.9194\n",
      "Epoch 25/100\n",
      "5120/5120 [==============================] - 1s 119us/step - loss: 0.0305 - val_loss: 3.7351\n",
      "Epoch 26/100\n",
      "5120/5120 [==============================] - 1s 116us/step - loss: 0.0260 - val_loss: 3.8027\n",
      "Epoch 27/100\n",
      "5120/5120 [==============================] - 1s 125us/step - loss: 0.0278 - val_loss: 3.7834\n",
      "Epoch 28/100\n",
      "5120/5120 [==============================] - 1s 108us/step - loss: 0.0309 - val_loss: 3.8488\n",
      "Epoch 29/100\n",
      "5120/5120 [==============================] - 1s 120us/step - loss: 0.0269 - val_loss: 3.7613\n",
      "Epoch 30/100\n",
      "5120/5120 [==============================] - 1s 132us/step - loss: 0.0249 - val_loss: 3.7134\n",
      "Epoch 31/100\n",
      "5120/5120 [==============================] - 1s 121us/step - loss: 0.0276 - val_loss: 3.7633\n",
      "Epoch 32/100\n",
      "5120/5120 [==============================] - 1s 124us/step - loss: 0.0216 - val_loss: 3.6523\n",
      "Epoch 33/100\n",
      "5120/5120 [==============================] - 1s 106us/step - loss: 0.0225 - val_loss: 3.8540\n",
      "Epoch 34/100\n",
      "5120/5120 [==============================] - ETA: 0s - loss: 0.020 - 1s 123us/step - loss: 0.0215 - val_loss: 3.8882\n",
      "Epoch 35/100\n",
      "5120/5120 [==============================] - 1s 111us/step - loss: 0.0207 - val_loss: 3.9191\n",
      "Epoch 36/100\n",
      "5120/5120 [==============================] - 1s 100us/step - loss: 0.0265 - val_loss: 3.7602\n",
      "Epoch 37/100\n",
      "5120/5120 [==============================] - 1s 115us/step - loss: 0.0242 - val_loss: 3.6228\n",
      "Epoch 38/100\n",
      "5120/5120 [==============================] - 1s 100us/step - loss: 0.0265 - val_loss: 3.8772\n",
      "Epoch 39/100\n",
      "5120/5120 [==============================] - 1s 118us/step - loss: 0.0284 - val_loss: 3.9098\n",
      "Epoch 40/100\n",
      "5120/5120 [==============================] - 1s 122us/step - loss: 0.0163 - val_loss: 3.9914\n",
      "Epoch 41/100\n",
      "5120/5120 [==============================] - 1s 100us/step - loss: 0.0243 - val_loss: 3.8245\n",
      "Epoch 42/100\n",
      "5120/5120 [==============================] - 1s 107us/step - loss: 0.0229 - val_loss: 4.1707\n",
      "Epoch 43/100\n",
      "5120/5120 [==============================] - 0s 89us/step - loss: 0.0259 - val_loss: 3.8580\n",
      "Epoch 44/100\n",
      "5120/5120 [==============================] - 1s 109us/step - loss: 0.0230 - val_loss: 3.9368\n",
      "Epoch 45/100\n",
      "5120/5120 [==============================] - 0s 90us/step - loss: 0.0255 - val_loss: 3.9166\n",
      "Epoch 46/100\n",
      "5120/5120 [==============================] - 1s 113us/step - loss: 0.0246 - val_loss: 4.0652\n",
      "Epoch 47/100\n",
      "5120/5120 [==============================] - 1s 115us/step - loss: 0.0199 - val_loss: 3.7680\n",
      "Epoch 48/100\n",
      "5120/5120 [==============================] - 1s 105us/step - loss: 0.0241 - val_loss: 3.9395\n",
      "Epoch 49/100\n",
      "5120/5120 [==============================] - 1s 116us/step - loss: 0.0200 - val_loss: 4.0520\n",
      "Epoch 50/100\n",
      "5120/5120 [==============================] - 1s 108us/step - loss: 0.0255 - val_loss: 3.9206\n",
      "Epoch 51/100\n",
      "5120/5120 [==============================] - 1s 118us/step - loss: 0.0222 - val_loss: 3.9378\n",
      "Epoch 52/100\n",
      "5120/5120 [==============================] - 1s 115us/step - loss: 0.0287 - val_loss: 4.0771\n",
      "Epoch 53/100\n",
      "5120/5120 [==============================] - 1s 102us/step - loss: 0.0316 - val_loss: 3.8502\n",
      "Epoch 54/100\n",
      "5120/5120 [==============================] - 1s 116us/step - loss: 0.0216 - val_loss: 3.9041\n",
      "Epoch 55/100\n",
      "5120/5120 [==============================] - 1s 105us/step - loss: 0.0251 - val_loss: 4.3341\n",
      "Epoch 56/100\n",
      "5120/5120 [==============================] - 1s 104us/step - loss: 0.0238 - val_loss: 3.9169\n",
      "Epoch 57/100\n",
      "5120/5120 [==============================] - 1s 106us/step - loss: 0.0282 - val_loss: 4.0804\n",
      "Epoch 58/100\n",
      "5120/5120 [==============================] - 0s 75us/step - loss: 0.0303 - val_loss: 4.0374\n",
      "Epoch 59/100\n",
      "5120/5120 [==============================] - 0s 78us/step - loss: 0.0160 - val_loss: 3.9063\n",
      "Epoch 60/100\n",
      "5120/5120 [==============================] - 0s 97us/step - loss: 0.0207 - val_loss: 3.9634\n",
      "Epoch 61/100\n",
      "5120/5120 [==============================] - 1s 113us/step - loss: 0.0270 - val_loss: 4.0199\n",
      "Epoch 62/100\n",
      "5120/5120 [==============================] - 1s 108us/step - loss: 0.0227 - val_loss: 4.2808\n",
      "Epoch 63/100\n",
      "5120/5120 [==============================] - 0s 86us/step - loss: 0.0199 - val_loss: 3.7586\n",
      "Epoch 64/100\n",
      "5120/5120 [==============================] - 0s 78us/step - loss: 0.0209 - val_loss: 4.3964\n",
      "Epoch 65/100\n",
      "5120/5120 [==============================] - 0s 82us/step - loss: 0.0217 - val_loss: 4.0154\n",
      "Epoch 66/100\n",
      "5120/5120 [==============================] - 0s 77us/step - loss: 0.0192 - val_loss: 4.1211\n",
      "Epoch 67/100\n",
      "5120/5120 [==============================] - 0s 77us/step - loss: 0.0213 - val_loss: 4.0757\n",
      "Epoch 68/100\n",
      "5120/5120 [==============================] - 0s 79us/step - loss: 0.0183 - val_loss: 4.0631\n",
      "Epoch 69/100\n",
      "5120/5120 [==============================] - 0s 78us/step - loss: 0.0247 - val_loss: 4.0567\n",
      "Epoch 70/100\n",
      "5120/5120 [==============================] - 0s 86us/step - loss: 0.0195 - val_loss: 4.3220\n",
      "Epoch 71/100\n",
      "5120/5120 [==============================] - 0s 79us/step - loss: 0.0228 - val_loss: 4.2280\n",
      "Epoch 72/100\n",
      "5120/5120 [==============================] - 0s 71us/step - loss: 0.0226 - val_loss: 4.0501\n",
      "Epoch 73/100\n",
      "5120/5120 [==============================] - 0s 67us/step - loss: 0.0266 - val_loss: 4.1453\n",
      "Epoch 74/100\n",
      "5120/5120 [==============================] - 0s 67us/step - loss: 0.0216 - val_loss: 4.1554\n",
      "Epoch 75/100\n",
      "5120/5120 [==============================] - 0s 76us/step - loss: 0.0211 - val_loss: 4.1932\n",
      "Epoch 76/100\n",
      "5120/5120 [==============================] - 0s 72us/step - loss: 0.0274 - val_loss: 4.0130\n",
      "Epoch 77/100\n",
      "5120/5120 [==============================] - 0s 68us/step - loss: 0.0228 - val_loss: 4.3442\n",
      "Epoch 78/100\n",
      "5120/5120 [==============================] - 0s 70us/step - loss: 0.0194 - val_loss: 4.0724\n",
      "Epoch 79/100\n",
      "5120/5120 [==============================] - 0s 68us/step - loss: 0.0172 - val_loss: 4.0531\n",
      "Epoch 80/100\n",
      "5120/5120 [==============================] - 0s 77us/step - loss: 0.0255 - val_loss: 4.2429\n",
      "Epoch 81/100\n",
      "5120/5120 [==============================] - 0s 86us/step - loss: 0.0132 - val_loss: 4.3458\n",
      "Epoch 82/100\n",
      "5120/5120 [==============================] - 0s 86us/step - loss: 0.0238 - val_loss: 4.2307\n",
      "Epoch 83/100\n",
      "5120/5120 [==============================] - 0s 86us/step - loss: 0.0220 - val_loss: 4.4958\n",
      "Epoch 84/100\n",
      "5120/5120 [==============================] - 0s 70us/step - loss: 0.0238 - val_loss: 4.3358\n",
      "Epoch 85/100\n",
      "5120/5120 [==============================] - 0s 90us/step - loss: 0.0207 - val_loss: 4.1392\n",
      "Epoch 86/100\n",
      "5120/5120 [==============================] - 0s 90us/step - loss: 0.0200 - val_loss: 4.1532\n",
      "Epoch 87/100\n",
      "5120/5120 [==============================] - 0s 92us/step - loss: 0.0224 - val_loss: 4.4295\n",
      "Epoch 88/100\n",
      "5120/5120 [==============================] - 0s 87us/step - loss: 0.0212 - val_loss: 4.1105\n",
      "Epoch 89/100\n",
      "5120/5120 [==============================] - 0s 75us/step - loss: 0.0241 - val_loss: 4.3948\n",
      "Epoch 90/100\n",
      "5120/5120 [==============================] - 0s 75us/step - loss: 0.0208 - val_loss: 4.3275\n",
      "Epoch 91/100\n",
      "5120/5120 [==============================] - 0s 77us/step - loss: 0.0204 - val_loss: 4.1307\n",
      "Epoch 92/100\n",
      "5120/5120 [==============================] - 0s 97us/step - loss: 0.0214 - val_loss: 4.3789\n",
      "Epoch 93/100\n",
      "5120/5120 [==============================] - 0s 93us/step - loss: 0.0235 - val_loss: 4.4844\n",
      "Epoch 94/100\n",
      "5120/5120 [==============================] - 0s 67us/step - loss: 0.0214 - val_loss: 4.3432\n",
      "Epoch 95/100\n",
      "5120/5120 [==============================] - 0s 78us/step - loss: 0.0205 - val_loss: 4.5113\n",
      "Epoch 96/100\n",
      "5120/5120 [==============================] - 0s 81us/step - loss: 0.0231 - val_loss: 4.2570\n",
      "Epoch 97/100\n",
      "5120/5120 [==============================] - 0s 68us/step - loss: 0.0241 - val_loss: 4.6197\n",
      "Epoch 98/100\n",
      "5120/5120 [==============================] - 0s 80us/step - loss: 0.0263 - val_loss: 4.2526\n",
      "Epoch 99/100\n",
      "5120/5120 [==============================] - 1s 99us/step - loss: 0.0223 - val_loss: 4.4064\n",
      "Epoch 100/100\n",
      "5120/5120 [==============================] - 0s 97us/step - loss: 0.0227 - val_loss: 4.3640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf11a8cd50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_inputs1 = np.random.randint(0,2, (5120, USER_DIM))\n",
    "account_inputs1 = np.random.randint(0,40, (5120, ACCOUNT_DIM))\n",
    "label1 = np.random.randint(0,2, (5120, 1))\n",
    "\n",
    "model.fit(\n",
    "    [user_inputs_, account_inputs_],\n",
    "    label_,\n",
    "    epochs = 100,  # 300\n",
    "    batch_size = 512,\n",
    "    #verbose = 10,  ## 每个epoch输出一行记录\n",
    "    #validation_split = 0.25,\n",
    "    validation_data = ([user_inputs1, account_inputs1], label1),\n",
    "    shuffle = True,\n",
    "    class_weight = {0:1, 1:1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
