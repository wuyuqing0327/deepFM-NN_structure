{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rnn import dynamic_rnn\n",
    "from tensorflow.python.ops.rnn_cell import *\n",
    "# from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _linear as _Linear\n",
    "# from tensorflow.contrib.rnn.python.ops.core_rnn_cell import _Linear\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from keras import backend as K\n",
    "from tensorflow.python.ops.rnn_cell import RNNCell, GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "RNNCell = rnn_cell_impl.RNNCell\n",
    "_WEIGHTS_VARIABLE_NAME = rnn_cell_impl._WEIGHTS_VARIABLE_NAME\n",
    "_BIAS_VARIABLE_NAME = rnn_cell_impl._BIAS_VARIABLE_NAME\n",
    "\n",
    "class _Linear(object):\n",
    "    \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "\n",
    "    Args:\n",
    "    args: a 2D Tensor or a list of 2D, batch, n, Tensors.\n",
    "    output_size: int, second dimension of weight variable.\n",
    "    dtype: data type for variables.\n",
    "    build_bias: boolean, whether to build a bias variable.\n",
    "    bias_initializer: starting value to initialize the bias\n",
    "      (default is all zeros).\n",
    "    kernel_initializer: starting value to initialize the weight.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if inputs_shape is wrong.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               args,\n",
    "               output_size,\n",
    "               build_bias,\n",
    "               bias_initializer=None,\n",
    "               kernel_initializer=None):\n",
    "        self._build_bias = build_bias\n",
    "\n",
    "        if args is None or (nest.is_sequence(args) and not args):\n",
    "            raise ValueError(\"`args` must be specified\")\n",
    "        if not nest.is_sequence(args):\n",
    "            args = [args]\n",
    "            self._is_sequence = False\n",
    "        else:\n",
    "            self._is_sequence = True\n",
    "\n",
    "        # Calculate the total size of arguments on dimension 1.\n",
    "        total_arg_size = 0\n",
    "        shapes = [a.get_shape() for a in args]\n",
    "        for shape in shapes:\n",
    "            if shape.ndims != 2:\n",
    "                raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
    "            if shape.dims[1].value is None:\n",
    "                raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
    "                             \"but saw %s\" % (shape, shape[1]))\n",
    "            else:\n",
    "                total_arg_size += shape.dims[1].value\n",
    "\n",
    "        dtype = [a.dtype for a in args][0]\n",
    "\n",
    "        scope = vs.get_variable_scope()\n",
    "        with vs.variable_scope(scope) as outer_scope:\n",
    "            self._weights = vs.get_variable(\n",
    "              _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n",
    "              dtype=dtype,\n",
    "              initializer=kernel_initializer)\n",
    "            if build_bias:\n",
    "                with vs.variable_scope(outer_scope) as inner_scope:\n",
    "                    inner_scope.set_partitioner(None)\n",
    "                if bias_initializer is None:\n",
    "                    bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n",
    "                self._biases = vs.get_variable(\n",
    "                  _BIAS_VARIABLE_NAME, [output_size],\n",
    "                  dtype=dtype,\n",
    "                  initializer=bias_initializer)\n",
    "\n",
    "    def __call__(self, args):\n",
    "        if not self._is_sequence:\n",
    "            args = [args]\n",
    "\n",
    "        if len(args) == 1:\n",
    "            res = math_ops.matmul(args[0], self._weights)\n",
    "        else:\n",
    "          # Explicitly creating a one for a minor performance improvement.\n",
    "            one = constant_op.constant(1, dtype=dtypes.int32)\n",
    "            res = math_ops.matmul(array_ops.concat(args, one), self._weights)\n",
    "        if self._build_bias:\n",
    "            res = nn_ops.bias_add(res, self._biases)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class VecAttGRUCell(RNNCell):\n",
    "    \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n",
    "    Args:\n",
    "      num_units: int, The number of units in the GRU cell.\n",
    "      activation: Nonlinearity to use.  Default: `tanh`.\n",
    "      reuse: (optional) Python boolean describing whether to reuse variables\n",
    "       in an existing scope.  If not `True`, and the existing scope already has\n",
    "       the given variables, an error is raised.\n",
    "      kernel_initializer: (optional) The initializer to use for the weight and\n",
    "      projection matrices.\n",
    "      bias_initializer: (optional) The initializer to use for the bias.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_units,\n",
    "                 activation=None,\n",
    "                 reuse=None,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=None):\n",
    "        super(VecAttGRUCell, self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation or math_ops.tanh\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._bias_initializer = bias_initializer\n",
    "        self._gate_linear = None\n",
    "        self._candidate_linear = None\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, att_score):\n",
    "        return self.call(inputs, state, att_score)\n",
    "\n",
    "    def call(self, inputs, state, att_score=None):\n",
    "        \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "        if self._gate_linear is None:\n",
    "            bias_ones = self._bias_initializer\n",
    "            if self._bias_initializer is None:\n",
    "                bias_ones = init_ops.constant_initializer(\n",
    "                    1.0, dtype=inputs.dtype)\n",
    "            with vs.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "                self._gate_linear = _Linear(\n",
    "                    [inputs, state],\n",
    "                    2 * self._num_units,\n",
    "                    True,\n",
    "                    bias_initializer=bias_ones,\n",
    "                    kernel_initializer=self._kernel_initializer)\n",
    "\n",
    "        value = math_ops.sigmoid(self._gate_linear([inputs, state]))\n",
    "        #value = math_ops.sigmoid(self._gate_linear)\n",
    "        r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n",
    "\n",
    "        r_state = r * state\n",
    "        if self._candidate_linear is None:\n",
    "            with vs.variable_scope(\"candidate\"):\n",
    "                self._candidate_linear = _Linear(\n",
    "                    [inputs, r_state],\n",
    "                    self._num_units,\n",
    "                    True,\n",
    "                    bias_initializer=self._bias_initializer,\n",
    "                    kernel_initializer=self._kernel_initializer)\n",
    "#         print(self._candidate_linear)\n",
    "        c = self._activation(self._candidate_linear([inputs, r_state]))\n",
    "#         c = self._activation(self._candidate_linear)\n",
    "        u = (1.0 - att_score) * u\n",
    "        new_h = u * state + (1 - u) * c\n",
    "        return new_h, new_h\n",
    "\n",
    "    \n",
    "def din_fcn_attention(query, facts, attention_size, mask, stag='null', mode='SUM', softmax_stag=1, time_major=False, return_alphas=False, forCnn=False):\n",
    "    if isinstance(facts, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        facts = tf.concat(facts, 2)\n",
    "    if len(facts.get_shape().as_list()) == 2:\n",
    "        facts = tf.expand_dims(facts, 1)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        facts = tf.array_ops.transpose(facts, [1, 0, 2])\n",
    "    # Trainable parameters\n",
    "    mask = tf.equal(mask, tf.ones_like(mask))\n",
    "    facts_size = facts.get_shape().as_list()[-1]  # D value - hidden size of the RNN layer\n",
    "    querry_size = query.get_shape().as_list()[-1]\n",
    "    query = tf.layers.dense(query, facts_size, activation=None, name='f1' + stag)\n",
    "    query = prelu(query)\n",
    "    queries = tf.tile(query, [1, tf.shape(facts)[1]])\n",
    "    queries = tf.reshape(queries, tf.shape(facts))\n",
    "    din_all = tf.concat([queries, facts, queries-facts, queries*facts], axis=-1)\n",
    "    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att' + stag)\n",
    "    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att' + stag)\n",
    "    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att' + stag)\n",
    "    d_layer_3_all = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(facts)[1]])\n",
    "    scores = d_layer_3_all\n",
    "    # Mask\n",
    "    # key_masks = tf.sequence_mask(facts_length, tf.shape(facts)[1])   # [B, T]\n",
    "    key_masks = tf.expand_dims(mask, 1) # [B, 1, T]\n",
    "    paddings = tf.ones_like(scores) * (-2 ** 32 + 1)\n",
    "    if not forCnn:\n",
    "        scores = tf.where(key_masks, scores, paddings)  # [B, 1, T]\n",
    "\n",
    "    # Scale\n",
    "    # scores = scores / (facts.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "    # Activation\n",
    "    if softmax_stag:\n",
    "        scores = tf.nn.softmax(scores)  # [B, 1, T]\n",
    "\n",
    "    # Weighted sum\n",
    "    if mode == 'SUM':\n",
    "        output = tf.matmul(scores, facts)  # [B, 1, H]\n",
    "        # output = tf.reshape(output, [-1, tf.shape(facts)[-1]])\n",
    "    else:\n",
    "        scores = tf.reshape(scores, [-1, tf.shape(facts)[1]])\n",
    "        output = facts * tf.expand_dims(scores, -1) # tf.expand_dims(scores, -1) => [B, T, 1], facts => [B,T,H]\n",
    "        output = tf.reshape(output, tf.shape(facts)) # [B, T, H]\n",
    "    if return_alphas:\n",
    "        return output, scores\n",
    "    return output\n",
    "\n",
    "\n",
    "def prelu(_x, scope=''):\n",
    "    \"\"\"parametric ReLU activation\"\"\"\n",
    "    with tf.variable_scope(name_or_scope=scope, default_name=\"prelu\"):\n",
    "        _alpha = tf.get_variable(\"prelu_\"+scope, shape=_x.get_shape()[-1],\n",
    "                                 dtype=_x.dtype, initializer=tf.constant_initializer(0.1))\n",
    "        return tf.maximum(0.0, _x) + _alpha * tf.minimum(0.0, _x)\n",
    "    \n",
    "\n",
    "def auxiliary_net(input_x, stag='auxiliary_net', reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"aux\", reuse=reuse):\n",
    "        bn1 = tf.layers.batch_normalization(inputs=input_x, name='bn1' + stag, reuse=tf.AUTO_REUSE, training=True)\n",
    "        dnn1 = tf.layers.dense(bn1, 32, activation=None, name='f1' + stag, reuse=tf.AUTO_REUSE)\n",
    "        dnn1 = tf.nn.relu(dnn1)\n",
    "        dnn2 = tf.layers.dense(dnn1, 1, activation=None, name='f2' + stag, reuse=tf.AUTO_REUSE)\n",
    "        y_hat = tf.nn.sigmoid(dnn2)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def auxiliary_loss(h_states, click_label, mask, stag=None):\n",
    "    #h_states = tf.placeholder(tf.float32, [None, 10, 32])\n",
    "    #mask = tf.placeholder(tf.float32, [None, 10, 1])\n",
    "    # click_label = tf.placeholder(tf.float32, [None, 10, 1])\n",
    "    # 每一个 用户状态序列中，如果长度不足，则将 click_prob 中根据补零数据预测出来的部分转化成0，同时label中不足长度的部分标记为0，以此来做处理\n",
    "    click_prob = auxiliary_net(h_states) * mask\n",
    "    loss=tf.reduce_mean(-tf.reduce_mean(1.0*click_label*tf.log(click_prob) + (1-click_label)*tf.log(1-click_prob), reduction_indices=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(?, ?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH=10\n",
    "FEA_NUM=40\n",
    "HIDDEN_SIZE=32\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "hist_item_emb = tf.placeholder(tf.float32, [None, SEQ_LENGTH, FEA_NUM])\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "target_item_emb = tf.placeholder(tf.float32,[None, FEA_NUM])\n",
    "mask = tf.placeholder(tf.float32, [None, None]) # [B, T]\n",
    "click_label = tf.placeholder(tf.float32, [None, SEQ_LENGTH, 1])\n",
    "aux_mask = tf.expand_dims(mask, 2) # [B, T, 1]\n",
    "\n",
    "print(aux_mask)\n",
    "\n",
    "# RNN for history sequence\n",
    "with tf.name_scope('rnn_hist'):\n",
    "    rnn_outputs, _ = dynamic_rnn(GRUCell(HIDDEN_SIZE), \n",
    "                                 inputs=hist_item_emb,\n",
    "                                 sequence_length=seq_length, \n",
    "                                 dtype=tf.float32,\n",
    "                                 scope=\"gru_hist\")\n",
    "#     tf.summary.histogram('GRU_output', rnn_outputs)\n",
    "\n",
    "with tf.name_scope('auxiliary_net_loss'):\n",
    "    aux_loss = auxiliary_loss(rnn_outputs, click_label, aux_mask)\n",
    "\n",
    "\n",
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    att_outputs, alphas = din_fcn_attention(target_item_emb, \n",
    "                                            rnn_outputs, \n",
    "                                            HIDDEN_SIZE, \n",
    "                                            mask=mask,\n",
    "                                            softmax_stag=1, \n",
    "                                            stag='1_1', \n",
    "                                            mode='LIST', \n",
    "                                            return_alphas=True)\n",
    "    \n",
    "#     tf.summary.histogram('alpha_output', alphas)\n",
    "    \n",
    "# RNN for top history sequence\n",
    "with tf.name_scope('rnn_top'):\n",
    "    rnn_outputs2, final_state2 = dynamic_rnn(VecAttGRUCell(HIDDEN_SIZE), \n",
    "                                             inputs=rnn_outputs,\n",
    "                                             att_scores =tf.expand_dims(alphas, -1),\n",
    "                                             sequence_length=seq_length,\n",
    "                                             dtype=tf.float32,\n",
    "                                             scope=\"gru2\")\n",
    "    \n",
    "with tf.name_scope('fc'):\n",
    "    o1 = tf.layers.dense(final_state2, 16, activation=None)\n",
    "    o2 = tf.nn.relu(o1)\n",
    "    o3 = tf.layers.dense(o2, 1, activation=None)\n",
    "    pred = tf.nn.sigmoid(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnn_hist/gru_hist/transpose:0' shape=(?, 10, 32) dtype=float32>,\n",
       " <function __main__.auxiliary_loss(h_states, click_label, mask, stag=None)>,\n",
       " <tf.Tensor 'Attention_layer/Reshape_3:0' shape=(?, 10, 32) dtype=float32>,\n",
       " <tf.Tensor 'Attention_layer/Reshape_2:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'rnn_top/gru2/transpose:0' shape=(?, 10, 32) dtype=float32>,\n",
       " <tf.Tensor 'rnn_top/gru2/while/Exit_2:0' shape=(?, 32) dtype=float32>,\n",
       " <tf.Tensor 'fc/Sigmoid:0' shape=(?, 1) dtype=float32>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_outputs, auxiliary_loss, att_outputs, alphas, rnn_outputs2, final_state2, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "params = [param for param in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'gru_hist/gru_cell/gates/kernel:0' shape=(72, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'gru_hist/gru_cell/gates/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'gru_hist/gru_cell/candidate/kernel:0' shape=(72, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'gru_hist/gru_cell/candidate/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/bn1auxiliary_net/gamma:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/bn1auxiliary_net/beta:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/bn1auxiliary_net/moving_mean:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/bn1auxiliary_net/moving_variance:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/f1auxiliary_net/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/f1auxiliary_net/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/f2auxiliary_net/kernel:0' shape=(32, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'aux/f2auxiliary_net/bias:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'f11_1/kernel:0' shape=(40, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'f11_1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'prelu_:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'f1_att1_1/kernel:0' shape=(128, 80) dtype=float32_ref>,\n",
       " <tf.Variable 'f1_att1_1/bias:0' shape=(80,) dtype=float32_ref>,\n",
       " <tf.Variable 'f2_att1_1/kernel:0' shape=(80, 40) dtype=float32_ref>,\n",
       " <tf.Variable 'f2_att1_1/bias:0' shape=(40,) dtype=float32_ref>,\n",
       " <tf.Variable 'f3_att1_1/kernel:0' shape=(40, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'f3_att1_1/bias:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'gru2/gates/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'gru2/gates/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'gru2/candidate/kernel:0' shape=(64, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'gru2/candidate/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'dense/bias:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(16, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, [None, 1])\n",
    "loss_train = tf.reduce_mean(-tf.reduce_sum(1.0*y_true*tf.log(pred) + (1-y_true)*tf.log(1-pred), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_5:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_train + aux_loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, var_list = params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Adam' type=NoOp>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer()) #初始化变量    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# hist_item_emb = tf.placeholder(tf.float32, [None, SEQ_LENGTH, FEA_NUM])\n",
    "# seq_length = tf.placeholder(tf.int32, [None])\n",
    "# target_item_emb = tf.placeholder(tf.float32,[None, FEA_NUM])\n",
    "# mask = tf.placeholder(tf.float32, [None, None]) # [B, T]\n",
    "# click_label = tf.placeholder(tf.float32, [None, SEQ_LENGTH, 1])\n",
    "# aux_mask = tf.expand_dims(mask, 2) # [B, T, 1]\n",
    "\n",
    "hist_item_emb_ = np.random.rand(5120,10,40)\n",
    "seq_length_ = np.random.randint(1,10, (5120))\n",
    "target_item_emb_ = np.random.rand(5120, 40)\n",
    "mask_ = np.random.rand(5120, 10)\n",
    "click_label_ = np.random.randint(0,2, (5120, 10, 1))\n",
    "# aux_mask = tf.expand_dims(mask, 2) \n",
    "y_true_ = np.random.randint(0,2,(5120, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00059676846 0.0021557226\n",
      "0.0006191259 0.0020848073\n",
      "0.00058878335 0.0021654188\n",
      "0.0006305096 0.0020741227\n",
      "0.0006068589 0.0019889178\n",
      "0.0005889498 0.0019987815\n",
      "0.0006311506 0.0019951523\n",
      "0.0005913933 0.0019470481\n",
      "0.00056619116 0.0019450344\n",
      "0.0005399228 0.0019892529\n",
      "0.0005686432 0.0018641523\n",
      "0.0005417549 0.0019100204\n",
      "0.00055691536 0.0018468474\n",
      "0.0005571193 0.0018342842\n",
      "0.0005238411 0.0018153994\n",
      "0.0005319339 0.0018153858\n",
      "0.00054773287 0.0017534752\n",
      "0.0005331985 0.0017464008\n",
      "0.00056321593 0.0016933263\n",
      "0.0005456777 0.0017470482\n",
      "0.00052895735 0.001698118\n",
      "0.0004876079 0.0016912373\n",
      "0.0005102125 0.001669775\n",
      "0.0005172413 0.0016325644\n",
      "0.0004925204 0.0016179007\n",
      "0.00049863616 0.0016043857\n",
      "0.0004910901 0.0016223409\n",
      "0.00048507997 0.0016142877\n",
      "0.00046554144 0.0015594846\n",
      "0.0004885109 0.00155946\n",
      "0.00045252306 0.0015332955\n",
      "0.00047434625 0.0015017588\n",
      "0.00046157464 0.0015112818\n",
      "0.0004771597 0.0015021015\n",
      "0.00046482505 0.0014795323\n",
      "0.00046771168 0.001465661\n",
      "0.0004593689 0.0014384973\n",
      "0.0004598759 0.0014326426\n",
      "0.00045377272 0.0013970183\n",
      "0.00043909842 0.0014165809\n",
      "0.00044942982 0.0014133968\n",
      "0.00042722808 0.0014293179\n",
      "0.0004311551 0.0013521488\n",
      "0.0004314389 0.0013747232\n",
      "0.00045548374 0.0013677033\n",
      "0.00046298112 0.0013312891\n",
      "0.00044272366 0.001319676\n",
      "0.00044364444 0.0013204236\n",
      "0.00040130364 0.001305153\n",
      "0.0004320385 0.0012877978\n",
      "0.00042596943 0.0012959165\n",
      "0.00040683226 0.0012808846\n",
      "0.00040911557 0.0012817616\n",
      "0.00041612625 0.0012371338\n",
      "0.00037906232 0.0012336818\n",
      "0.00040818175 0.0012413395\n",
      "0.0003772264 0.0012332922\n",
      "0.00039673015 0.0011944504\n",
      "0.00038970838 0.0011962062\n",
      "0.00039506465 0.0011866754\n",
      "0.0003970527 0.0011785199\n",
      "0.00037046146 0.0011702898\n",
      "0.00039329118 0.0011639186\n",
      "0.000378771 0.0011577911\n",
      "0.00039320748 0.0011388265\n",
      "0.00036063738 0.0011314994\n",
      "0.00038365746 0.0011122721\n",
      "0.0003843216 0.0011230027\n",
      "0.00037932227 0.0010988308\n",
      "0.00033994706 0.0011295427\n",
      "0.00034361804 0.001109699\n",
      "0.00035690688 0.0010828441\n",
      "0.00038680405 0.0010768963\n",
      "0.00037261122 0.001069872\n",
      "0.00033107176 0.0010600246\n",
      "0.0003564194 0.0010461535\n",
      "0.00033968632 0.0010546005\n",
      "0.00034682758 0.001073746\n",
      "0.00032427564 0.0010378897\n",
      "0.00035121889 0.001019106\n",
      "0.00032288564 0.001034668\n",
      "0.0003558096 0.0010291167\n",
      "0.00033131693 0.0009954512\n",
      "0.00033866044 0.0010150836\n",
      "0.000348562 0.000986219\n",
      "0.00033511926 0.0009876607\n",
      "0.00032821402 0.0009795602\n",
      "0.00031792597 0.00096656213\n",
      "0.00034819014 0.00096490124\n",
      "0.0003100616 0.0009788099\n",
      "0.00032791644 0.0009569877\n",
      "0.00030397056 0.00095002854\n",
      "0.00031767235 0.0009299986\n",
      "0.0003153434 0.00093228725\n",
      "0.00033107275 0.0009223085\n",
      "0.0002896023 0.00094222435\n",
      "0.00032117136 0.00090441376\n",
      "0.00031922077 0.00090130157\n",
      "0.00033179438 0.0008880474\n",
      "0.00030095564 0.0008966114\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    \n",
    "    _ = sess.run([train_op], {\n",
    "                            hist_item_emb:hist_item_emb_,\n",
    "                            seq_length:seq_length_,\n",
    "                            target_item_emb:target_item_emb_,\n",
    "                            mask:mask_,\n",
    "                            click_label:click_label_,\n",
    "                            y_true:y_true_\n",
    "                           })\n",
    "    hist_item_emb_ = np.random.rand(5120,10,40)\n",
    "    seq_length_ = np.random.randint(1,10, (5120))\n",
    "    target_item_emb_ = np.random.rand(5120, 40)\n",
    "    mask_ = np.random.rand(5120, 10)\n",
    "    click_label_ = np.random.randint(0,1, (5120, 10, 1))\n",
    "    y_true_ = np.random.randint(0,1,(5120, 1))\n",
    "    \n",
    "    loss1, loss2 = sess.run([loss_train, aux_loss], {\n",
    "                            hist_item_emb:hist_item_emb_,\n",
    "                            seq_length:seq_length_,\n",
    "                            target_item_emb:target_item_emb_,\n",
    "                            mask:mask_,\n",
    "                            click_label:click_label_,\n",
    "                            y_true:y_true_\n",
    "                           })\n",
    "    \n",
    "    print(loss1, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
