{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import keras \n",
    "\n",
    "optimizers = keras.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "# from seq_feature_generate import seq_feature_generate\n",
    "# from seq_feature_generate_v2 import seq_feature_generate_v2\n",
    "from util_func.eval_function import auroc, cal_ks, auc, ks, fea_psi_calc\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing.data import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from util_func import TCN_V3 as TCN\n",
    "from util_func import self_attention as sf\n",
    "from util_func.look_ahead import Lookahead\n",
    "from util_func.LayerNormalization import LayerNormalization\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "load_model =  keras.models.load_model\n",
    "Sequential = keras.models.Sequential\n",
    "Dense = keras.layers.Dense\n",
    "Activation = keras.layers.Activation\n",
    "Dropout = keras.layers.Dropout\n",
    "initializers = keras.initializers\n",
    "regularizers = keras.regularizers\n",
    "optimizers = keras.optimizers\n",
    "Input = keras.layers.Input\n",
    "add = keras.layers.add\n",
    "Model = keras.models.Model\n",
    "BatchNormalization = keras.layers.BatchNormalization \n",
    "EarlyStopping =  keras.callbacks.EarlyStopping\n",
    "ModelCheckpoint = keras.callbacks.ModelCheckpoint\n",
    "K = keras.backend\n",
    "Concatenate = keras.layers.Concatenate\n",
    "Reshape = keras.layers.Reshape\n",
    "Flatten = keras.layers.Flatten\n",
    "Lambda = keras.layers.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class MyFlatten(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(MyFlatten, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask==None:\n",
    "            return mask\n",
    "        return K.batch_flatten(mask)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return K.batch_flatten(inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], np.prod(input_shape[1:]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MyMeanPool(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        super(MyMeanPool, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # need not to pass the mask to next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            if K.ndim(x)!=K.ndim(mask):\n",
    "                mask = K.repeat(mask, x.shape[-1])\n",
    "                mask = tf.transpose(mask, [0,2,1])\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=self.axis) / K.sum(mask, axis=self.axis)\n",
    "        else:\n",
    "            return K.mean(x, axis=self.axis)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = []\n",
    "        for i in range(len(input_shape)):\n",
    "            if i!=self.axis:\n",
    "                output_shape.append(input_shape[i])\n",
    "        return tuple(output_shape)\n",
    "    \n",
    "\n",
    "class MySumLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        super(MySumLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask (batch, time)\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            if K.ndim(x)!=K.ndim(mask):\n",
    "                mask = K.repeat(mask, x.shape[-1])\n",
    "                mask = tf.transpose(mask, [0,2,1])\n",
    "            x = x * mask\n",
    "            if K.ndim(x)==2:\n",
    "                x = K.expand_dims(x)\n",
    "            return K.sum(x, axis=self.axis)\n",
    "        else:\n",
    "            if K.ndim(x)==2:\n",
    "                x = K.expand_dims(x)\n",
    "            return K.sum(x, axis=self.axis)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = []\n",
    "        for i in range(len(input_shape)):\n",
    "            if i!=self.axis:\n",
    "                output_shape.append(input_shape[i])\n",
    "        if len(output_shape)==1:\n",
    "            output_shape.append(1)\n",
    "        return tuple(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/'\n",
    "train_info_path = data_dir + 'feature_model_file/train_info.csv'\n",
    "test1_info_path = data_dir + 'feature_model_file/test1_info.csv'\n",
    "test_info_path = data_dir + 'feature_model_file/test_info.csv'\n",
    "\n",
    "fm_feature_train_norm_path = data_dir + 'feature_model_file/fm_feature_train.npy'\n",
    "fm_feature_test1_norm_path = data_dir + 'feature_model_file/fm_feature_test1.npy'\n",
    "fm_feature_test_norm_path = data_dir + 'feature_model_file/fm_feature_test.npy'\n",
    "\n",
    "train_label_path = data_dir + 'feature_model_file/train_label.npy'\n",
    "test1_label_path = data_dir + 'feature_model_file/test1_label.npy'\n",
    "test_label_path = data_dir + 'feature_model_file/test_label.npy'\n",
    "\n",
    "train_loan_seq_matrix_path = data_dir + 'feature_model_file/train_loan_seq_matrix.npy'\n",
    "train_repay_seq_matrix_path = data_dir + 'feature_model_file/train_repay_seq_matrix.npy'\n",
    "train_event1_seq_matrix_path = data_dir + 'feature_model_file/train_event1_seq_matrix.npy'\n",
    "train_event2_seq_matrix_path = data_dir + 'feature_model_file/train_event2_seq_matrix.npy'\n",
    "\n",
    "test1_loan_seq_matrix_path = data_dir + 'feature_model_file/test1_loan_seq_matrix.npy'\n",
    "test1_repay_seq_matrix_path = data_dir + 'feature_model_file/test1_repay_seq_matrix.npy'\n",
    "test1_event1_seq_matrix_path = data_dir + 'feature_model_file/test1_event1_seq_matrix.npy'\n",
    "test1_event2_seq_matrix_path = data_dir + 'feature_model_file/test1_event2_seq_matrix.npy'\n",
    "\n",
    "test_loan_seq_matrix_path = data_dir + 'feature_model_file/test_loan_seq_matrix.npy'\n",
    "test_repay_seq_matrix_path = data_dir + 'feature_model_file/test_repay_seq_matrix.npy'\n",
    "test_event1_seq_matrix_path = data_dir + 'feature_model_file/test_event1_seq_matrix.npy'\n",
    "test_event2_seq_matrix_path = data_dir + 'feature_model_file/test_event2_seq_matrix.npy'\n",
    "\n",
    "stat_feature_train_norm_path = data_dir + 'feature_model_file/stat_feature_train.npy'\n",
    "stat_feature_test1_norm_path = data_dir + 'feature_model_file/stat_feature_test1.npy'\n",
    "stat_feature_test_norm_path = data_dir + 'feature_model_file/stat_feature_test.npy'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "fm_user_fins_feature_train_norm_path = data_dir + 'feature_model_file/fm_user_fins_feature_train.npy'\n",
    "fm_user_fins_feature_test1_norm_path = data_dir + 'feature_model_file/fm_user_fins_feature_test1.npy'\n",
    "fm_user_fins_feature_test_norm_path = data_dir + 'feature_model_file/fm_user_fins_feature_test.npy'\n",
    "fm_user_people_feature_train_norm_path = data_dir + 'feature_model_file/fm_user_people_feature_train.npy'\n",
    "fm_user_people_feature_test1_norm_path = data_dir + 'feature_model_file/fm_user_people_feature_test1.npy'\n",
    "fm_user_people_feature_test_norm_path = data_dir + 'feature_model_file/fm_user_people_feature_test.npy'\n",
    "fm_account_feature_train_norm_path = data_dir + 'feature_model_file/fm_account_feature_train.npy'\n",
    "fm_account_feature_test1_norm_path = data_dir + 'feature_model_file/fm_account_feature_test1.npy'\n",
    "fm_account_feature_test_norm_path = data_dir + 'feature_model_file/fm_account_feature_test.npy'\n",
    "\n",
    "new_model_pred_train_path = data_dir + 'model_file/fm_pred_train.csv'\n",
    "new_model_pred_test_path = data_dir + 'model_file/fm_pred_test.csv'\n",
    "new_model_pred_test1_path = data_dir + 'model_file/fm_pred_test1.csv'\n",
    "save_model_file = data_dir + 'model_file/fm_64_107_3_0909.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "save_pb_file_path = data_dir + 'model_file'\n",
    "save_pb_file_name = 'demand_card_model_v1_online.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fm_feature_train_norm = np.load(fm_feature_train_norm_path)\n",
    "fm_feature_test1_norm = np.load(fm_feature_test1_norm_path)\n",
    "fm_feature_test_norm = np.load(fm_feature_test_norm_path)\n",
    "\n",
    "train_event_seq_matrix = np.load(train_event2_seq_matrix_path)\n",
    "train_loan_seq_matrix = np.load(train_loan_seq_matrix_path)\n",
    "train_repay_seq_matrix = np.load(train_repay_seq_matrix_path)\n",
    "\n",
    "test_event_seq_matrix = np.load(test_event2_seq_matrix_path)\n",
    "test_loan_seq_matrix = np.load(test_loan_seq_matrix_path)\n",
    "test_repay_seq_matrix = np.load(test_repay_seq_matrix_path)\n",
    "\n",
    "test1_event_seq_matrix = np.load(test1_event2_seq_matrix_path)\n",
    "test1_loan_seq_matrix = np.load(test1_loan_seq_matrix_path)\n",
    "test1_repay_seq_matrix = np.load(test1_repay_seq_matrix_path)\n",
    "\n",
    "baseline_feature_top_train_norm = np.load(stat_feature_train_norm_path)\n",
    "baseline_feature_top_test_norm = np.load(stat_feature_test_norm_path)\n",
    "baseline_feature_top_test1_norm = np.load(stat_feature_test1_norm_path)\n",
    "\n",
    "train_label = np.load(train_label_path)\n",
    "test_label = np.load(test_label_path)\n",
    "test1_label = np.load(test1_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812883"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39216165683877463"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in test_label if x == 1])/len([x for x in test_label if x == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fm_user_fins_feature_train_norm = np.load(fm_user_fins_feature_train_norm_path)\n",
    "fm_user_fins_feature_test1_norm = np.load(fm_user_fins_feature_test1_norm_path)\n",
    "fm_user_fins_feature_test_norm = np.load(fm_user_fins_feature_test_norm_path)\n",
    "fm_user_people_feature_train_norm = np.load(fm_user_people_feature_train_norm_path)\n",
    "fm_user_people_feature_test1_norm = np.load(fm_user_people_feature_test1_norm_path)\n",
    "fm_user_people_feature_test_norm = np.load(fm_user_people_feature_test_norm_path)\n",
    "fm_account_feature_train_norm = np.load(fm_account_feature_train_norm_path)\n",
    "fm_account_feature_test1_norm = np.load(fm_account_feature_test1_norm_path)\n",
    "fm_account_feature_test_norm = np.load(fm_account_feature_test_norm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "LOAN_SEQ_LENGTH=10\n",
    "LOAN_FEATURE_NUM=26\n",
    "\n",
    "REPAY_SEQ_LENGTH=15\n",
    "REPAY_FEATURE_NUM=17\n",
    "# EVENT_ACTION_SEQ_LENGTH=30\n",
    "# EVENT_ACTION_FEATURE_NUM=2\n",
    "\n",
    "EVENT_SEQ_LENGTH=20\n",
    "EVENT_FEATURE_NUM=55\n",
    "\n",
    "STAT_FEATURE_NUM = 194\n",
    "FM_FEA_DIM=107\n",
    "\n",
    "FM_ACCOUNT_FEA_DIM=39\n",
    "FM_USER_FINS_FEA_DIM=22\n",
    "FM_USER_PEOPLE_FEA_DIM=46\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fm_user_fins_feature_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# fm structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 107), dtype=float32)\n",
      "Tensor(\"embedding_2/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_3/Sum:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fm_fea_inputs = Input(shape=[FM_FEA_DIM], name=\"fm_fea\") # None*USER_DIM\n",
    "\n",
    "''' First Order Embeddings ''' \n",
    "fm_frist_o = Reshape([FM_FEA_DIM])(Embedding(FM_FEA_DIM, 1)(fm_fea_inputs)) # None*1, \n",
    "print(fm_frist_o)\n",
    "\n",
    "latent = 8\n",
    "'''Second Order Embeddings'''\n",
    "fm_fea_inputs_ = Input(shape=[FM_FEA_DIM], name=\"fm_fea_\") # None*USER_DIM\n",
    "emb = Embedding(FM_FEA_DIM, latent)(fm_fea_inputs_) # None * USER_DIM * K\n",
    "print(emb)\n",
    "\n",
    "'''compute FM part'''\n",
    "summed_features_emb = MySumLayer(axis=1)(emb) # None * K\n",
    "summed_features_emb_square = Multiply()([summed_features_emb,summed_features_emb]) # None * K\n",
    "\n",
    "squared_features_emb = Multiply()([emb, emb]) # None * 9 * K\n",
    "squared_sum_features_emb = MySumLayer(axis=1)(squared_features_emb) # Non * K\n",
    "\n",
    "sub = Subtract()([summed_features_emb_square, squared_sum_features_emb]) # None * K\n",
    "sub = Lambda(lambda x:x*0.5)(sub) # None * K\n",
    "\n",
    "fm_second_o = MySumLayer(axis=1)(sub) # None * 1\n",
    "print(fm_second_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loan_seq_input:0\", shape=(?, 10, 26), dtype=float32)\n",
      "use gru to encode loan seq...\n",
      "use gru to encode repay seq...\n",
      "use gru to encode event seq...\n",
      "WQ.shape (?, 3, 32)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (?, 32, 3)\n",
      "QK.shape (?, 3, 3)\n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Tensor(\"fc/Relu:0\", shape=(?, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ---------- loan seq encoder ----------\n",
    "loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "print(loan_inputs)\n",
    "print('use gru to encode loan seq...')\n",
    "# use gru to encode loan seq\n",
    "l1_param, l2_param=1e-5, 1e-5\n",
    "loan_o = keras.layers.GRU( \n",
    "    input_shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM),\n",
    "    units=32, use_bias=True, \n",
    "    return_sequences=False,\n",
    "    kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "    recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "    bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "    activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "    name = 'loan_gru')(loan_inputs)\n",
    "\n",
    "# -------------- repay seq encoder -------------\n",
    "repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "print('use gru to encode repay seq...')\n",
    "# use gru to encode repay seq\n",
    "l1_param, l2_param=1e-5, 1e-5\n",
    "repay_o = keras.layers.GRU( \n",
    "            input_shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM),\n",
    "            units=32, use_bias=True, \n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "            recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "            bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "            activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "            name='repay_gru')(repay_inputs)\n",
    "\n",
    "# -------------- event seq encoder ---------------\n",
    "event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "print('use gru to encode event seq...')\n",
    "# use gru to encode repay seq\n",
    "l1_param, l2_param=1e-5, 1e-5\n",
    "event_o = keras.layers.GRU( \n",
    "            input_shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM),\n",
    "            units=32, use_bias=True, \n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "            recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "            bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "            activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "            name='event_gru')(event_inputs)\n",
    "\n",
    "\n",
    "# self attention for click, loan, repay encode vector\n",
    "combine_input_multi = keras.layers.concatenate([loan_o, repay_o, event_o], name='encoder_concatenate')\n",
    "combine_input_multi = keras.layers.Reshape((3,32), name = 'reshape')(combine_input_multi)\n",
    "seq_encoder = sf.Self_Attention_cross_seq(32,32)(combine_input_multi)\n",
    "seq_encoder = keras.layers.Flatten(name='flatten_encoder')(seq_encoder)\n",
    "\n",
    "# --------- baseline dense layer ---------\n",
    "baseline_input = keras.layers.Input(shape=(197,), name='baseline_top_inputs')\n",
    "\n",
    "baseline_o = Dense(128, activation = 'relu',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer1')(baseline_input)\n",
    "baseline_o = Dropout(0.2)(baseline_o)\n",
    "\n",
    "baseline_o = Dense(64, activation = 'relu',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer2')(baseline_o)\n",
    "baseline_o = Dropout(0.1)(baseline_o)\n",
    "\n",
    "\n",
    "# -------- combine layer ------------\n",
    "combine_o = keras.layers.concatenate([baseline_o, seq_encoder], name='comine_input')\n",
    "# -------- combine dense ------------\n",
    "combine_o = Dense(64, activation = 'relu',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc')(combine_o)\n",
    "\n",
    "print(combine_o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc/Relu:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 107), dtype=float32)\n",
      "Tensor(\"my_sum_layer_3/Sum:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(combine_o)\n",
    "print(fm_frist_o)\n",
    "print(fm_second_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "loan_seq_input (InputLayer)     (None, 10, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repay_seq_input (InputLayer)    (None, 15, 17)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_seq_input (InputLayer)    (None, 20, 55)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "baseline_top_inputs (InputLayer (None, 197)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loan_gru (GRU)                  (None, 32)           5664        loan_seq_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "repay_gru (GRU)                 (None, 32)           4800        repay_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "event_gru (GRU)                 (None, 32)           8448        event_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fm_fea_ (InputLayer)            (None, 107)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 128)          25344       baseline_top_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_concatenate (Concatenat (None, 96)           0           loan_gru[0][0]                   \n",
      "                                                                 repay_gru[0][0]                  \n",
      "                                                                 event_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 107, 8)       856         fm_fea_[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 3, 32)        0           encoder_concatenate[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_1 (MySumLayer)     (None, 8)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 107, 8)       0           embedding_2[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_cross_seq_1 (Se (None, 3, 32)        3072        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           my_sum_layer_1[0][0]             \n",
      "                                                                 my_sum_layer_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_2 (MySumLayer)     (None, 8)            0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_encoder (Flatten)       (None, 96)           0           self__attention_cross_seq_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "fm_fea (InputLayer)             (None, 107)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 8)            0           multiply_1[0][0]                 \n",
      "                                                                 my_sum_layer_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "comine_input (Concatenate)      (None, 160)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 107, 1)       107         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 8)            0           subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 64)           10304       comine_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 107)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_3 (MySumLayer)     (None, 1)            0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "comine (Concatenate)            (None, 172)          0           fc[0][0]                         \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 my_sum_layer_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fc_output (Dense)               (None, 1)            173         comine[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 67,024\n",
      "Trainable params: 67,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "combine_o = keras.layers.concatenate([combine_o, fm_frist_o, fm_second_o], name='comine')\n",
    "# -------- combine dense ------------\n",
    "o = Dense(1, activation = 'sigmoid',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_output')(combine_o)\n",
    "\n",
    "# whole global model\n",
    "combine_model = Model(inputs = [loan_inputs, repay_inputs, event_inputs, baseline_input,fm_fea_inputs,fm_fea_inputs_], outputs = o)\n",
    "print(combine_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plot_model(combine_model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 781990 samples, validate on 131122 samples\n",
      "Epoch 1/50\n",
      "781990/781990 [==============================] - 150s 192us/step - loss: 0.4455 - auroc: 0.8250 - cal_ks: 0.5222 - val_loss: 0.4404 - val_auroc: 0.8378 - val_cal_ks: 0.5404\n",
      "\n",
      "Epoch 00001: val_auroc improved from -inf to 0.83778, saving model to ./fm.h5\n",
      "Epoch 2/50\n",
      "781990/781990 [==============================] - 146s 187us/step - loss: 0.3908 - auroc: 0.8484 - cal_ks: 0.5591 - val_loss: 0.4274 - val_auroc: 0.8448 - val_cal_ks: 0.5516\n",
      "\n",
      "Epoch 00002: val_auroc improved from 0.83778 to 0.84482, saving model to ./fm.h5\n",
      "Epoch 3/50\n",
      "781990/781990 [==============================] - 156s 199us/step - loss: 0.3800 - auroc: 0.8544 - cal_ks: 0.5690 - val_loss: 0.4188 - val_auroc: 0.8487 - val_cal_ks: 0.5559\n",
      "\n",
      "Epoch 00003: val_auroc improved from 0.84482 to 0.84874, saving model to ./fm.h5\n",
      "Epoch 4/50\n",
      "781990/781990 [==============================] - 167s 214us/step - loss: 0.3740 - auroc: 0.8578 - cal_ks: 0.5746 - val_loss: 0.4148 - val_auroc: 0.8523 - val_cal_ks: 0.5617\n",
      "\n",
      "Epoch 00004: val_auroc improved from 0.84874 to 0.85228, saving model to ./fm.h5\n",
      "Epoch 5/50\n",
      "781990/781990 [==============================] - 160s 205us/step - loss: 0.3699 - auroc: 0.8604 - cal_ks: 0.5788 - val_loss: 0.4122 - val_auroc: 0.8528 - val_cal_ks: 0.5651\n",
      "\n",
      "Epoch 00005: val_auroc improved from 0.85228 to 0.85279, saving model to ./fm.h5\n",
      "Epoch 6/50\n",
      "781990/781990 [==============================] - 143s 182us/step - loss: 0.3664 - auroc: 0.8628 - cal_ks: 0.5827 - val_loss: 0.4106 - val_auroc: 0.8558 - val_cal_ks: 0.5669\n",
      "\n",
      "Epoch 00006: val_auroc improved from 0.85279 to 0.85576, saving model to ./fm.h5\n",
      "Epoch 7/50\n",
      "781990/781990 [==============================] - 142s 182us/step - loss: 0.3639 - auroc: 0.8646 - cal_ks: 0.5858 - val_loss: 0.4047 - val_auroc: 0.8571 - val_cal_ks: 0.5698\n",
      "\n",
      "Epoch 00007: val_auroc improved from 0.85576 to 0.85715, saving model to ./fm.h5\n",
      "Epoch 8/50\n",
      "781990/781990 [==============================] - 140s 179us/step - loss: 0.3619 - auroc: 0.8660 - cal_ks: 0.5881 - val_loss: 0.4060 - val_auroc: 0.8590 - val_cal_ks: 0.5722\n",
      "\n",
      "Epoch 00008: val_auroc improved from 0.85715 to 0.85904, saving model to ./fm.h5\n",
      "Epoch 9/50\n",
      "781990/781990 [==============================] - 144s 185us/step - loss: 0.3600 - auroc: 0.8674 - cal_ks: 0.5900 - val_loss: 0.4056 - val_auroc: 0.8591 - val_cal_ks: 0.5731\n",
      "\n",
      "Epoch 00009: val_auroc improved from 0.85904 to 0.85906, saving model to ./fm.h5\n",
      "Epoch 10/50\n",
      "781990/781990 [==============================] - 141s 180us/step - loss: 0.3586 - auroc: 0.8686 - cal_ks: 0.5924 - val_loss: 0.4013 - val_auroc: 0.8603 - val_cal_ks: 0.5758\n",
      "\n",
      "Epoch 00010: val_auroc improved from 0.85906 to 0.86029, saving model to ./fm.h5\n",
      "Epoch 11/50\n",
      "781990/781990 [==============================] - 150s 192us/step - loss: 0.3571 - auroc: 0.8697 - cal_ks: 0.5932 - val_loss: 0.3996 - val_auroc: 0.8605 - val_cal_ks: 0.5744\n",
      "\n",
      "Epoch 00011: val_auroc improved from 0.86029 to 0.86048, saving model to ./fm.h5\n",
      "Epoch 12/50\n",
      "781990/781990 [==============================] - 141s 180us/step - loss: 0.3560 - auroc: 0.8704 - cal_ks: 0.5954 - val_loss: 0.4025 - val_auroc: 0.8612 - val_cal_ks: 0.5762\n",
      "\n",
      "Epoch 00012: val_auroc improved from 0.86048 to 0.86122, saving model to ./fm.h5\n",
      "Epoch 13/50\n",
      "781990/781990 [==============================] - 145s 185us/step - loss: 0.3549 - auroc: 0.8714 - cal_ks: 0.5975 - val_loss: 0.4023 - val_auroc: 0.8619 - val_cal_ks: 0.5793\n",
      "\n",
      "Epoch 00013: val_auroc improved from 0.86122 to 0.86194, saving model to ./fm.h5\n",
      "Epoch 14/50\n",
      "781990/781990 [==============================] - 142s 182us/step - loss: 0.3540 - auroc: 0.8722 - cal_ks: 0.5984 - val_loss: 0.3987 - val_auroc: 0.8628 - val_cal_ks: 0.5789\n",
      "\n",
      "Epoch 00014: val_auroc improved from 0.86194 to 0.86280, saving model to ./fm.h5\n",
      "Epoch 15/50\n",
      "781990/781990 [==============================] - 142s 182us/step - loss: 0.3533 - auroc: 0.8727 - cal_ks: 0.5997 - val_loss: 0.3987 - val_auroc: 0.8627 - val_cal_ks: 0.5790\n",
      "\n",
      "Epoch 00015: val_auroc did not improve from 0.86280\n",
      "Epoch 16/50\n",
      "781990/781990 [==============================] - 150s 192us/step - loss: 0.3523 - auroc: 0.8735 - cal_ks: 0.6005 - val_loss: 0.3990 - val_auroc: 0.8621 - val_cal_ks: 0.5786\n",
      "\n",
      "Epoch 00016: val_auroc did not improve from 0.86280\n",
      "Epoch 17/50\n",
      "781990/781990 [==============================] - 147s 188us/step - loss: 0.3520 - auroc: 0.8739 - cal_ks: 0.6014 - val_loss: 0.3981 - val_auroc: 0.8629 - val_cal_ks: 0.5790\n",
      "\n",
      "Epoch 00017: val_auroc improved from 0.86280 to 0.86286, saving model to ./fm.h5\n",
      "Epoch 18/50\n",
      "781990/781990 [==============================] - 144s 184us/step - loss: 0.3511 - auroc: 0.8746 - cal_ks: 0.6028 - val_loss: 0.3977 - val_auroc: 0.8630 - val_cal_ks: 0.5805\n",
      "\n",
      "Epoch 00018: val_auroc improved from 0.86286 to 0.86303, saving model to ./fm.h5\n",
      "Epoch 19/50\n",
      "781990/781990 [==============================] - 140s 179us/step - loss: 0.3507 - auroc: 0.8749 - cal_ks: 0.6037 - val_loss: 0.3961 - val_auroc: 0.8638 - val_cal_ks: 0.5816\n",
      "\n",
      "Epoch 00019: val_auroc improved from 0.86303 to 0.86377, saving model to ./fm.h5\n",
      "Epoch 20/50\n",
      "781990/781990 [==============================] - 138s 176us/step - loss: 0.3502 - auroc: 0.8753 - cal_ks: 0.6036 - val_loss: 0.3939 - val_auroc: 0.8641 - val_cal_ks: 0.5826\n",
      "\n",
      "Epoch 00020: val_auroc improved from 0.86377 to 0.86408, saving model to ./fm.h5\n",
      "Epoch 21/50\n",
      "781990/781990 [==============================] - 145s 186us/step - loss: 0.3495 - auroc: 0.8759 - cal_ks: 0.6057 - val_loss: 0.3968 - val_auroc: 0.8634 - val_cal_ks: 0.5807\n",
      "\n",
      "Epoch 00021: val_auroc did not improve from 0.86408\n",
      "Epoch 22/50\n",
      "781990/781990 [==============================] - 149s 190us/step - loss: 0.3492 - auroc: 0.8762 - cal_ks: 0.6070 - val_loss: 0.3958 - val_auroc: 0.8638 - val_cal_ks: 0.5815\n",
      "\n",
      "Epoch 00022: val_auroc did not improve from 0.86408\n",
      "Epoch 23/50\n",
      "781990/781990 [==============================] - 143s 183us/step - loss: 0.3486 - auroc: 0.8767 - cal_ks: 0.6068 - val_loss: 0.3991 - val_auroc: 0.8640 - val_cal_ks: 0.5821\n",
      "\n",
      "Epoch 00023: val_auroc did not improve from 0.86408\n",
      "Epoch 24/50\n",
      "781990/781990 [==============================] - 144s 185us/step - loss: 0.3486 - auroc: 0.8769 - cal_ks: 0.6071 - val_loss: 0.4003 - val_auroc: 0.8644 - val_cal_ks: 0.5820\n",
      "\n",
      "Epoch 00024: val_auroc improved from 0.86408 to 0.86438, saving model to ./fm.h5\n",
      "Epoch 25/50\n",
      "781990/781990 [==============================] - 142s 182us/step - loss: 0.3479 - auroc: 0.8773 - cal_ks: 0.6082 - val_loss: 0.3953 - val_auroc: 0.8642 - val_cal_ks: 0.5826\n",
      "\n",
      "Epoch 00025: val_auroc did not improve from 0.86438\n",
      "Epoch 26/50\n",
      "781990/781990 [==============================] - 144s 185us/step - loss: 0.3473 - auroc: 0.8780 - cal_ks: 0.6100 - val_loss: 0.4005 - val_auroc: 0.8639 - val_cal_ks: 0.5821\n",
      "\n",
      "Epoch 00026: val_auroc did not improve from 0.86438\n",
      "Epoch 27/50\n",
      "781990/781990 [==============================] - 144s 184us/step - loss: 0.3473 - auroc: 0.8778 - cal_ks: 0.6090 - val_loss: 0.3966 - val_auroc: 0.8648 - val_cal_ks: 0.5842\n",
      "\n",
      "Epoch 00027: val_auroc improved from 0.86438 to 0.86483, saving model to ./fm.h5\n",
      "Epoch 28/50\n",
      "781990/781990 [==============================] - 144s 185us/step - loss: 0.3466 - auroc: 0.8785 - cal_ks: 0.6096 - val_loss: 0.3955 - val_auroc: 0.8646 - val_cal_ks: 0.5829\n",
      "\n",
      "Epoch 00028: val_auroc did not improve from 0.86483\n",
      "Epoch 29/50\n",
      "781990/781990 [==============================] - 150s 192us/step - loss: 0.3464 - auroc: 0.8789 - cal_ks: 0.6110 - val_loss: 0.3921 - val_auroc: 0.8659 - val_cal_ks: 0.5851\n",
      "\n",
      "Epoch 00029: val_auroc improved from 0.86483 to 0.86591, saving model to ./fm.h5\n",
      "Epoch 30/50\n",
      "781990/781990 [==============================] - 145s 186us/step - loss: 0.3461 - auroc: 0.8790 - cal_ks: 0.6113 - val_loss: 0.3961 - val_auroc: 0.8644 - val_cal_ks: 0.5824\n",
      "\n",
      "Epoch 00030: val_auroc did not improve from 0.86591\n",
      "Epoch 31/50\n",
      "781990/781990 [==============================] - 147s 188us/step - loss: 0.3460 - auroc: 0.8792 - cal_ks: 0.6110 - val_loss: 0.3931 - val_auroc: 0.8654 - val_cal_ks: 0.5848\n",
      "\n",
      "Epoch 00031: val_auroc did not improve from 0.86591\n",
      "Epoch 32/50\n",
      "781990/781990 [==============================] - 147s 188us/step - loss: 0.3452 - auroc: 0.8798 - cal_ks: 0.6134 - val_loss: 0.3949 - val_auroc: 0.8655 - val_cal_ks: 0.5829\n",
      "\n",
      "Epoch 00032: val_auroc did not improve from 0.86591\n",
      "Epoch 33/50\n",
      "781990/781990 [==============================] - 146s 186us/step - loss: 0.3449 - auroc: 0.8802 - cal_ks: 0.6136 - val_loss: 0.3980 - val_auroc: 0.8647 - val_cal_ks: 0.5836\n",
      "\n",
      "Epoch 00033: val_auroc did not improve from 0.86591\n",
      "Epoch 34/50\n",
      "781990/781990 [==============================] - 143s 183us/step - loss: 0.3448 - auroc: 0.8802 - cal_ks: 0.6136 - val_loss: 0.3957 - val_auroc: 0.8661 - val_cal_ks: 0.5875\n",
      "\n",
      "Epoch 00034: val_auroc improved from 0.86591 to 0.86608, saving model to ./fm.h5\n",
      "Epoch 35/50\n",
      "781990/781990 [==============================] - 149s 190us/step - loss: 0.3445 - auroc: 0.8805 - cal_ks: 0.6141 - val_loss: 0.3957 - val_auroc: 0.8653 - val_cal_ks: 0.5855\n",
      "\n",
      "Epoch 00035: val_auroc did not improve from 0.86608\n",
      "Epoch 36/50\n",
      "781990/781990 [==============================] - 144s 184us/step - loss: 0.3445 - auroc: 0.8806 - cal_ks: 0.6141 - val_loss: 0.3972 - val_auroc: 0.8640 - val_cal_ks: 0.5829\n",
      "\n",
      "Epoch 00036: val_auroc did not improve from 0.86608\n",
      "Epoch 37/50\n",
      "781990/781990 [==============================] - 147s 188us/step - loss: 0.3440 - auroc: 0.8809 - cal_ks: 0.6154 - val_loss: 0.3960 - val_auroc: 0.8658 - val_cal_ks: 0.5837\n",
      "\n",
      "Epoch 00037: val_auroc did not improve from 0.86608\n",
      "Epoch 38/50\n",
      "781990/781990 [==============================] - 153s 196us/step - loss: 0.3437 - auroc: 0.8812 - cal_ks: 0.6154 - val_loss: 0.3974 - val_auroc: 0.8655 - val_cal_ks: 0.5842\n",
      "\n",
      "Epoch 00038: val_auroc did not improve from 0.86608\n",
      "Epoch 39/50\n",
      "781990/781990 [==============================] - 148s 189us/step - loss: 0.3436 - auroc: 0.8812 - cal_ks: 0.6154 - val_loss: 0.3985 - val_auroc: 0.8658 - val_cal_ks: 0.5849\n",
      "\n",
      "Epoch 00039: val_auroc did not improve from 0.86608\n",
      "Epoch 40/50\n",
      "781990/781990 [==============================] - 151s 194us/step - loss: 0.3434 - auroc: 0.8816 - cal_ks: 0.6160 - val_loss: 0.3966 - val_auroc: 0.8660 - val_cal_ks: 0.5851\n",
      "\n",
      "Epoch 00040: val_auroc did not improve from 0.86608\n",
      "Epoch 41/50\n",
      "781990/781990 [==============================] - 147s 188us/step - loss: 0.3433 - auroc: 0.8816 - cal_ks: 0.6168 - val_loss: 0.3939 - val_auroc: 0.8663 - val_cal_ks: 0.5861\n",
      "\n",
      "Epoch 00041: val_auroc improved from 0.86608 to 0.86632, saving model to ./fm.h5\n",
      "Epoch 42/50\n",
      "781990/781990 [==============================] - 149s 190us/step - loss: 0.3431 - auroc: 0.8820 - cal_ks: 0.6168 - val_loss: 0.3945 - val_auroc: 0.8658 - val_cal_ks: 0.5850\n",
      "\n",
      "Epoch 00042: val_auroc did not improve from 0.86632\n",
      "Epoch 43/50\n",
      "781990/781990 [==============================] - 150s 192us/step - loss: 0.3431 - auroc: 0.8819 - cal_ks: 0.6172 - val_loss: 0.3955 - val_auroc: 0.8661 - val_cal_ks: 0.5870\n",
      "\n",
      "Epoch 00043: val_auroc did not improve from 0.86632\n",
      "Epoch 44/50\n",
      "781990/781990 [==============================] - 152s 195us/step - loss: 0.3427 - auroc: 0.8824 - cal_ks: 0.6180 - val_loss: 0.3960 - val_auroc: 0.8666 - val_cal_ks: 0.5871\n",
      "\n",
      "Epoch 00044: val_auroc improved from 0.86632 to 0.86660, saving model to ./fm.h5\n",
      "Epoch 45/50\n",
      "781990/781990 [==============================] - 146s 187us/step - loss: 0.3426 - auroc: 0.8822 - cal_ks: 0.6171 - val_loss: 0.3940 - val_auroc: 0.8667 - val_cal_ks: 0.5877\n",
      "\n",
      "Epoch 00045: val_auroc improved from 0.86660 to 0.86675, saving model to ./fm.h5\n",
      "Epoch 46/50\n",
      "781990/781990 [==============================] - 147s 188us/step - loss: 0.3424 - auroc: 0.8824 - cal_ks: 0.6183 - val_loss: 0.3988 - val_auroc: 0.8660 - val_cal_ks: 0.5855\n",
      "\n",
      "Epoch 00046: val_auroc did not improve from 0.86675\n",
      "Epoch 47/50\n",
      "781990/781990 [==============================] - 145s 186us/step - loss: 0.3424 - auroc: 0.8825 - cal_ks: 0.6179 - val_loss: 0.3931 - val_auroc: 0.8661 - val_cal_ks: 0.5859\n",
      "\n",
      "Epoch 00047: val_auroc did not improve from 0.86675\n",
      "Epoch 48/50\n",
      "781990/781990 [==============================] - 146s 187us/step - loss: 0.3419 - auroc: 0.8830 - cal_ks: 0.6192 - val_loss: 0.3995 - val_auroc: 0.8659 - val_cal_ks: 0.5846\n",
      "\n",
      "Epoch 00048: val_auroc did not improve from 0.86675\n",
      "Epoch 49/50\n",
      "781990/781990 [==============================] - 146s 187us/step - loss: 0.3419 - auroc: 0.8832 - cal_ks: 0.6189 - val_loss: 0.3947 - val_auroc: 0.8661 - val_cal_ks: 0.5874\n",
      "\n",
      "Epoch 00049: val_auroc did not improve from 0.86675\n",
      "Epoch 00049: early stopping\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3\n",
    "adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07,)\n",
    "look_ahead = Lookahead(adam, sync_period=5, slow_step=0.5)\n",
    "combine_model.compile(optimizer = look_ahead, loss = 'binary_crossentropy', metrics = [auroc, cal_ks], sample_weight_mode = None) \n",
    "# # Training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=2, mode='min')\n",
    "save_model_file = './fm.h5'\n",
    "checkpoint = ModelCheckpoint(save_model_file, monitor='val_auroc', verbose=1, save_best_only=True, mode='max')\n",
    "history_v1 = combine_model.fit(\n",
    "    [train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm,fm_feature_train_norm,fm_feature_train_norm],\n",
    "    train_label,\n",
    "    epochs = 50,\n",
    "    batch_size = 512,\n",
    "    #verbose = 10,  ## 每个epoch输出一行记录\n",
    "    # validation_split = 0.25,\n",
    "    validation_data = ([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm,fm_feature_test_norm,fm_feature_test_norm]\n",
    "                       , test_label),\n",
    "    shuffle = True,\n",
    "    class_weight = {0:1, 1:1},\n",
    "    initial_epoch=0,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------model evaluatie------------------\n",
      "evaluate on train data:\n",
      "0.8898516391439962 0.607055109888319\n",
      "evaluate on test data:\n",
      "0.8671795857550249 0.5622282035709534\n",
      "evaluate on test1 data:\n",
      "0.8734067887779338 0.5762020798675376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combine_model.load_weights('./fm.h5')\n",
    "\n",
    "print(\"-------------------model evaluatie------------------\")\n",
    "print(\"evaluate on train data:\")\n",
    "pred_train = combine_model.predict([train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm,fm_feature_train_norm,fm_feature_train_norm])\n",
    "train_auc = auc(pred_train[:,0], train_label)\n",
    "train_ks = ks(train_label, pred_train[:,0])\n",
    "print(train_auc, train_ks)\n",
    "\n",
    "print(\"evaluate on test data:\")\n",
    "pred_test = combine_model.predict([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm,fm_feature_test_norm,fm_feature_test_norm])\n",
    "test_auc = auc(pred_test[:,0], test_label)\n",
    "test_ks = ks(test_label, pred_test[:,0])\n",
    "print(test_auc, test_ks)\n",
    "\n",
    "print(\"evaluate on test1 data:\")\n",
    "pred_test1 = combine_model.predict([test1_loan_seq_matrix, test1_repay_seq_matrix, test1_event_seq_matrix,baseline_feature_top_test1_norm,fm_feature_test1_norm,fm_feature_test1_norm])\n",
    "test1_auc = auc(pred_test1[:,0], test1_label)\n",
    "test1_ks = ks(test1_label, pred_test1[:,0])\n",
    "print(test1_auc, test1_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "0.86675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def fm_first_order_embedding(fm_fea_inputs):\n",
    "\n",
    "#     fm_fea_inputs = Input(shape=[FM_FEA_DIM], name=\"fm_fea\") # None*USER_DIM\n",
    "\n",
    "    ''' First Order Embeddings ''' \n",
    "    emb_fm = Reshape([FM_FEA_DIM])(Embedding(FM_FEA_DIM, 1)(fm_fea_inputs)) # None*1, \n",
    "    print(emb_fm)\n",
    "    return emb_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def fm_second_order_embedding(fm_fea_inputs_):\n",
    "    latent = 8\n",
    "    '''Second Order Embeddings'''\n",
    "#     fm_fea_inputs_ = Input(shape=[FM_FEA_DIM], name=\"fm_fea_\") # None*USER_DIM\n",
    "    emb = Embedding(FM_FEA_DIM, latent)(fm_fea_inputs_) # None * USER_DIM * K\n",
    "    print(emb)\n",
    "\n",
    "    '''compute FM part'''\n",
    "    summed_features_emb = MySumLayer(axis=1)(emb) # None * K\n",
    "    summed_features_emb_square = Multiply()([summed_features_emb,summed_features_emb]) # None * K\n",
    "\n",
    "    squared_features_emb = Multiply()([emb, emb]) # None * 9 * K\n",
    "    squared_sum_features_emb = MySumLayer(axis=1)(squared_features_emb) # Non * K\n",
    "\n",
    "    sub = Subtract()([summed_features_emb_square, squared_sum_features_emb]) # None * K\n",
    "    sub = Lambda(lambda x:x*0.5)(sub) # None * K\n",
    "\n",
    "    y_second_order = MySumLayer(axis=1)(sub) # None * 1\n",
    "    print(y_second_order)\n",
    "    return y_second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def baseline_2(loan_inputs,repay_inputs,event_inputs,baseline_input):\n",
    "    # ---------- loan seq encoder ----------\n",
    "#     loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "    print(loan_inputs)\n",
    "    print('use gru to encode loan seq...')\n",
    "    # use gru to encode loan seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    loan_o = keras.layers.GRU( \n",
    "        input_shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM),\n",
    "        units=32, use_bias=True, \n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "        name = 'loan_gru')(loan_inputs)\n",
    "\n",
    "    # -------------- repay seq encoder -------------\n",
    "#     repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "    print('use gru to encode repay seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    repay_o = keras.layers.GRU( \n",
    "                input_shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM),\n",
    "                units=32, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='repay_gru')(repay_inputs)\n",
    "\n",
    "    # -------------- event seq encoder ---------------\n",
    "#     event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "    print('use gru to encode event seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    event_o = keras.layers.GRU( \n",
    "                input_shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM),\n",
    "                units=32, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='event_gru')(event_inputs)\n",
    "\n",
    "\n",
    "    # self attention for click, loan, repay encode vector\n",
    "    combine_input_multi = keras.layers.concatenate([loan_o, repay_o, event_o], name='encoder_concatenate')\n",
    "    combine_input_multi = keras.layers.Reshape((3,32), name = 'reshape')(combine_input_multi)\n",
    "    seq_encoder = sf.Self_Attention_cross_seq(32,32)(combine_input_multi)\n",
    "    seq_encoder = keras.layers.Flatten(name='flatten_encoder')(seq_encoder)\n",
    "\n",
    "    # --------- baseline dense layer ---------\n",
    "#     baseline_input = keras.layers.Input(shape=(197,), name='baseline_top_inputs')\n",
    "\n",
    "    baseline_o = Dense(128, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer1')(baseline_input)\n",
    "    baseline_o = Dropout(0.2)(baseline_o)\n",
    "\n",
    "    baseline_o = Dense(64, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer2')(baseline_o)\n",
    "    baseline_o = Dropout(0.1)(baseline_o)\n",
    "\n",
    "\n",
    "    # -------- combine layer ------------\n",
    "    combine_o = keras.layers.concatenate([baseline_o, seq_encoder], name='comine_input')\n",
    "#     # -------- combine dense ------------\n",
    "#     combine_o = Dense(64, activation = 'relu',\n",
    "#             kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "#             bias_initializer = keras.initializers.constant(value=0),\n",
    "#             kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc')(combine_o)\n",
    "\n",
    "    print(combine_o)\n",
    "    return combine_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loan_seq_input_2:0\", shape=(?, 10, 26), dtype=float32)\n",
      "use gru to encode loan seq...\n",
      "use gru to encode repay seq...\n",
      "use gru to encode event seq...\n",
      "WQ.shape (?, 3, 32)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (?, 32, 3)\n",
      "QK.shape (?, 3, 3)\n",
      "Tensor(\"comine_input_2/concat:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"reshape_3_1/Reshape:0\", shape=(?, 107), dtype=float32)\n",
      "Tensor(\"embedding_10/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_12/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_11/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_15/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_12/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_18/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_13/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_21/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_14/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_24/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_15/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_27/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_16/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_30/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_17/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_33/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_18/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_36/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"embedding_19/embedding_lookup/Identity:0\", shape=(?, 107, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_39/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "loan_seq_input (InputLayer)     (None, 10, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repay_seq_input (InputLayer)    (None, 15, 17)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_seq_input (InputLayer)    (None, 20, 55)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "baseline_top_inputs (InputLayer (None, 194)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loan_gru (GRU)                  (None, 32)           5664        loan_seq_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "repay_gru (GRU)                 (None, 32)           4800        repay_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "event_gru (GRU)                 (None, 32)           8448        event_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fm_fea (InputLayer)             (None, 107)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 128)          24960       baseline_top_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_concatenate (Concatenat (None, 96)           0           loan_gru[0][0]                   \n",
      "                                                                 repay_gru[0][0]                  \n",
      "                                                                 event_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 107, 8)       856         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 3, 32)        0           encoder_concatenate[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_10 (MySumLayer)    (None, 8)            0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 107, 8)       0           embedding_10[0][0]               \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_13 (MySumLayer)    (None, 8)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 107, 8)       0           embedding_11[0][0]               \n",
      "                                                                 embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_16 (MySumLayer)    (None, 8)            0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 107, 8)       0           embedding_12[0][0]               \n",
      "                                                                 embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_19 (MySumLayer)    (None, 8)            0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 107, 8)       0           embedding_13[0][0]               \n",
      "                                                                 embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_22 (MySumLayer)    (None, 8)            0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 107, 8)       0           embedding_14[0][0]               \n",
      "                                                                 embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_25 (MySumLayer)    (None, 8)            0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 107, 8)       0           embedding_15[0][0]               \n",
      "                                                                 embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_28 (MySumLayer)    (None, 8)            0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 107, 8)       0           embedding_16[0][0]               \n",
      "                                                                 embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_31 (MySumLayer)    (None, 8)            0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 107, 8)       0           embedding_17[0][0]               \n",
      "                                                                 embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_34 (MySumLayer)    (None, 8)            0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_24 (Multiply)          (None, 107, 8)       0           embedding_18[0][0]               \n",
      "                                                                 embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_37 (MySumLayer)    (None, 8)            0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 107, 8)       0           embedding_19[0][0]               \n",
      "                                                                 embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 64)           8256        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_cross_seq_3 (Se (None, 3, 32)        3072        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 8)            0           my_sum_layer_10[0][0]            \n",
      "                                                                 my_sum_layer_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_11 (MySumLayer)    (None, 8)            0           multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 8)            0           my_sum_layer_13[0][0]            \n",
      "                                                                 my_sum_layer_13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_14 (MySumLayer)    (None, 8)            0           multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 8)            0           my_sum_layer_16[0][0]            \n",
      "                                                                 my_sum_layer_16[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_17 (MySumLayer)    (None, 8)            0           multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 8)            0           my_sum_layer_19[0][0]            \n",
      "                                                                 my_sum_layer_19[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_20 (MySumLayer)    (None, 8)            0           multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 8)            0           my_sum_layer_22[0][0]            \n",
      "                                                                 my_sum_layer_22[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_23 (MySumLayer)    (None, 8)            0           multiply_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 8)            0           my_sum_layer_25[0][0]            \n",
      "                                                                 my_sum_layer_25[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_26 (MySumLayer)    (None, 8)            0           multiply_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 8)            0           my_sum_layer_28[0][0]            \n",
      "                                                                 my_sum_layer_28[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_29 (MySumLayer)    (None, 8)            0           multiply_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 8)            0           my_sum_layer_31[0][0]            \n",
      "                                                                 my_sum_layer_31[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_32 (MySumLayer)    (None, 8)            0           multiply_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 8)            0           my_sum_layer_34[0][0]            \n",
      "                                                                 my_sum_layer_34[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_35 (MySumLayer)    (None, 8)            0           multiply_24[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_25 (Multiply)          (None, 8)            0           my_sum_layer_37[0][0]            \n",
      "                                                                 my_sum_layer_37[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_38 (MySumLayer)    (None, 8)            0           multiply_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_encoder (Flatten)       (None, 96)           0           self__attention_cross_seq_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 107, 1)       107         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 8)            0           multiply_7[0][0]                 \n",
      "                                                                 my_sum_layer_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 8)            0           multiply_9[0][0]                 \n",
      "                                                                 my_sum_layer_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_6 (Subtract)           (None, 8)            0           multiply_11[0][0]                \n",
      "                                                                 my_sum_layer_17[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 8)            0           multiply_13[0][0]                \n",
      "                                                                 my_sum_layer_20[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_8 (Subtract)           (None, 8)            0           multiply_15[0][0]                \n",
      "                                                                 my_sum_layer_23[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_9 (Subtract)           (None, 8)            0           multiply_17[0][0]                \n",
      "                                                                 my_sum_layer_26[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (None, 8)            0           multiply_19[0][0]                \n",
      "                                                                 my_sum_layer_29[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_11 (Subtract)          (None, 8)            0           multiply_21[0][0]                \n",
      "                                                                 my_sum_layer_32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_12 (Subtract)          (None, 8)            0           multiply_23[0][0]                \n",
      "                                                                 my_sum_layer_35[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_13 (Subtract)          (None, 8)            0           multiply_25[0][0]                \n",
      "                                                                 my_sum_layer_38[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "comine_input (Concatenate)      (None, 160)          0           dropout_6[0][0]                  \n",
      "                                                                 flatten_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 107)          0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 8)            0           subtract_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 8)            0           subtract_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 8)            0           subtract_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 8)            0           subtract_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 8)            0           subtract_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 8)            0           subtract_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 8)            0           subtract_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 8)            0           subtract_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 8)            0           subtract_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 8)            0           subtract_13[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "comine (Concatenate)            (None, 267)          0           comine_input[0][0]               \n",
      "                                                                 reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_12 (MySumLayer)    (None, 1)            0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_15 (MySumLayer)    (None, 1)            0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_18 (MySumLayer)    (None, 1)            0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_21 (MySumLayer)    (None, 1)            0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_24 (MySumLayer)    (None, 1)            0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_27 (MySumLayer)    (None, 1)            0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_30 (MySumLayer)    (None, 1)            0           lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_33 (MySumLayer)    (None, 1)            0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_36 (MySumLayer)    (None, 1)            0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_39 (MySumLayer)    (None, 1)            0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "comine_2 (Concatenate)          (None, 277)          0           comine[0][0]                     \n",
      "                                                                 my_sum_layer_12[0][0]            \n",
      "                                                                 my_sum_layer_15[0][0]            \n",
      "                                                                 my_sum_layer_18[0][0]            \n",
      "                                                                 my_sum_layer_21[0][0]            \n",
      "                                                                 my_sum_layer_24[0][0]            \n",
      "                                                                 my_sum_layer_27[0][0]            \n",
      "                                                                 my_sum_layer_30[0][0]            \n",
      "                                                                 my_sum_layer_33[0][0]            \n",
      "                                                                 my_sum_layer_36[0][0]            \n",
      "                                                                 my_sum_layer_39[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 128)          35584       comine_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_output (Dense)               (None, 1)            129         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 99,580\n",
      "Trainable params: 99,580\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "baseline_input = keras.layers.Input(shape=(STAT_FEATURE_NUM,), name='baseline_top_inputs')\n",
    "fm_fea_inputs = Input(shape=[FM_FEA_DIM], name=\"fm_fea\")\n",
    "fm_fea_inputs_1 = Input(shape=[FM_FEA_DIM], name=\"fm_fea_1\")\n",
    "fm_fea_inputs_2 = Input(shape=[FM_FEA_DIM], name=\"fm_fea_2\")\n",
    "\n",
    "#model\n",
    "combine_o = baseline_2(loan_inputs,repay_inputs,event_inputs,baseline_input)\n",
    "fm_frist_o = fm_first_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_1 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_2 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_3 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_4 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_5 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_6 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_7 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_8 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_9 = fm_second_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_0 = fm_second_order_embedding(fm_fea_inputs)\n",
    "\n",
    "#combine\n",
    "combine_o_1 = keras.layers.concatenate([combine_o, fm_frist_o], name='comine')\n",
    "# o_1 = Dense(128, activation = 'relu',\n",
    "#         kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "#         bias_initializer = keras.initializers.constant(value=0),\n",
    "#         kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_1')(combine_o_1)\n",
    "\n",
    "# o_2 = Dense(64, activation = 'relu',\n",
    "#         kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "#         bias_initializer = keras.initializers.constant(value=0),\n",
    "#         kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_2')(o_1)\n",
    "\n",
    "# combine2 --\n",
    "combine_o_2 = keras.layers.concatenate([combine_o_1, fm_second_o_1,fm_second_o_2,fm_second_o_3,fm_second_o_4,fm_second_o_5,fm_second_o_6,fm_second_o_7,fm_second_o_8,fm_second_o_9,fm_second_o_0], name='comine_2')\n",
    "o_1 = Dense(128, activation = 'relu',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_1')(combine_o_2)\n",
    "o_1 = Dropout(0.1)(o_1)\n",
    "\n",
    "o = Dense(1, activation = 'sigmoid',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_output')(o_1)\n",
    "\n",
    "\n",
    "# whole global model\n",
    "combine_model = Model(inputs = [loan_inputs, repay_inputs, event_inputs, baseline_input,fm_fea_inputs], outputs = o)\n",
    "print(combine_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(combine_model, 'model2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/offline_model/fm/util_func/eval_function.py:10: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 781990 samples, validate on 131122 samples\n",
      "Epoch 1/50\n",
      "781990/781990 [==============================] - 310s 396us/step - loss: 0.4420 - auroc: 0.8249 - cal_ks: 0.5212 - val_loss: 0.4442 - val_auroc: 0.8384 - val_cal_ks: 0.5419\n",
      "\n",
      "Epoch 00001: val_auroc improved from -inf to 0.83845, saving model to ./fm_3.h5\n",
      "Epoch 2/50\n",
      "781990/781990 [==============================] - 293s 375us/step - loss: 0.3886 - auroc: 0.8496 - cal_ks: 0.5612 - val_loss: 0.4296 - val_auroc: 0.8435 - val_cal_ks: 0.5482\n",
      "\n",
      "Epoch 00002: val_auroc improved from 0.83845 to 0.84351, saving model to ./fm_3.h5\n",
      "Epoch 3/50\n",
      "781990/781990 [==============================] - 290s 370us/step - loss: 0.3790 - auroc: 0.8551 - cal_ks: 0.5706 - val_loss: 0.4184 - val_auroc: 0.8518 - val_cal_ks: 0.5616\n",
      "\n",
      "Epoch 00003: val_auroc improved from 0.84351 to 0.85185, saving model to ./fm_3.h5\n",
      "Epoch 4/50\n",
      "781990/781990 [==============================] - 285s 365us/step - loss: 0.3728 - auroc: 0.8590 - cal_ks: 0.5762 - val_loss: 0.4099 - val_auroc: 0.8532 - val_cal_ks: 0.5634\n",
      "\n",
      "Epoch 00004: val_auroc improved from 0.85185 to 0.85324, saving model to ./fm_3.h5\n",
      "Epoch 5/50\n",
      "781990/781990 [==============================] - 284s 364us/step - loss: 0.3684 - auroc: 0.8620 - cal_ks: 0.5817 - val_loss: 0.4139 - val_auroc: 0.8554 - val_cal_ks: 0.5666\n",
      "\n",
      "Epoch 00005: val_auroc improved from 0.85324 to 0.85538, saving model to ./fm_3.h5\n",
      "Epoch 6/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3651 - auroc: 0.8639 - cal_ks: 0.5842 - val_loss: 0.4101 - val_auroc: 0.8560 - val_cal_ks: 0.5687\n",
      "\n",
      "Epoch 00006: val_auroc improved from 0.85538 to 0.85600, saving model to ./fm_3.h5\n",
      "Epoch 7/50\n",
      "781990/781990 [==============================] - 284s 363us/step - loss: 0.3626 - auroc: 0.8658 - cal_ks: 0.5875 - val_loss: 0.4069 - val_auroc: 0.8585 - val_cal_ks: 0.5713\n",
      "\n",
      "Epoch 00007: val_auroc improved from 0.85600 to 0.85848, saving model to ./fm_3.h5\n",
      "Epoch 8/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3603 - auroc: 0.8673 - cal_ks: 0.5893 - val_loss: 0.4024 - val_auroc: 0.8583 - val_cal_ks: 0.5712\n",
      "\n",
      "Epoch 00008: val_auroc did not improve from 0.85848\n",
      "Epoch 9/50\n",
      "781990/781990 [==============================] - 284s 364us/step - loss: 0.3584 - auroc: 0.8688 - cal_ks: 0.5926 - val_loss: 0.4018 - val_auroc: 0.8600 - val_cal_ks: 0.5738\n",
      "\n",
      "Epoch 00009: val_auroc improved from 0.85848 to 0.86004, saving model to ./fm_3.h5\n",
      "Epoch 10/50\n",
      "781990/781990 [==============================] - 283s 362us/step - loss: 0.3572 - auroc: 0.8698 - cal_ks: 0.5939 - val_loss: 0.4009 - val_auroc: 0.8617 - val_cal_ks: 0.5778\n",
      "\n",
      "Epoch 00010: val_auroc improved from 0.86004 to 0.86173, saving model to ./fm_3.h5\n",
      "Epoch 11/50\n",
      "781990/781990 [==============================] - 284s 363us/step - loss: 0.3561 - auroc: 0.8706 - cal_ks: 0.5958 - val_loss: 0.4017 - val_auroc: 0.8600 - val_cal_ks: 0.5744\n",
      "\n",
      "Epoch 00011: val_auroc did not improve from 0.86173\n",
      "Epoch 12/50\n",
      "781990/781990 [==============================] - 284s 363us/step - loss: 0.3548 - auroc: 0.8716 - cal_ks: 0.5972 - val_loss: 0.3978 - val_auroc: 0.8625 - val_cal_ks: 0.5792\n",
      "\n",
      "Epoch 00012: val_auroc improved from 0.86173 to 0.86249, saving model to ./fm_3.h5\n",
      "Epoch 13/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3538 - auroc: 0.8725 - cal_ks: 0.5992 - val_loss: 0.3979 - val_auroc: 0.8616 - val_cal_ks: 0.5769\n",
      "\n",
      "Epoch 00013: val_auroc did not improve from 0.86249\n",
      "Epoch 14/50\n",
      "781990/781990 [==============================] - 286s 365us/step - loss: 0.3529 - auroc: 0.8731 - cal_ks: 0.6009 - val_loss: 0.3972 - val_auroc: 0.8632 - val_cal_ks: 0.5786\n",
      "\n",
      "Epoch 00014: val_auroc improved from 0.86249 to 0.86325, saving model to ./fm_3.h5\n",
      "Epoch 15/50\n",
      "781990/781990 [==============================] - 284s 363us/step - loss: 0.3522 - auroc: 0.8738 - cal_ks: 0.6011 - val_loss: 0.4022 - val_auroc: 0.8629 - val_cal_ks: 0.5790\n",
      "\n",
      "Epoch 00015: val_auroc did not improve from 0.86325\n",
      "Epoch 16/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3515 - auroc: 0.8743 - cal_ks: 0.6022 - val_loss: 0.3963 - val_auroc: 0.8632 - val_cal_ks: 0.5797\n",
      "\n",
      "Epoch 00016: val_auroc did not improve from 0.86325\n",
      "Epoch 17/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3504 - auroc: 0.8751 - cal_ks: 0.6036 - val_loss: 0.3986 - val_auroc: 0.8636 - val_cal_ks: 0.5799\n",
      "\n",
      "Epoch 00017: val_auroc improved from 0.86325 to 0.86363, saving model to ./fm_3.h5\n",
      "Epoch 18/50\n",
      "781990/781990 [==============================] - 285s 365us/step - loss: 0.3499 - auroc: 0.8756 - cal_ks: 0.6050 - val_loss: 0.4052 - val_auroc: 0.8630 - val_cal_ks: 0.5795\n",
      "\n",
      "Epoch 00018: val_auroc did not improve from 0.86363\n",
      "Epoch 19/50\n",
      "781990/781990 [==============================] - 290s 370us/step - loss: 0.3493 - auroc: 0.8760 - cal_ks: 0.6058 - val_loss: 0.3962 - val_auroc: 0.8633 - val_cal_ks: 0.5799\n",
      "\n",
      "Epoch 00019: val_auroc did not improve from 0.86363\n",
      "Epoch 20/50\n",
      "781990/781990 [==============================] - 282s 361us/step - loss: 0.3485 - auroc: 0.8768 - cal_ks: 0.6067 - val_loss: 0.3971 - val_auroc: 0.8644 - val_cal_ks: 0.5818\n",
      "\n",
      "Epoch 00020: val_auroc improved from 0.86363 to 0.86440, saving model to ./fm_3.h5\n",
      "Epoch 21/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3482 - auroc: 0.8770 - cal_ks: 0.6077 - val_loss: 0.3965 - val_auroc: 0.8646 - val_cal_ks: 0.5832\n",
      "\n",
      "Epoch 00021: val_auroc improved from 0.86440 to 0.86461, saving model to ./fm_3.h5\n",
      "Epoch 22/50\n",
      "781990/781990 [==============================] - 285s 365us/step - loss: 0.3476 - auroc: 0.8774 - cal_ks: 0.6087 - val_loss: 0.3971 - val_auroc: 0.8641 - val_cal_ks: 0.5810\n",
      "\n",
      "Epoch 00022: val_auroc did not improve from 0.86461\n",
      "Epoch 23/50\n",
      "781990/781990 [==============================] - 285s 364us/step - loss: 0.3469 - auroc: 0.8782 - cal_ks: 0.6095 - val_loss: 0.3975 - val_auroc: 0.8642 - val_cal_ks: 0.5808\n",
      "\n",
      "Epoch 00023: val_auroc did not improve from 0.86461\n",
      "Epoch 24/50\n",
      "781990/781990 [==============================] - 288s 369us/step - loss: 0.3463 - auroc: 0.8785 - cal_ks: 0.6100 - val_loss: 0.3971 - val_auroc: 0.8655 - val_cal_ks: 0.5852\n",
      "\n",
      "Epoch 00024: val_auroc improved from 0.86461 to 0.86551, saving model to ./fm_3.h5\n",
      "Epoch 25/50\n",
      "781990/781990 [==============================] - 295s 377us/step - loss: 0.3459 - auroc: 0.8790 - cal_ks: 0.6111 - val_loss: 0.3988 - val_auroc: 0.8645 - val_cal_ks: 0.5825\n",
      "\n",
      "Epoch 00025: val_auroc did not improve from 0.86551\n",
      "Epoch 26/50\n",
      "781990/781990 [==============================] - 298s 381us/step - loss: 0.3454 - auroc: 0.8794 - cal_ks: 0.6126 - val_loss: 0.4014 - val_auroc: 0.8641 - val_cal_ks: 0.5810\n",
      "\n",
      "Epoch 00026: val_auroc did not improve from 0.86551\n",
      "Epoch 27/50\n",
      "781990/781990 [==============================] - 287s 366us/step - loss: 0.3447 - auroc: 0.8799 - cal_ks: 0.6126 - val_loss: 0.3962 - val_auroc: 0.8651 - val_cal_ks: 0.5841\n",
      "\n",
      "Epoch 00027: val_auroc did not improve from 0.86551\n",
      "Epoch 28/50\n",
      "781990/781990 [==============================] - 287s 367us/step - loss: 0.3444 - auroc: 0.8802 - cal_ks: 0.6127 - val_loss: 0.3933 - val_auroc: 0.8656 - val_cal_ks: 0.5845\n",
      "\n",
      "Epoch 00028: val_auroc improved from 0.86551 to 0.86561, saving model to ./fm_3.h5\n",
      "Epoch 29/50\n",
      "781990/781990 [==============================] - 287s 367us/step - loss: 0.3440 - auroc: 0.8806 - cal_ks: 0.6147 - val_loss: 0.3938 - val_auroc: 0.8644 - val_cal_ks: 0.5820\n",
      "\n",
      "Epoch 00029: val_auroc did not improve from 0.86561\n",
      "Epoch 30/50\n",
      "781990/781990 [==============================] - 292s 373us/step - loss: 0.3436 - auroc: 0.8809 - cal_ks: 0.6146 - val_loss: 0.3980 - val_auroc: 0.8658 - val_cal_ks: 0.5846\n",
      "\n",
      "Epoch 00030: val_auroc improved from 0.86561 to 0.86576, saving model to ./fm_3.h5\n",
      "Epoch 31/50\n",
      "781990/781990 [==============================] - 293s 375us/step - loss: 0.3435 - auroc: 0.8811 - cal_ks: 0.6153 - val_loss: 0.3988 - val_auroc: 0.8657 - val_cal_ks: 0.5851\n",
      "\n",
      "Epoch 00031: val_auroc did not improve from 0.86576\n",
      "Epoch 32/50\n",
      "781990/781990 [==============================] - 291s 373us/step - loss: 0.3432 - auroc: 0.8812 - cal_ks: 0.6155 - val_loss: 0.3954 - val_auroc: 0.8659 - val_cal_ks: 0.5844\n",
      "\n",
      "Epoch 00032: val_auroc improved from 0.86576 to 0.86595, saving model to ./fm_3.h5\n",
      "Epoch 33/50\n",
      "781990/781990 [==============================] - 292s 373us/step - loss: 0.3424 - auroc: 0.8819 - cal_ks: 0.6169 - val_loss: 0.3932 - val_auroc: 0.8659 - val_cal_ks: 0.5850\n",
      "\n",
      "Epoch 00033: val_auroc did not improve from 0.86595\n",
      "Epoch 34/50\n",
      "781990/781990 [==============================] - 287s 367us/step - loss: 0.3422 - auroc: 0.8822 - cal_ks: 0.6170 - val_loss: 0.3961 - val_auroc: 0.8661 - val_cal_ks: 0.5836\n",
      "\n",
      "Epoch 00034: val_auroc improved from 0.86595 to 0.86607, saving model to ./fm_3.h5\n",
      "Epoch 35/50\n",
      "781990/781990 [==============================] - 289s 370us/step - loss: 0.3418 - auroc: 0.8823 - cal_ks: 0.6170 - val_loss: 0.3966 - val_auroc: 0.8656 - val_cal_ks: 0.5853\n",
      "\n",
      "Epoch 00035: val_auroc did not improve from 0.86607\n",
      "Epoch 36/50\n",
      "781990/781990 [==============================] - 290s 371us/step - loss: 0.3419 - auroc: 0.8825 - cal_ks: 0.6174 - val_loss: 0.3951 - val_auroc: 0.8644 - val_cal_ks: 0.5819\n",
      "\n",
      "Epoch 00036: val_auroc did not improve from 0.86607\n",
      "Epoch 37/50\n",
      "781990/781990 [==============================] - 293s 375us/step - loss: 0.3412 - auroc: 0.8831 - cal_ks: 0.6190 - val_loss: 0.3952 - val_auroc: 0.8657 - val_cal_ks: 0.5841\n",
      "\n",
      "Epoch 00037: val_auroc did not improve from 0.86607\n",
      "Epoch 38/50\n",
      "781990/781990 [==============================] - 290s 370us/step - loss: 0.3411 - auroc: 0.8832 - cal_ks: 0.6189 - val_loss: 0.3954 - val_auroc: 0.8657 - val_cal_ks: 0.5854\n",
      "\n",
      "Epoch 00038: val_auroc did not improve from 0.86607\n",
      "Epoch 39/50\n",
      "781990/781990 [==============================] - 290s 370us/step - loss: 0.3407 - auroc: 0.8835 - cal_ks: 0.6198 - val_loss: 0.3948 - val_auroc: 0.8661 - val_cal_ks: 0.5854\n",
      "\n",
      "Epoch 00039: val_auroc improved from 0.86607 to 0.86612, saving model to ./fm_3.h5\n",
      "Epoch 40/50\n",
      "781990/781990 [==============================] - 290s 371us/step - loss: 0.3405 - auroc: 0.8835 - cal_ks: 0.6196 - val_loss: 0.3933 - val_auroc: 0.8662 - val_cal_ks: 0.5840\n",
      "\n",
      "Epoch 00040: val_auroc improved from 0.86612 to 0.86615, saving model to ./fm_3.h5\n",
      "Epoch 41/50\n",
      "781990/781990 [==============================] - 289s 369us/step - loss: 0.3404 - auroc: 0.8839 - cal_ks: 0.6209 - val_loss: 0.3985 - val_auroc: 0.8659 - val_cal_ks: 0.5846\n",
      "\n",
      "Epoch 00041: val_auroc did not improve from 0.86615\n",
      "Epoch 42/50\n",
      "  8704/781990 [..............................] - ETA: 4:38 - loss: 0.3362 - auroc: 0.8869 - cal_ks: 0.6245"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3\n",
    "adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07,)\n",
    "look_ahead = Lookahead(adam, sync_period=5, slow_step=0.5)\n",
    "combine_model.compile(optimizer = look_ahead, loss = 'binary_crossentropy', metrics = [auroc, cal_ks], sample_weight_mode = None) \n",
    "# # Training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=2, mode='min')\n",
    "save_model_file='./fm_3.h5'\n",
    "filepath=save_model_file\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auroc', verbose=1, save_best_only=True, mode='max')\n",
    "history_v1 = combine_model.fit(\n",
    "    [train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm,fm_feature_train_norm],\n",
    "    train_label,\n",
    "    epochs = 50,\n",
    "    batch_size = 512,\n",
    "    #verbose = 10,  ## 每个epoch输出一行记录\n",
    "    # validation_split = 0.25,\n",
    "    validation_data = ([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm,fm_feature_test_norm]\n",
    "                       , test_label),\n",
    "    shuffle = True,\n",
    "    class_weight = {0:1, 1:1},\n",
    "    initial_epoch=0,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 10 layers into a model with 19 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2890799cce7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombine_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./fm.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------model evaluatie------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluate on train data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_loan_seq_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_repay_seq_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_event_seq_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_feature_top_train_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfm_feature_train_norm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/user/local/miniconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/user/local/miniconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 10 layers into a model with 19 layers."
     ]
    }
   ],
   "source": [
    "\n",
    "combine_model.load_weights('./fm_3.h5')\n",
    "\n",
    "print(\"-------------------model evaluatie------------------\")\n",
    "print(\"evaluate on train data:\")\n",
    "pred_train = combine_model.predict([train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm,fm_feature_train_norm])\n",
    "train_auc = auc(pred_train[:,0], train_label)\n",
    "train_ks = ks(train_label, pred_train[:,0])\n",
    "print(train_auc, train_ks)\n",
    "\n",
    "print(\"evaluate on test data:\")\n",
    "pred_test = combine_model.predict([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm,fm_feature_test_norm])\n",
    "test_auc = auc(pred_test[:,0], test_label)\n",
    "test_ks = ks(test_label, pred_test[:,0])\n",
    "print(test_auc, test_ks)\n",
    "\n",
    "print(\"evaluate on test1 data:\")\n",
    "pred_test1 = combine_model.predict([test1_loan_seq_matrix, test1_repay_seq_matrix, test1_event_seq_matrix,baseline_feature_top_test1_norm,fm_feature_test1_norm])\n",
    "test1_auc = auc(pred_test1[:,0], test1_label)\n",
    "test1_ks = ks(test1_label, pred_test1[:,0])\n",
    "print(test1_auc, test1_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873353257219281 0.5758015652595565\n"
     ]
    }
   ],
   "source": [
    "print(test1_auc, test1_ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# fm_part_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def baseline_2(loan_inputs,repay_inputs,event_inputs,baseline_input):\n",
    "    # ---------- loan seq encoder ----------\n",
    "#     loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "    print(loan_inputs)\n",
    "    print('use gru to encode loan seq...')\n",
    "    # use gru to encode loan seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    loan_o = keras.layers.GRU( \n",
    "        input_shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM),\n",
    "        units=32, use_bias=True, \n",
    "        return_sequences=False,\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "        activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "        name = 'loan_gru')(loan_inputs)\n",
    "\n",
    "    # -------------- repay seq encoder -------------\n",
    "#     repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "    print('use gru to encode repay seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    repay_o = keras.layers.GRU( \n",
    "                input_shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM),\n",
    "                units=32, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='repay_gru')(repay_inputs)\n",
    "\n",
    "    # -------------- event seq encoder ---------------\n",
    "#     event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "    print('use gru to encode event seq...')\n",
    "    # use gru to encode repay seq\n",
    "    l1_param, l2_param=1e-5, 1e-5\n",
    "    event_o = keras.layers.GRU( \n",
    "                input_shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM),\n",
    "                units=32, use_bias=True, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                recurrent_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                bias_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param), \n",
    "                activity_regularizer=keras.regularizers.l1_l2(l1=l1_param, l2=l2_param),\n",
    "                name='event_gru')(event_inputs)\n",
    "\n",
    "\n",
    "    # self attention for click, loan, repay encode vector\n",
    "    combine_input_multi = keras.layers.concatenate([loan_o, repay_o, event_o], name='encoder_concatenate')\n",
    "    combine_input_multi = keras.layers.Reshape((3,32), name = 'reshape')(combine_input_multi)\n",
    "    seq_encoder = sf.Self_Attention_cross_seq(32,32)(combine_input_multi)\n",
    "    seq_encoder = keras.layers.Flatten(name='flatten_encoder')(seq_encoder)\n",
    "\n",
    "    # --------- baseline dense layer ---------\n",
    "#     baseline_input = keras.layers.Input(shape=(STAT_FEATURE_NUM,), name='baseline_top_inputs')\n",
    "\n",
    "    baseline_o = Dense(128, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer1')(baseline_input)\n",
    "    baseline_o = Dropout(0.2)(baseline_o)\n",
    "\n",
    "    baseline_o = Dense(64, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=1e-6),name = 'layer2')(baseline_o)\n",
    "    baseline_o = Dropout(0.1)(baseline_o)\n",
    "\n",
    "\n",
    "    # -------- combine layer ------------\n",
    "    combine_o = keras.layers.concatenate([baseline_o, seq_encoder], name='comine_input')\n",
    "#     # -------- combine dense ------------\n",
    "    combine_o = Dense(64, activation = 'relu',\n",
    "            kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "            bias_initializer = keras.initializers.constant(value=0),\n",
    "            kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc')(combine_o)\n",
    "\n",
    "    print(combine_o)\n",
    "    return combine_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def fm_first_order_embedding(fm_fea_inputs):\n",
    "\n",
    "#     fm_fea_inputs = Input(shape=[FM_FEA_DIM], name=\"fm_fea\") # None*USER_DIM\n",
    "\n",
    "    ''' First Order Embeddings ''' \n",
    "    emb_fm = Reshape([FM_FEA_DIM])(Embedding(FM_FEA_DIM, 1)(fm_fea_inputs)) # None*1, \n",
    "    print(emb_fm)\n",
    "    return emb_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def fm_second_order_embedding(fm_fea_inputs_a,a_dim,fm_fea_inputs_b,b_dim):\n",
    "    latent = 8\n",
    "    '''Second Order Embeddings'''\n",
    "    emb_a = Embedding(a_dim, latent)(fm_fea_inputs_a) # None * USER_DIM * K\n",
    "    emb_b = Embedding(b_dim, latent)(fm_fea_inputs_b) # None * ACCOUNT_DIM * K\n",
    "    emb = Concatenate(axis=1)([emb_a,emb_b]) # None * (USER_DIM+ ACCOUNT_DIM) * K\n",
    "    print(emb)\n",
    "\n",
    "    '''compute FM part'''\n",
    "    summed_features_emb = MySumLayer(axis=1)(emb) # None * K\n",
    "    summed_features_emb_square = Multiply()([summed_features_emb,summed_features_emb]) # None * K\n",
    "\n",
    "    squared_features_emb = Multiply()([emb, emb]) # None * 9 * K\n",
    "    squared_sum_features_emb = MySumLayer(axis=1)(squared_features_emb) # Non * K\n",
    "\n",
    "    sub = Subtract()([summed_features_emb_square, squared_sum_features_emb]) # None * K\n",
    "    sub = Lambda(lambda x:x*0.5)(sub) # None * K\n",
    "\n",
    "    y_second_order = MySumLayer(axis=1)(sub) # None * 1\n",
    "    print(y_second_order)\n",
    "    return y_second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loan_seq_input:0\", shape=(?, 10, 26), dtype=float32)\n",
      "use gru to encode loan seq...\n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "use gru to encode repay seq...\n",
      "use gru to encode event seq...\n",
      "WQ.shape (?, 3, 32)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (?, 32, 3)\n",
      "QK.shape (?, 3, 3)\n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Tensor(\"fc/Relu:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 107), dtype=float32)\n",
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 61, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_3/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 85, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_6/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concatenate_3/concat:0\", shape=(?, 68, 8), dtype=float32)\n",
      "Tensor(\"my_sum_layer_9/Sum:0\", shape=(?, 1), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "loan_seq_input (InputLayer)     (None, 10, 26)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repay_seq_input (InputLayer)    (None, 15, 17)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_seq_input (InputLayer)    (None, 20, 55)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fm_fea_accont (InputLayer)      (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fm_fea_user_fins (InputLayer)   (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fm_fea_user_people (InputLayer) (None, 46)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "baseline_top_inputs (InputLayer (None, 194)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loan_gru (GRU)                  (None, 32)           5664        loan_seq_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "repay_gru (GRU)                 (None, 32)           4800        repay_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "event_gru (GRU)                 (None, 32)           8448        event_seq_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 39, 8)        312         fm_fea_accont[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 22, 8)        176         fm_fea_user_fins[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 39, 8)        312         fm_fea_accont[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 46, 8)        368         fm_fea_user_people[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 22, 8)        176         fm_fea_user_fins[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 46, 8)        368         fm_fea_user_people[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 128)          24960       baseline_top_inputs[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "encoder_concatenate (Concatenat (None, 96)           0           loan_gru[0][0]                   \n",
      "                                                                 repay_gru[0][0]                  \n",
      "                                                                 event_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 61, 8)        0           embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 85, 8)        0           embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 68, 8)        0           embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 3, 32)        0           encoder_concatenate[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_1 (MySumLayer)     (None, 8)            0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 61, 8)        0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_4 (MySumLayer)     (None, 8)            0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 85, 8)        0           concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_7 (MySumLayer)     (None, 8)            0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 68, 8)        0           concatenate_3[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_cross_seq_1 (Se (None, 3, 32)        3072        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           my_sum_layer_1[0][0]             \n",
      "                                                                 my_sum_layer_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_2 (MySumLayer)     (None, 8)            0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 8)            0           my_sum_layer_4[0][0]             \n",
      "                                                                 my_sum_layer_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_5 (MySumLayer)     (None, 8)            0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 8)            0           my_sum_layer_7[0][0]             \n",
      "                                                                 my_sum_layer_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_8 (MySumLayer)     (None, 8)            0           multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_encoder (Flatten)       (None, 96)           0           self__attention_cross_seq_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "fm_fea (InputLayer)             (None, 107)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 8)            0           multiply_1[0][0]                 \n",
      "                                                                 my_sum_layer_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 8)            0           multiply_3[0][0]                 \n",
      "                                                                 my_sum_layer_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 8)            0           multiply_5[0][0]                 \n",
      "                                                                 my_sum_layer_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "comine_input (Concatenate)      (None, 160)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_encoder[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 107, 1)       107         fm_fea[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 8)            0           subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 8)            0           subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 8)            0           subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 64)           10304       comine_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 107)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_3 (MySumLayer)     (None, 1)            0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_6 (MySumLayer)     (None, 1)            0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "my_sum_layer_9 (MySumLayer)     (None, 1)            0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "comine_2 (Concatenate)          (None, 174)          0           fc[0][0]                         \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 my_sum_layer_3[0][0]             \n",
      "                                                                 my_sum_layer_6[0][0]             \n",
      "                                                                 my_sum_layer_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fc_output (Dense)               (None, 1)            175         comine_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 67,498\n",
      "Trainable params: 67,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "loan_inputs = keras.layers.Input(shape=(LOAN_SEQ_LENGTH, LOAN_FEATURE_NUM), name='loan_seq_input')\n",
    "repay_inputs = keras.layers.Input(shape=(REPAY_SEQ_LENGTH, REPAY_FEATURE_NUM), name='repay_seq_input')\n",
    "event_inputs = keras.layers.Input(shape=(EVENT_SEQ_LENGTH, EVENT_FEATURE_NUM), name='event_seq_input')\n",
    "baseline_input = keras.layers.Input(shape=(STAT_FEATURE_NUM,), name='baseline_top_inputs')\n",
    "\n",
    "fm_fea_inputs = Input(shape=[FM_FEA_DIM], name=\"fm_fea\")\n",
    "fm_fea_inputs_account = Input(shape=[FM_ACCOUNT_FEA_DIM], name=\"fm_fea_accont\")\n",
    "fm_fea_inputs_user_fins = Input(shape=[FM_USER_FINS_FEA_DIM], name=\"fm_fea_user_fins\")\n",
    "fm_fea_inputs_user_people = Input(shape=[FM_USER_PEOPLE_FEA_DIM], name=\"fm_fea_user_people\")\n",
    "\n",
    "#model\n",
    "combine_o = baseline_2(loan_inputs,repay_inputs,event_inputs,baseline_input)\n",
    "fm_frist_o = fm_first_order_embedding(fm_fea_inputs)\n",
    "fm_second_o_1 = fm_second_order_embedding(fm_fea_inputs_account,FM_ACCOUNT_FEA_DIM,fm_fea_inputs_user_fins,FM_USER_FINS_FEA_DIM)\n",
    "fm_second_o_2 = fm_second_order_embedding(fm_fea_inputs_account,FM_ACCOUNT_FEA_DIM,fm_fea_inputs_user_people,FM_USER_PEOPLE_FEA_DIM)\n",
    "fm_second_o_3 = fm_second_order_embedding(fm_fea_inputs_user_fins,FM_USER_FINS_FEA_DIM,fm_fea_inputs_user_people,FM_USER_PEOPLE_FEA_DIM)\n",
    "\n",
    "\n",
    "#combine\n",
    "# combine_o_1 = keras.layers.concatenate([combine_o, fm_frist_o], name='comine')\n",
    "# o_1 = Dense(128, activation = 'relu',\n",
    "#         kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "#         bias_initializer = keras.initializers.constant(value=0),\n",
    "#         kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_1')(combine_o_1)\n",
    "\n",
    "# o_2 = Dense(64, activation = 'relu',\n",
    "#         kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "#         bias_initializer = keras.initializers.constant(value=0),\n",
    "#         kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_2')(o_1)\n",
    "\n",
    "# combine2 --\n",
    "combine_o_2 = keras.layers.concatenate([combine_o, fm_frist_o, fm_second_o_1,fm_second_o_2,fm_second_o_3], name='comine_2')\n",
    "# o_1 = Dense(128, activation = 'relu',\n",
    "#         kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "#         bias_initializer = keras.initializers.constant(value=0),\n",
    "#         kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_1')(combine_o_2)\n",
    "# o_1 = Dropout(0.1)(o_1)\n",
    "\n",
    "o = Dense(1, activation = 'sigmoid',\n",
    "        kernel_initializer = keras.initializers.random_normal(mean=0.0, stddev=0.05, seed=None),\n",
    "        bias_initializer = keras.initializers.constant(value=0),\n",
    "        kernel_regularizer= keras.regularizers.l1_l2(l2=0.0, l1=0.0),name = 'fc_output')(combine_o_2)\n",
    "\n",
    "\n",
    "# whole global model\n",
    "combine_model = Model(inputs = [loan_inputs, repay_inputs, event_inputs, baseline_input,fm_fea_inputs,fm_fea_inputs_account,fm_fea_inputs_user_fins,fm_fea_inputs_user_people], outputs = o)\n",
    "print(combine_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(combine_model, 'model_0909.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(baseline_feature_top_train_norm).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/offline_model/fm/util_func/eval_function.py:10: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /user/local/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 812883 samples, validate on 96797 samples\n",
      "Epoch 1/100\n",
      "812883/812883 [==============================] - 256s 314us/step - loss: 0.4951 - auroc: 0.8139 - cal_ks: 0.4991 - val_loss: 0.4553 - val_auroc: 0.8296 - val_cal_ks: 0.5231\n",
      "\n",
      "Epoch 00001: val_auroc improved from -inf to 0.82962, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 2/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.4423 - auroc: 0.8338 - cal_ks: 0.5300 - val_loss: 0.4381 - val_auroc: 0.8377 - val_cal_ks: 0.5362\n",
      "\n",
      "Epoch 00002: val_auroc improved from 0.82962 to 0.83767, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 3/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.4323 - auroc: 0.8391 - cal_ks: 0.5378 - val_loss: 0.4313 - val_auroc: 0.8412 - val_cal_ks: 0.5409\n",
      "\n",
      "Epoch 00003: val_auroc improved from 0.83767 to 0.84121, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 4/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.4269 - auroc: 0.8422 - cal_ks: 0.5428 - val_loss: 0.4280 - val_auroc: 0.8433 - val_cal_ks: 0.5446\n",
      "\n",
      "Epoch 00004: val_auroc improved from 0.84121 to 0.84334, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 5/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.4231 - auroc: 0.8444 - cal_ks: 0.5461 - val_loss: 0.4238 - val_auroc: 0.8456 - val_cal_ks: 0.5492\n",
      "\n",
      "Epoch 00005: val_auroc improved from 0.84334 to 0.84557, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 6/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.4203 - auroc: 0.8461 - cal_ks: 0.5488 - val_loss: 0.4232 - val_auroc: 0.8465 - val_cal_ks: 0.5504\n",
      "\n",
      "Epoch 00006: val_auroc improved from 0.84557 to 0.84646, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 7/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.4180 - auroc: 0.8478 - cal_ks: 0.5504 - val_loss: 0.4212 - val_auroc: 0.8475 - val_cal_ks: 0.5506\n",
      "\n",
      "Epoch 00007: val_auroc improved from 0.84646 to 0.84754, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 8/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.4160 - auroc: 0.8491 - cal_ks: 0.5531 - val_loss: 0.4193 - val_auroc: 0.8485 - val_cal_ks: 0.5534\n",
      "\n",
      "Epoch 00008: val_auroc improved from 0.84754 to 0.84854, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 9/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.4144 - auroc: 0.8503 - cal_ks: 0.5550 - val_loss: 0.4189 - val_auroc: 0.8493 - val_cal_ks: 0.5516\n",
      "\n",
      "Epoch 00009: val_auroc improved from 0.84854 to 0.84932, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 10/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.4128 - auroc: 0.8514 - cal_ks: 0.5565 - val_loss: 0.4166 - val_auroc: 0.8508 - val_cal_ks: 0.5559\n",
      "\n",
      "Epoch 00010: val_auroc improved from 0.84932 to 0.85081, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 11/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.4116 - auroc: 0.8523 - cal_ks: 0.5577 - val_loss: 0.4174 - val_auroc: 0.8507 - val_cal_ks: 0.5557\n",
      "\n",
      "Epoch 00011: val_auroc did not improve from 0.85081\n",
      "Epoch 12/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.4106 - auroc: 0.8531 - cal_ks: 0.5589 - val_loss: 0.4159 - val_auroc: 0.8512 - val_cal_ks: 0.5563\n",
      "\n",
      "Epoch 00012: val_auroc improved from 0.85081 to 0.85116, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 13/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.4098 - auroc: 0.8538 - cal_ks: 0.5602 - val_loss: 0.4151 - val_auroc: 0.8523 - val_cal_ks: 0.5586\n",
      "\n",
      "Epoch 00013: val_auroc improved from 0.85116 to 0.85225, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 14/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.4089 - auroc: 0.8545 - cal_ks: 0.5609 - val_loss: 0.4134 - val_auroc: 0.8530 - val_cal_ks: 0.5595\n",
      "\n",
      "Epoch 00014: val_auroc improved from 0.85225 to 0.85298, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 15/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.4081 - auroc: 0.8550 - cal_ks: 0.5622 - val_loss: 0.4134 - val_auroc: 0.8531 - val_cal_ks: 0.5600\n",
      "\n",
      "Epoch 00015: val_auroc improved from 0.85298 to 0.85312, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 16/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.4076 - auroc: 0.8555 - cal_ks: 0.5630 - val_loss: 0.4134 - val_auroc: 0.8529 - val_cal_ks: 0.5598\n",
      "\n",
      "Epoch 00016: val_auroc did not improve from 0.85312\n",
      "Epoch 17/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.4066 - auroc: 0.8563 - cal_ks: 0.5642 - val_loss: 0.4145 - val_auroc: 0.8534 - val_cal_ks: 0.5604\n",
      "\n",
      "Epoch 00017: val_auroc improved from 0.85312 to 0.85343, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 18/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.4061 - auroc: 0.8567 - cal_ks: 0.5648 - val_loss: 0.4137 - val_auroc: 0.8538 - val_cal_ks: 0.5610\n",
      "\n",
      "Epoch 00018: val_auroc improved from 0.85343 to 0.85378, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 19/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.4057 - auroc: 0.8572 - cal_ks: 0.5660 - val_loss: 0.4136 - val_auroc: 0.8535 - val_cal_ks: 0.5615\n",
      "\n",
      "Epoch 00019: val_auroc did not improve from 0.85378\n",
      "Epoch 20/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.4052 - auroc: 0.8577 - cal_ks: 0.5669 - val_loss: 0.4125 - val_auroc: 0.8537 - val_cal_ks: 0.5600\n",
      "\n",
      "Epoch 00020: val_auroc did not improve from 0.85378\n",
      "Epoch 21/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.4044 - auroc: 0.8582 - cal_ks: 0.5678 - val_loss: 0.4131 - val_auroc: 0.8546 - val_cal_ks: 0.5617\n",
      "\n",
      "Epoch 00021: val_auroc improved from 0.85378 to 0.85456, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 22/100\n",
      "812883/812883 [==============================] - 228s 281us/step - loss: 0.4041 - auroc: 0.8585 - cal_ks: 0.5680 - val_loss: 0.4107 - val_auroc: 0.8551 - val_cal_ks: 0.5637\n",
      "\n",
      "Epoch 00022: val_auroc improved from 0.85456 to 0.85514, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 23/100\n",
      "812883/812883 [==============================] - 228s 281us/step - loss: 0.4038 - auroc: 0.8589 - cal_ks: 0.5686 - val_loss: 0.4115 - val_auroc: 0.8554 - val_cal_ks: 0.5642\n",
      "\n",
      "Epoch 00023: val_auroc improved from 0.85514 to 0.85540, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 24/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.4030 - auroc: 0.8596 - cal_ks: 0.5701 - val_loss: 0.4107 - val_auroc: 0.8554 - val_cal_ks: 0.5635\n",
      "\n",
      "Epoch 00024: val_auroc did not improve from 0.85540\n",
      "Epoch 25/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.4027 - auroc: 0.8598 - cal_ks: 0.5705 - val_loss: 0.4103 - val_auroc: 0.8562 - val_cal_ks: 0.5644\n",
      "\n",
      "Epoch 00025: val_auroc improved from 0.85540 to 0.85617, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 26/100\n",
      "812883/812883 [==============================] - 230s 282us/step - loss: 0.4021 - auroc: 0.8605 - cal_ks: 0.5718 - val_loss: 0.4116 - val_auroc: 0.8556 - val_cal_ks: 0.5638\n",
      "\n",
      "Epoch 00026: val_auroc did not improve from 0.85617\n",
      "Epoch 27/100\n",
      "812883/812883 [==============================] - 228s 281us/step - loss: 0.4016 - auroc: 0.8608 - cal_ks: 0.5727 - val_loss: 0.4094 - val_auroc: 0.8565 - val_cal_ks: 0.5658\n",
      "\n",
      "Epoch 00027: val_auroc improved from 0.85617 to 0.85647, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 28/100\n",
      "812883/812883 [==============================] - 228s 281us/step - loss: 0.4013 - auroc: 0.8612 - cal_ks: 0.5738 - val_loss: 0.4098 - val_auroc: 0.8566 - val_cal_ks: 0.5664\n",
      "\n",
      "Epoch 00028: val_auroc improved from 0.85647 to 0.85662, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 29/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.4009 - auroc: 0.8614 - cal_ks: 0.5737 - val_loss: 0.4106 - val_auroc: 0.8563 - val_cal_ks: 0.5636\n",
      "\n",
      "Epoch 00029: val_auroc did not improve from 0.85662\n",
      "Epoch 30/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.4003 - auroc: 0.8621 - cal_ks: 0.5749 - val_loss: 0.4082 - val_auroc: 0.8574 - val_cal_ks: 0.5658\n",
      "\n",
      "Epoch 00030: val_auroc improved from 0.85662 to 0.85740, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 31/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.4000 - auroc: 0.8623 - cal_ks: 0.5755 - val_loss: 0.4087 - val_auroc: 0.8574 - val_cal_ks: 0.5672\n",
      "\n",
      "Epoch 00031: val_auroc did not improve from 0.85740\n",
      "Epoch 32/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3994 - auroc: 0.8626 - cal_ks: 0.5754 - val_loss: 0.4089 - val_auroc: 0.8571 - val_cal_ks: 0.5677\n",
      "\n",
      "Epoch 00032: val_auroc did not improve from 0.85740\n",
      "Epoch 33/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.3993 - auroc: 0.8630 - cal_ks: 0.5769 - val_loss: 0.4109 - val_auroc: 0.8574 - val_cal_ks: 0.5650\n",
      "\n",
      "Epoch 00033: val_auroc improved from 0.85740 to 0.85742, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 34/100\n",
      "812883/812883 [==============================] - 233s 286us/step - loss: 0.3989 - auroc: 0.8633 - cal_ks: 0.5774 - val_loss: 0.4076 - val_auroc: 0.8584 - val_cal_ks: 0.5683\n",
      "\n",
      "Epoch 00034: val_auroc improved from 0.85742 to 0.85841, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 35/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.3988 - auroc: 0.8634 - cal_ks: 0.5779 - val_loss: 0.4077 - val_auroc: 0.8581 - val_cal_ks: 0.5696\n",
      "\n",
      "Epoch 00035: val_auroc did not improve from 0.85841\n",
      "Epoch 36/100\n",
      "812883/812883 [==============================] - 230s 282us/step - loss: 0.3982 - auroc: 0.8638 - cal_ks: 0.5789 - val_loss: 0.4083 - val_auroc: 0.8577 - val_cal_ks: 0.5673\n",
      "\n",
      "Epoch 00036: val_auroc did not improve from 0.85841\n",
      "Epoch 37/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.3981 - auroc: 0.8640 - cal_ks: 0.5787 - val_loss: 0.4078 - val_auroc: 0.8582 - val_cal_ks: 0.5694\n",
      "\n",
      "Epoch 00037: val_auroc did not improve from 0.85841\n",
      "Epoch 38/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.3977 - auroc: 0.8643 - cal_ks: 0.5791 - val_loss: 0.4081 - val_auroc: 0.8580 - val_cal_ks: 0.5680\n",
      "\n",
      "Epoch 00038: val_auroc did not improve from 0.85841\n",
      "Epoch 39/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3976 - auroc: 0.8644 - cal_ks: 0.5792 - val_loss: 0.4079 - val_auroc: 0.8581 - val_cal_ks: 0.5685\n",
      "\n",
      "Epoch 00039: val_auroc did not improve from 0.85841\n",
      "Epoch 40/100\n",
      "812883/812883 [==============================] - 236s 291us/step - loss: 0.3972 - auroc: 0.8647 - cal_ks: 0.5799 - val_loss: 0.4069 - val_auroc: 0.8586 - val_cal_ks: 0.5692\n",
      "\n",
      "Epoch 00040: val_auroc improved from 0.85841 to 0.85858, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 41/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.3971 - auroc: 0.8650 - cal_ks: 0.5808 - val_loss: 0.4084 - val_auroc: 0.8579 - val_cal_ks: 0.5678\n",
      "\n",
      "Epoch 00041: val_auroc did not improve from 0.85858\n",
      "Epoch 42/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.3970 - auroc: 0.8649 - cal_ks: 0.5807 - val_loss: 0.4087 - val_auroc: 0.8578 - val_cal_ks: 0.5663\n",
      "\n",
      "Epoch 00042: val_auroc did not improve from 0.85858\n",
      "Epoch 43/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3968 - auroc: 0.8653 - cal_ks: 0.5815 - val_loss: 0.4083 - val_auroc: 0.8590 - val_cal_ks: 0.5689\n",
      "\n",
      "Epoch 00043: val_auroc improved from 0.85858 to 0.85897, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 44/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3966 - auroc: 0.8653 - cal_ks: 0.5814 - val_loss: 0.4075 - val_auroc: 0.8583 - val_cal_ks: 0.5700\n",
      "\n",
      "Epoch 00044: val_auroc did not improve from 0.85897\n",
      "Epoch 45/100\n",
      "812883/812883 [==============================] - 227s 280us/step - loss: 0.3965 - auroc: 0.8654 - cal_ks: 0.5825 - val_loss: 0.4076 - val_auroc: 0.8582 - val_cal_ks: 0.5680\n",
      "\n",
      "Epoch 00045: val_auroc did not improve from 0.85897\n",
      "Epoch 46/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3960 - auroc: 0.8658 - cal_ks: 0.5825 - val_loss: 0.4083 - val_auroc: 0.8578 - val_cal_ks: 0.5677\n",
      "\n",
      "Epoch 00046: val_auroc did not improve from 0.85897\n",
      "Epoch 47/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3959 - auroc: 0.8661 - cal_ks: 0.5819 - val_loss: 0.4075 - val_auroc: 0.8589 - val_cal_ks: 0.5706\n",
      "\n",
      "Epoch 00047: val_auroc did not improve from 0.85897\n",
      "Epoch 48/100\n",
      "812883/812883 [==============================] - 229s 281us/step - loss: 0.3956 - auroc: 0.8661 - cal_ks: 0.5820 - val_loss: 0.4072 - val_auroc: 0.8588 - val_cal_ks: 0.5699\n",
      "\n",
      "Epoch 00048: val_auroc did not improve from 0.85897\n",
      "Epoch 49/100\n",
      "812883/812883 [==============================] - 227s 280us/step - loss: 0.3953 - auroc: 0.8664 - cal_ks: 0.5831 - val_loss: 0.4078 - val_auroc: 0.8584 - val_cal_ks: 0.5690\n",
      "\n",
      "Epoch 00049: val_auroc did not improve from 0.85897\n",
      "Epoch 50/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3953 - auroc: 0.8664 - cal_ks: 0.5836 - val_loss: 0.4075 - val_auroc: 0.8585 - val_cal_ks: 0.5683\n",
      "\n",
      "Epoch 00050: val_auroc did not improve from 0.85897\n",
      "Epoch 51/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3954 - auroc: 0.8663 - cal_ks: 0.5829 - val_loss: 0.4070 - val_auroc: 0.8592 - val_cal_ks: 0.5711\n",
      "\n",
      "Epoch 00051: val_auroc improved from 0.85897 to 0.85916, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 52/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3951 - auroc: 0.8667 - cal_ks: 0.5838 - val_loss: 0.4070 - val_auroc: 0.8593 - val_cal_ks: 0.5714\n",
      "\n",
      "Epoch 00052: val_auroc improved from 0.85916 to 0.85926, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 53/100\n",
      "812883/812883 [==============================] - 226s 277us/step - loss: 0.3949 - auroc: 0.8669 - cal_ks: 0.5837 - val_loss: 0.4079 - val_auroc: 0.8585 - val_cal_ks: 0.5693\n",
      "\n",
      "Epoch 00053: val_auroc did not improve from 0.85926\n",
      "Epoch 54/100\n",
      "812883/812883 [==============================] - 224s 276us/step - loss: 0.3948 - auroc: 0.8671 - cal_ks: 0.5845 - val_loss: 0.4069 - val_auroc: 0.8593 - val_cal_ks: 0.5700\n",
      "\n",
      "Epoch 00054: val_auroc improved from 0.85926 to 0.85929, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 55/100\n",
      "812883/812883 [==============================] - 227s 280us/step - loss: 0.3944 - auroc: 0.8672 - cal_ks: 0.5842 - val_loss: 0.4070 - val_auroc: 0.8589 - val_cal_ks: 0.5696\n",
      "\n",
      "Epoch 00055: val_auroc did not improve from 0.85929\n",
      "Epoch 56/100\n",
      "812883/812883 [==============================] - 225s 276us/step - loss: 0.3945 - auroc: 0.8672 - cal_ks: 0.5853 - val_loss: 0.4072 - val_auroc: 0.8588 - val_cal_ks: 0.5691\n",
      "\n",
      "Epoch 00056: val_auroc did not improve from 0.85929\n",
      "Epoch 57/100\n",
      "812883/812883 [==============================] - 224s 275us/step - loss: 0.3943 - auroc: 0.8674 - cal_ks: 0.5850 - val_loss: 0.4070 - val_auroc: 0.8588 - val_cal_ks: 0.5701\n",
      "\n",
      "Epoch 00057: val_auroc did not improve from 0.85929\n",
      "Epoch 58/100\n",
      "812883/812883 [==============================] - 225s 277us/step - loss: 0.3941 - auroc: 0.8674 - cal_ks: 0.5854 - val_loss: 0.4082 - val_auroc: 0.8586 - val_cal_ks: 0.5703\n",
      "\n",
      "Epoch 00058: val_auroc did not improve from 0.85929\n",
      "Epoch 59/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3939 - auroc: 0.8677 - cal_ks: 0.5858 - val_loss: 0.4080 - val_auroc: 0.8579 - val_cal_ks: 0.5686\n",
      "\n",
      "Epoch 00059: val_auroc did not improve from 0.85929\n",
      "Epoch 60/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3937 - auroc: 0.8680 - cal_ks: 0.5859 - val_loss: 0.4073 - val_auroc: 0.8590 - val_cal_ks: 0.5695\n",
      "\n",
      "Epoch 00060: val_auroc did not improve from 0.85929\n",
      "Epoch 61/100\n",
      "812883/812883 [==============================] - 225s 277us/step - loss: 0.3939 - auroc: 0.8678 - cal_ks: 0.5855 - val_loss: 0.4078 - val_auroc: 0.8589 - val_cal_ks: 0.5694\n",
      "\n",
      "Epoch 00061: val_auroc did not improve from 0.85929\n",
      "Epoch 62/100\n",
      "812883/812883 [==============================] - 225s 277us/step - loss: 0.3938 - auroc: 0.8680 - cal_ks: 0.5859 - val_loss: 0.4075 - val_auroc: 0.8589 - val_cal_ks: 0.5694\n",
      "\n",
      "Epoch 00062: val_auroc did not improve from 0.85929\n",
      "Epoch 63/100\n",
      "812883/812883 [==============================] - 225s 277us/step - loss: 0.3936 - auroc: 0.8680 - cal_ks: 0.5861 - val_loss: 0.4078 - val_auroc: 0.8584 - val_cal_ks: 0.5689\n",
      "\n",
      "Epoch 00063: val_auroc did not improve from 0.85929\n",
      "Epoch 64/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3935 - auroc: 0.8681 - cal_ks: 0.5862 - val_loss: 0.4080 - val_auroc: 0.8588 - val_cal_ks: 0.5711\n",
      "\n",
      "Epoch 00064: val_auroc did not improve from 0.85929\n",
      "Epoch 65/100\n",
      "812883/812883 [==============================] - 226s 279us/step - loss: 0.3933 - auroc: 0.8683 - cal_ks: 0.5870 - val_loss: 0.4076 - val_auroc: 0.8588 - val_cal_ks: 0.5693\n",
      "\n",
      "Epoch 00065: val_auroc did not improve from 0.85929\n",
      "Epoch 66/100\n",
      "812883/812883 [==============================] - 226s 279us/step - loss: 0.3931 - auroc: 0.8684 - cal_ks: 0.5871 - val_loss: 0.4088 - val_auroc: 0.8579 - val_cal_ks: 0.5676\n",
      "\n",
      "Epoch 00066: val_auroc did not improve from 0.85929\n",
      "Epoch 67/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3930 - auroc: 0.8686 - cal_ks: 0.5872 - val_loss: 0.4082 - val_auroc: 0.8586 - val_cal_ks: 0.5681\n",
      "\n",
      "Epoch 00067: val_auroc did not improve from 0.85929\n",
      "Epoch 68/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3927 - auroc: 0.8688 - cal_ks: 0.5873 - val_loss: 0.4085 - val_auroc: 0.8585 - val_cal_ks: 0.5684\n",
      "\n",
      "Epoch 00068: val_auroc did not improve from 0.85929\n",
      "Epoch 69/100\n",
      "812883/812883 [==============================] - 231s 284us/step - loss: 0.3931 - auroc: 0.8685 - cal_ks: 0.5873 - val_loss: 0.4076 - val_auroc: 0.8589 - val_cal_ks: 0.5688\n",
      "\n",
      "Epoch 00069: val_auroc did not improve from 0.85929\n",
      "Epoch 70/100\n",
      "812883/812883 [==============================] - 230s 283us/step - loss: 0.3925 - auroc: 0.8689 - cal_ks: 0.5885 - val_loss: 0.4078 - val_auroc: 0.8582 - val_cal_ks: 0.5694\n",
      "\n",
      "Epoch 00070: val_auroc did not improve from 0.85929\n",
      "Epoch 71/100\n",
      "812883/812883 [==============================] - 229s 282us/step - loss: 0.3928 - auroc: 0.8688 - cal_ks: 0.5873 - val_loss: 0.4077 - val_auroc: 0.8593 - val_cal_ks: 0.5706\n",
      "\n",
      "Epoch 00071: val_auroc improved from 0.85929 to 0.85933, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 72/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3926 - auroc: 0.8690 - cal_ks: 0.5878 - val_loss: 0.4077 - val_auroc: 0.8587 - val_cal_ks: 0.5696\n",
      "\n",
      "Epoch 00072: val_auroc did not improve from 0.85933\n",
      "Epoch 73/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3926 - auroc: 0.8690 - cal_ks: 0.5879 - val_loss: 0.4089 - val_auroc: 0.8578 - val_cal_ks: 0.5674\n",
      "\n",
      "Epoch 00073: val_auroc did not improve from 0.85933\n",
      "Epoch 74/100\n",
      "812883/812883 [==============================] - 226s 279us/step - loss: 0.3922 - auroc: 0.8692 - cal_ks: 0.5883 - val_loss: 0.4075 - val_auroc: 0.8589 - val_cal_ks: 0.5700\n",
      "\n",
      "Epoch 00074: val_auroc did not improve from 0.85933\n",
      "Epoch 75/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3924 - auroc: 0.8691 - cal_ks: 0.5875 - val_loss: 0.4079 - val_auroc: 0.8589 - val_cal_ks: 0.5693\n",
      "\n",
      "Epoch 00075: val_auroc did not improve from 0.85933\n",
      "Epoch 76/100\n",
      "812883/812883 [==============================] - 228s 280us/step - loss: 0.3921 - auroc: 0.8693 - cal_ks: 0.5889 - val_loss: 0.4078 - val_auroc: 0.8586 - val_cal_ks: 0.5689\n",
      "\n",
      "Epoch 00076: val_auroc did not improve from 0.85933\n",
      "Epoch 77/100\n",
      "812883/812883 [==============================] - 227s 280us/step - loss: 0.3920 - auroc: 0.8695 - cal_ks: 0.5891 - val_loss: 0.4089 - val_auroc: 0.8584 - val_cal_ks: 0.5681\n",
      "\n",
      "Epoch 00077: val_auroc did not improve from 0.85933\n",
      "Epoch 78/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3920 - auroc: 0.8695 - cal_ks: 0.5887 - val_loss: 0.4078 - val_auroc: 0.8593 - val_cal_ks: 0.5708\n",
      "\n",
      "Epoch 00078: val_auroc improved from 0.85933 to 0.85935, saving model to /home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5\n",
      "Epoch 79/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3919 - auroc: 0.8697 - cal_ks: 0.5892 - val_loss: 0.4086 - val_auroc: 0.8587 - val_cal_ks: 0.5689\n",
      "\n",
      "Epoch 00079: val_auroc did not improve from 0.85935\n",
      "Epoch 80/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3917 - auroc: 0.8698 - cal_ks: 0.5894 - val_loss: 0.4080 - val_auroc: 0.8589 - val_cal_ks: 0.5708\n",
      "\n",
      "Epoch 00080: val_auroc did not improve from 0.85935\n",
      "Epoch 81/100\n",
      "812883/812883 [==============================] - 227s 280us/step - loss: 0.3918 - auroc: 0.8698 - cal_ks: 0.5898 - val_loss: 0.4090 - val_auroc: 0.8578 - val_cal_ks: 0.5680\n",
      "\n",
      "Epoch 00081: val_auroc did not improve from 0.85935\n",
      "Epoch 82/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3916 - auroc: 0.8699 - cal_ks: 0.5895 - val_loss: 0.4087 - val_auroc: 0.8583 - val_cal_ks: 0.5684\n",
      "\n",
      "Epoch 00082: val_auroc did not improve from 0.85935\n",
      "Epoch 83/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3915 - auroc: 0.8700 - cal_ks: 0.5895 - val_loss: 0.4087 - val_auroc: 0.8584 - val_cal_ks: 0.5680\n",
      "\n",
      "Epoch 00083: val_auroc did not improve from 0.85935\n",
      "Epoch 84/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3915 - auroc: 0.8699 - cal_ks: 0.5898 - val_loss: 0.4081 - val_auroc: 0.8589 - val_cal_ks: 0.5704\n",
      "\n",
      "Epoch 00084: val_auroc did not improve from 0.85935\n",
      "Epoch 85/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3915 - auroc: 0.8701 - cal_ks: 0.5903 - val_loss: 0.4080 - val_auroc: 0.8587 - val_cal_ks: 0.5688\n",
      "\n",
      "Epoch 00085: val_auroc did not improve from 0.85935\n",
      "Epoch 86/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3914 - auroc: 0.8703 - cal_ks: 0.5909 - val_loss: 0.4085 - val_auroc: 0.8584 - val_cal_ks: 0.5681\n",
      "\n",
      "Epoch 00086: val_auroc did not improve from 0.85935\n",
      "Epoch 87/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3912 - auroc: 0.8702 - cal_ks: 0.5903 - val_loss: 0.4082 - val_auroc: 0.8584 - val_cal_ks: 0.5685\n",
      "\n",
      "Epoch 00087: val_auroc did not improve from 0.85935\n",
      "Epoch 88/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3910 - auroc: 0.8704 - cal_ks: 0.5907 - val_loss: 0.4081 - val_auroc: 0.8588 - val_cal_ks: 0.5689\n",
      "\n",
      "Epoch 00088: val_auroc did not improve from 0.85935\n",
      "Epoch 89/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3909 - auroc: 0.8706 - cal_ks: 0.5910 - val_loss: 0.4085 - val_auroc: 0.8586 - val_cal_ks: 0.5678\n",
      "\n",
      "Epoch 00089: val_auroc did not improve from 0.85935\n",
      "Epoch 90/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3906 - auroc: 0.8708 - cal_ks: 0.5915 - val_loss: 0.4078 - val_auroc: 0.8586 - val_cal_ks: 0.5682\n",
      "\n",
      "Epoch 00090: val_auroc did not improve from 0.85935\n",
      "Epoch 91/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3906 - auroc: 0.8706 - cal_ks: 0.5909 - val_loss: 0.4086 - val_auroc: 0.8585 - val_cal_ks: 0.5674\n",
      "\n",
      "Epoch 00091: val_auroc did not improve from 0.85935\n",
      "Epoch 92/100\n",
      "812883/812883 [==============================] - 228s 281us/step - loss: 0.3909 - auroc: 0.8706 - cal_ks: 0.5913 - val_loss: 0.4081 - val_auroc: 0.8587 - val_cal_ks: 0.5682\n",
      "\n",
      "Epoch 00092: val_auroc did not improve from 0.85935\n",
      "Epoch 93/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3906 - auroc: 0.8707 - cal_ks: 0.5910 - val_loss: 0.4096 - val_auroc: 0.8586 - val_cal_ks: 0.5680\n",
      "\n",
      "Epoch 00093: val_auroc did not improve from 0.85935\n",
      "Epoch 94/100\n",
      "812883/812883 [==============================] - 226s 279us/step - loss: 0.3906 - auroc: 0.8709 - cal_ks: 0.5916 - val_loss: 0.4092 - val_auroc: 0.8581 - val_cal_ks: 0.5673\n",
      "\n",
      "Epoch 00094: val_auroc did not improve from 0.85935\n",
      "Epoch 95/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3902 - auroc: 0.8710 - cal_ks: 0.5911 - val_loss: 0.4092 - val_auroc: 0.8581 - val_cal_ks: 0.5679\n",
      "\n",
      "Epoch 00095: val_auroc did not improve from 0.85935\n",
      "Epoch 96/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3906 - auroc: 0.8709 - cal_ks: 0.5914 - val_loss: 0.4096 - val_auroc: 0.8583 - val_cal_ks: 0.5688\n",
      "\n",
      "Epoch 00096: val_auroc did not improve from 0.85935\n",
      "Epoch 97/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3904 - auroc: 0.8711 - cal_ks: 0.5920 - val_loss: 0.4086 - val_auroc: 0.8583 - val_cal_ks: 0.5678\n",
      "\n",
      "Epoch 00097: val_auroc did not improve from 0.85935\n",
      "Epoch 98/100\n",
      "812883/812883 [==============================] - 227s 279us/step - loss: 0.3905 - auroc: 0.8709 - cal_ks: 0.5918 - val_loss: 0.4079 - val_auroc: 0.8588 - val_cal_ks: 0.5690\n",
      "\n",
      "Epoch 00098: val_auroc did not improve from 0.85935\n",
      "Epoch 99/100\n",
      "812883/812883 [==============================] - 226s 278us/step - loss: 0.3903 - auroc: 0.8709 - cal_ks: 0.5911 - val_loss: 0.4083 - val_auroc: 0.8585 - val_cal_ks: 0.5686\n",
      "\n",
      "Epoch 00099: val_auroc did not improve from 0.85935\n",
      "Epoch 100/100\n",
      "812883/812883 [==============================] - 225s 277us/step - loss: 0.3902 - auroc: 0.8710 - cal_ks: 0.5914 - val_loss: 0.4092 - val_auroc: 0.8576 - val_cal_ks: 0.5672\n",
      "\n",
      "Epoch 00100: val_auroc did not improve from 0.85935\n"
     ]
    }
   ],
   "source": [
    "learning_rate= 1e-3\n",
    "adam = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07,)\n",
    "look_ahead = Lookahead(adam, sync_period=5, slow_step=0.5)\n",
    "combine_model.compile(optimizer = look_ahead, loss = 'binary_crossentropy', metrics = [auroc, cal_ks], sample_weight_mode = None) \n",
    "# # Training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=2, mode='min')\n",
    "# save_model_file='./fm_0902.h5'\n",
    "# filepath=save_model_file\n",
    "checkpoint = ModelCheckpoint(save_model_file, monitor='val_auroc', verbose=1, save_best_only=True, mode='max')\n",
    "history_v1 = combine_model.fit(\n",
    "    [train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm,fm_feature_train_norm,fm_account_feature_train_norm,fm_user_fins_feature_train_norm,fm_user_people_feature_train_norm],\n",
    "    train_label,\n",
    "    epochs = 100,\n",
    "    batch_size = 512,\n",
    "    #verbose = 10,  ## 每个epoch输出一行记录\n",
    "    # validation_split = 0.25,\n",
    "    validation_data = ([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm,fm_feature_test_norm,fm_account_feature_test_norm,fm_user_fins_feature_test_norm,fm_user_people_feature_test_norm]\n",
    "                       , test_label),\n",
    "    shuffle = True,\n",
    "    class_weight = {0:1, 1:1},\n",
    "    initial_epoch=0,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------model evaluatie------------------\n",
      "evaluate on train data:\n",
      "0.8779968687589085 0.5808458520204136\n",
      "evaluate on test data:\n",
      "0.8599058151797188 0.5465188985530481\n",
      "evaluate on test1 data:\n",
      "0.861698630715575 0.5524368116081387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combine_model.load_weights(save_model_file)\n",
    "\n",
    "print(\"-------------------model evaluatie------------------\")\n",
    "print(\"evaluate on train data:\")\n",
    "pred_train = combine_model.predict([train_loan_seq_matrix, train_repay_seq_matrix, train_event_seq_matrix, baseline_feature_top_train_norm,fm_feature_train_norm,fm_account_feature_train_norm,fm_user_fins_feature_train_norm,fm_user_people_feature_train_norm])\n",
    "train_auc = auc(pred_train[:,0], train_label)\n",
    "train_ks = ks(train_label, pred_train[:,0])\n",
    "print(train_auc, train_ks)\n",
    "\n",
    "print(\"evaluate on test data:\")\n",
    "pred_test = combine_model.predict([test_loan_seq_matrix, test_repay_seq_matrix, test_event_seq_matrix,baseline_feature_top_test_norm,fm_feature_test_norm,fm_account_feature_test_norm,fm_user_fins_feature_test_norm,fm_user_people_feature_test_norm])\n",
    "test_auc = auc(pred_test[:,0], test_label)\n",
    "test_ks = ks(test_label, pred_test[:,0])\n",
    "print(test_auc, test_ks)\n",
    "\n",
    "print(\"evaluate on test1 data:\")\n",
    "pred_test1 = combine_model.predict([test1_loan_seq_matrix, test1_repay_seq_matrix, test1_event_seq_matrix,baseline_feature_top_test1_norm,fm_feature_test1_norm,fm_account_feature_test1_norm,fm_user_fins_feature_test1_norm,fm_user_people_feature_test1_norm])\n",
    "test1_auc = auc(pred_test1[:,0], test1_label)\n",
    "test1_ks = ks(test1_label, pred_test1[:,0])\n",
    "print(test1_auc, test1_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pred_train,columns=['fm_pred_train']).to_csv(new_model_pred_train_path, sep='\\t',index=0)\n",
    "pd.DataFrame(pred_test,columns=['fm_pred_test']).to_csv(new_model_pred_test_path, sep='\\t',index=0)\n",
    "pd.DataFrame(pred_test1,columns=['fm_pred_test1']).to_csv(new_model_pred_test1_path, sep='\\t',index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(test_label, pred_test, pos_label=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "baseline1_pred_test_path = data_dir + 'model_file/baseline1_test_pred.csv'\n",
    "baseline1_test_pred = pd.read_csv(baseline1_pred_test_path)\n",
    "test_pred_bl1 = baseline1_test_pred['baseline1_pred_test'].tolist()\n",
    "precision_bl1, recall_bl1, thresholds_bl1 = metrics.precision_recall_curve(test_label, test_pred_bl1, pos_label=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "new_model_pred_test_path = data_dir + 'model_file/baseline2_pred_test.csv'\n",
    "baseline2_test_pred = pd.read_csv(new_model_pred_test_path)\n",
    "test_pred_baseline2 = baseline2_test_pred['baseline2_pred_test'].tolist()\n",
    "precision_bl2, recall_bl2, thresholds_bl2 = metrics.precision_recall_curve(test_label, test_pred_baseline2, pos_label=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dien_pred = pd.read_csv(data_dir + 'model_file/dien_pred.csv')\n",
    "test_pred_dien = dien_pred['pred'].tolist()\n",
    "precision_dien, recall_dien, thresholds_dien = metrics.precision_recall_curve(test_label, test_pred_dien, pos_label=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/user/local/miniconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"P-R Curve\",figsize=(10,10))\n",
    "plt.title('Precision/Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.plot(recall_bl1,precision_bl1,label='baseline1')\n",
    "plt.plot(recall_bl2,precision_bl2,label='baseline2')\n",
    "plt.plot(recall_dien,precision_dien,label='dien')\n",
    "plt.plot(recall,precision,label='fm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_top_pr(df,i):\n",
    "    pred_test = df['pred_test']\n",
    "    cuts = np.arange(int(10), 100, int(10))\n",
    "    cut_test = np.percentile(pred_test, cuts) \n",
    "    df_top1 = df[df['pred_test']>cut_test[len(cut_test)-i]]\n",
    "    precision = df_top1['test_label'].value_counts()[1]/df_top1.shape[0]\n",
    "    recall = df_top1['test_label'].value_counts()[1]/df['test_label'].value_counts()[1]\n",
    "    return precision,recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7859376191565622, 0.2951486339423793)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(test_label,columns=['test_label'])\n",
    "df_1['pred_test'] = test_pred_bl1\n",
    "p_1,r_1=get_top_pr(df_1,1)\n",
    "p_1,r_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8546480591779151, 0.3209519445558165)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.DataFrame(test_label,columns=['test_label'])\n",
    "df_2['pred_test'] = test_pred_baseline2\n",
    "p_2,r_2=get_top_pr(df_2,1)\n",
    "p_2,r_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.864866925951346, 0.3247895068446074)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(test_label,columns=['test_label'])\n",
    "df['pred_test'] = pred_test\n",
    "p,r=get_top_pr(df,1)\n",
    "p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.864028063753527, 0.324474483074632)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4 = pd.DataFrame(test_label,columns=['test_label'])\n",
    "df_4['pred_test'] = test_pred_dien\n",
    "p_4,r_4=get_top_pr(df_4,1)\n",
    "p_4,r_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fm_test_pred = pd.read_csv(new_model_pred_test_path,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fm_test_pred['label'] = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fm_pred_test</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.442537</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fm_pred_test  label\n",
       "0      0.105050      0\n",
       "1      0.047718      0\n",
       "2      0.002010      0\n",
       "3      0.442537      0\n",
       "4      0.230132      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cuts = np.arange(int(10), 100, int(10))\n",
    "cut_old = np.percentile(fm_test_pred['fm_pred_test'], cuts) \n",
    "cut_old = np.append(np.array([float('-Inf')]), cut_old, axis=0)\n",
    "cut_old = np.append(cut_old, np.array([float('Inf')]), axis=0)\n",
    "br_old = fm_test_pred.groupby(pd.cut(fm_test_pred['fm_pred_test'], cut_old))['label'].mean()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "                   'cut': fm_test_pred.groupby(pd.cut(fm_test_pred['fm_pred_test'], cut_old))['fm_pred_test'].mean().index.astype(str),\n",
    "                   'rate' : br_old.values,\n",
    "                   \n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.006406\n",
       "1    0.023261\n",
       "2    0.050946\n",
       "3    0.082672\n",
       "4    0.132550\n",
       "5    0.197987\n",
       "6    0.291718\n",
       "7    0.415574\n",
       "8    0.597010\n",
       "9    0.864867\n",
       "Name: rate, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def model_save_as_pb(combine_model,save_model_file,save_pb_file_path,save_pb_file_name):\n",
    "    \n",
    "    combine_model.load_weights(save_model_file)\n",
    "    for node in combine_model.inputs:\n",
    "        print(node.op.name)\n",
    "\n",
    "    for node in combine_model.outputs:\n",
    "        print(node.op.name)\n",
    "\n",
    "    from util_func.gen_pb import convert_variables_to_constants\n",
    "    graph = convert_variables_to_constants(K.get_session(), K.get_session().graph_def, output_node_names=[\"fc_output/Sigmoid\"],)\n",
    "    tf.train.write_graph(graph, save_pb_file_path, save_pb_file_name, as_text=False)   \n",
    "\n",
    "    a = np.array([0.8]*260).reshape(1,10,26)\n",
    "    b = np.array([0.8]*255).reshape(1,15,17)\n",
    "    c = np.array([0.7]*1100).reshape(1,20,55)\n",
    "    d = np.array([0.7]*194).reshape(1,194)\n",
    "    f1 = np.array([1]*107).reshape(1,107)\n",
    "    f2 = np.array([1]*39).reshape(1,39)\n",
    "    f3 = np.array([1]*22).reshape(1,22)\n",
    "    f4 = np.array([1]*46).reshape(1,46)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        output_graph_def = tf.GraphDef()\n",
    "\n",
    "        with open(save_pb_file_path+\"/\"+save_pb_file_name, \"rb\") as f:\n",
    "            output_graph_def.ParseFromString(f.read())\n",
    "            _ = tf.import_graph_def(output_graph_def, name=\"\")\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            output = sess.graph.get_tensor_by_name(\"fc_output/Sigmoid:0\")\n",
    "            input_event_seq = sess.graph.get_tensor_by_name(\"event_seq_input:0\")\n",
    "            input_loan_seq = sess.graph.get_tensor_by_name(\"loan_seq_input:0\")\n",
    "            input_repay_seq = sess.graph.get_tensor_by_name(\"repay_seq_input:0\")\n",
    "            input_baseline = sess.graph.get_tensor_by_name(\"baseline_top_inputs:0\")\n",
    "            fm_fea = sess.graph.get_tensor_by_name(\"fm_fea:0\")\n",
    "            fm_fea_accont = sess.graph.get_tensor_by_name(\"fm_fea_accont:0\")\n",
    "            fm_fea_user_fins = sess.graph.get_tensor_by_name(\"fm_fea_user_fins:0\")\n",
    "            fm_fea_user_people = sess.graph.get_tensor_by_name(\"fm_fea_user_people:0\")\n",
    "            \n",
    "            demo = sess.run(output, feed_dict={input_loan_seq:a,\n",
    "                                                  input_repay_seq:b,\n",
    "                                                  input_event_seq:c,\n",
    "                                                  input_baseline:d,\n",
    "                                                  fm_fea:f1,\n",
    "                                                  fm_fea_accont:f2,\n",
    "                                                  fm_fea_user_fins:f3,\n",
    "                                                  fm_fea_user_people:f4\n",
    "                                              })\n",
    "    if demo == combine_model.predict([a, b, c, d,f1,f2,f3,f4]):\n",
    "        print(demo,\"done!\")\n",
    "    else:\n",
    "        print(\"error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_seq_input\n",
      "repay_seq_input\n",
      "event_seq_input\n",
      "baseline_top_inputs\n",
      "fm_fea\n",
      "fm_fea_accont\n",
      "fm_fea_user_fins\n",
      "fm_fea_user_people\n",
      "fc_output/Sigmoid\n",
      "INFO:tensorflow:Froze 25 variables.\n",
      "INFO:tensorflow:Converted 25 variables to const ops.\n",
      "[[0.78487563]] done!\n"
     ]
    }
   ],
   "source": [
    "model_save_as_pb(combine_model,save_model_file,save_pb_file_path,save_pb_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/odin/chengbixiao/git_code/User_loan_demand/user_loan_demand/offline/data/0909/model_file/fm_64_107_3_0909.h5'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
